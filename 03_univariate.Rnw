\chapter{Eine einzelne numerische Variable beschreiben}\label{ch:descriptives}
In diesem Kapitel wollen wir illustrieren,
wie man eine endliche Liste von Beobachtungen 
einer einzelnen numerischen Variablen
grafisch und in Zahlen zusammenfassen kann.
Dafür arbeiten wir mit einem kleinen, aber dafür übersichtlichen Datensatz,
der meiner Bachelorarbeit zu Grunde lag.
Für diese Arbeit habe ich 23 Studierenden im zweiten Jahr im
Fach schwedische Sprach- und Literaturwissenschaft
an der Universität Gent vier Leseverstehensaufgaben vorgelegt:
einen auf Schwedisch, einen auf auf
Dänisch, einen auf bokm\aa{}l-Norwegisch
und einen auf nynorsk-Norwegisch.
Die Ergebnisse aus den unterschiedlichen Lesetests
sind nicht miteinander vergleichbar und die nynorsk-Daten
sind im Datensatz nicht vorhanden.
Daneben gibt es Angaben zu den sonstigen Sprachkenntnissen
der Teilnehmenden; diese ignorieren wir hier.
Ich gehe davon aus, dass das \texttt{tidyverse}-Bündel
und das \texttt{here}-Package geladen sind und dass
Sie die Datei \texttt{jv\_bachpap.csv} in den Ordner
\texttt{data} in Ihrem R-Projekt abgelegt haben.
<<message = FALSE>>=
d <- read_csv(here("data", "jv_bachpap.csv"))
d |>
  slice_head(n = 3)
@

In diesem Kapitel widmen wir uns der grafischen
und numerischen Beschreibung einer einzelnen numerischen
Variablen: den Ergebnissen beim norwegischen Lesetest.
Vorübergehend gehen wir davon aus, dass wir uns
ausschliesslich für die 23 Ergebnisse im Datensatz
interessieren und keine allgemeineren Aussagen machen
möchten -- z.B.\ über das Leseverständnis im Norwegischen
von Studierenden im zweiten Jahr im
Fach schwedische Sprach- und Literaturwissenschaft, die
nicht im Datensatz vorhanden sind.
Wir betrachten die Ergebnisse, die uns zur Verfügung
stehen, also als die ganze, endliche \term{Population}, für die
wir uns interessieren, und nicht als bloss eine
\term{Stichprobe}, d.h., einen Teil der Population,
die von Interesse wäre.

\section{Das Punktdiagramm}
Wenn wir über die Ergebnisse kommunizieren wollen
und der Datensatz ganz klein ist, könnten wir
ihn einfach direkt reproduzieren. Aber sogar
für den relativ kleinen Datensatz hier -- bloss 23
Beobachtungen -- würde es sich lohnen, die
Daten grafisch darzustellen und numerisch
zusammenzufassen.

Eine erste grafische Darstellung
ist das \term{Punktdiagramm} (oder \term{Cleveland dot chart}),
siehe Abbildung \ref{fig:dotchart}. Anstatt
lediglich die Zahlen im Datensatz aufzulisten,
werden die Beobachtungen als Punkte
auf separaten Linien entlang der $x$-Achse dargestellt.
Jede Linie wird mit einer ID entlang der $y$-Achse vermerkt.
<<echo = FALSE, fig.cap = "Ein Punktdiagramm sortiert nach den IDs der Versuchspersonen (links) und eins geordnet nach dem Ergebnis (rechts). Es gibt keine Werte, die weit von anderen liegen.\\label{fig:dotchart}", fig.width = 6, fig.height = 3,  out.width = '.8\\textwidth'>>=
p1 <- ggplot(data = d,
       aes(x = Norwegian,
           y = Participant)) +
  geom_point() +
  xlab("Ergebnis Norwegisch") +
  ylab("ID Versuchsperson")

p2 <- ggplot(data = d,
       aes(x = Norwegian,
           y = reorder(Participant, Norwegian))) +
  geom_point() +
  xlab("Ergebnis Norwegisch") +
  ylab("ID Versuchsperson")

gridExtra::grid.arrange(p1, p2, ncol = 2)
@

Um die linke Grafik zu zeichnen, können Sie den unten stehenden Befehl
verwenden. Mittels des \#-Zeichens habe ich diesem Befehl mit \textbf{Kommentaren}
versehen. Diese erläutern, wie die Grafik aufgebaut ist.
Am Anfang Ihrer R-Karriere empfehle ich Ihnen, Ihren Code reichlich mit
Kommentaren auszustatten, sodass Sie ihn mehrere Wochen und Monate später
noch verstehen können. Mit der Zeit werden Sie Ihren R-Code immer besser
lesen können und dann reichen kargere Kommentare durchaus.
<<eval = FALSE>>=
ggplot(data = d,                # Datensatz mit den Variablen
       # aes() = aesthetics = welche Variable wie dargestellt werden soll
       aes(x = Norwegian,       # Variable auf x-Achse
           y = Participant)) +  # Variable auf y-Achse
  geom_point() +                # Daten als Punkte darstellen
  xlab("Ergebnis Norwegisch") + # insb. bei Arbeiten/Vorträgen/Artikeln:
  ylab("ID Versuchsperson")     #     Achsen beschriften
@

Achten Sie darauf, dass das \texttt{tidyverse}-Bündel
geladen ist: Auch wenn Sie es installiert haben und in einer anderen
Session verwendet haben, müssen Sie es in jeder Session erneut laden.
Achten Sie weiter auf Gross- und Kleinschreibung, auf die Klammern
und Kommas und auf die Pluszeichen. Mit Letzteren werden der Grafik
zusätzliche Schichten hinzugefügt. Mit dem
Befehl auf den ersten vier Zeilen (\texttt{ggplot(...)}) wird
lediglich die `Leinwand' der Grafik gezeichnet. Nach dem
Pluszeichen folgt der Befehl \texttt{geom\_point(...)},
der Punkte auf die Leinwand malt.
Mit den Befehlen \texttt{xlab(...)} und \texttt{ylab(...)} werden
Achsenbeschriftungen hinzugefügt bzw.\ überschrieben.
In einem \texttt{ggplot()}-Befehl werden die unterschiedlichen Schichten
mit einem +-Zeichen zusammengefügt, nicht mit einem \textit{pipe} (|>).

Um die rechte Grafik zu zeichnen, ersetzen Sie in der 4.\ Zeile
\texttt{y = Participant} durch \texttt{y = reorder(Participant, Norwegian)}
(Klammer nicht vergessen!).

\mypar[Code mit Stil]{Bemerkung}
Mit dem folgenden Code können Sie ebenfalls die Grafik links
zeichnen, denn Leerzeichen und Zeilenbrüche werden von R
mehrheitlich ignoriert.
<<eval = FALSE>>=
ggplot(data=d,aes(x=
Norwegian,y=Participant))+geom_point(
)+xlab("Ergebnis Norwegisch")+ylab("ID Versuchsperson")
@
Die erste Variante ist jedoch viel übersichtlicher, denn
die Struktur des Codes (inkl.\ Einrückungen) widerspiegelt
die logische Struktur des Befehls und die Leerzeichen
machen den Code lesbarer.
Versuchen Sie daher bereits am Anfang Ihrer R-Karriere,
einen übersichtlichen und konsistenten Codierstil zu pflegen,
etwa indem Sie meinen adoptieren oder sich an einer
Gestaltungsrichtlinie (z.B.\ \url{https://style.tidyverse.org/}) orientieren.
\parend

\mypar[Grafiken speichern]{Bemerkung}
Nachdem Sie eine Grafik mit \texttt{ggplot()}
erzeugt haben, können Sie diese mit dem Befehl
\texttt{ggsave()} speichern. Siehe hierzu \texttt{?ggsave}.

Es gibt aber eine allgemeinere Methode, die nicht nur bei von \texttt{ggplot()}
erzeugten Grafiken funktioniert. Um die linke Grafik aus Abbildung \ref{fig:dotchart}
zu speichern, können Sie den \texttt{ggplot()}-Befehlen zwischen die Befehle
\texttt{pdf()} und \texttt{dev.off()} zu stellen, wie folgt:
<<eval = FALSE>>=
pdf(here("figs", "dotchart.pdf"), 
    width = 5, height = 4) # Höhe und Breite in Zoll
ggplot(data = d,              
       aes(x = Norwegian,       
           y = Participant)) + 
  geom_point() +               
  xlab("Ergebnis Norwegisch") + 
  ylab("ID Versuchsperson")
dev.off()
@
Die Abbildung finden Sie jetzt als eine PDF-Datei
mit dem Namen \texttt{dotchart.pdf} im Unterordner \texttt{figs} in Ihrem Projektordner.

Wenn Sie die Grafik lieber in einem anderen Format speichern, können
Sie statt \texttt{pdf()} auch \texttt{svg()}, \texttt{png()}, \texttt{tiff()} oder
\texttt{bmp()} verwenden.

Für eine schnelle Grafik können Sie natürlich auch das \texttt{Export}-Menü
in der Registerkarte \texttt{Plots} im Fenster rechts unten in RStudio verwenden.
Aber ich empfehle Ihnen, den Gebrauch von \texttt{pdf()} zu umarmen, da Sie so
in Ihrem Code dokumentieren, mit welchen Einstellungen die Grafik gespeichert wurde.
Das ist nämlich sehr praktisch, wenn Sie später alle Grafiken mit leicht anderen
Einstellungen neu zeichnen müssen.
\parend

In diesem Beispiel ist das Punktdiagramm insbesondere nützlich
aufgrund von dem, was es eben nicht aufzeigt. Es scheint
nämlich keine Datenpunkte, oder Grüppchen von Datenpunkten, zu geben,
die ziemlich weit von den anderen entfernt liegen. Solche Datenpunkte,
die man \term{Ausreisser} nennt, können das Ergebnis einer Analyse
stark beeinflussen, sodass es wichtig ist, zu wissen, dass es sie gibt.
Ausreisser können (nicht müssen) auch auf technische
Fehler hinweisen und sollten also nochmals kontrolliert werden.
Abbildung \ref{fig:outlier} zeigt ein fiktives Beispiel, in dem
die Anzahl morphologischer Fehler pro Textseite pro Lerner aufgeführt wird.
Ein Datenpunkt liegt so weit von den anderen entfernt, dass man hier
auf jeden Fall nochmals kontrollieren sollte, ob die Angabe tatsächlich stimmt,
und nicht etwa auf einem Tippfehler bei der Dateneingabe beruht.
<<echo = FALSE, fig.cap = "Beispiel eines Ausreissers. Hier müsste man kontrollieren, ob der Datenpunkt (17.6) richtig eingetragen wurde und nicht etwa ein Tippfehler ist (statt 1.76).\\label{fig:outlier}", fig.height=2.5>>=
# fiktive Daten eintragen
df <- data.frame(fehler = c(1.35, 2.54, 17.6, 1.75, 1.98, 2.09, 2.43),
                 id = paste("Lerner", LETTERS[1:7]))
ggplot(data = df,
        aes(x = fehler,
            y = reorder(id, fehler))) +
   geom_point() +
   xlab("Fehler pro Seite") +
   ylab("ID Versuchsperson") +
   annotate(geom = "text",
            x = 17.6, y = 6.5,
             hjust = "right", vjust = "top",
             label = "Ausreisser:\nmöglicher Eingabefehler?")
@

\section{Das Histogramm}\label{sec:histogram}
Eine zweite nützliche Grafik ist das \term{Histogramm}.
Für ein Histogramm wird die Variable, die man darstellen
möchte, in Intervalle (\textit{bins}) aufgeteilt
und es wird gezählt, wie viele Beobachtungen es in jedem \textit{bin} gibt.
Diese Anzahlen werden in der Grafik als Bälkchen dargestellt,
siehe Abbildung \ref{fig:histogram}.
Wie in diesem Beispiel sind die \textit{bins}
in den allermeisten Histogrammen gleich breit.
Die Breite der \textit{bins} muss man selber festlegen;
wie die Befehle und Kommentare unten zeigen, ist dies
eine Frage von Ausprobieren.

Verglichen mit dem Punktdiagramm ist ein Vorteil
des Histogramms, dass auch sehr grosse Datensätze sinnvoll
dargestellt werden können, während man in einem Punktdiagramm
wohl schnell den Überblick verliert.
<<eval = FALSE>>=
ggplot(data = d,
       aes(x = Norwegian)) +
  # Defaulteinstellungen fürs Histogramm (immer 30 bins)
  geom_histogram()

ggplot(data = d,
       aes(x = Norwegian)) +
  # Anzahl 'bins' definieren
  geom_histogram(bins = 10)

ggplot(data = d,
       aes(x = Norwegian)) +
  # Binbreite definieren
  geom_histogram(binwidth = 3)

ggplot(data = d,
       aes(x = Norwegian)) +
  # Grenzen selbst festlegen, hier etwa bei 0, 4, 8, 12, 16.
  # Kürzel: seq(from = 0, to = 16, by = 4).
  # Die Farben kann man selbst auswählen.
  geom_histogram(breaks = seq(from = 0, to = 16, by = 4),
                 fill = "lightgreen",
                 colour = "darkgreen") +
  # Achsenbezeichnungen
  xlab("Ergebnisse cloze-Test Norwegisch") +
  ylab("Anzahl Studierende")
@

<<fig.cap = "Drei Histogramme mit den \\texttt{Norwegian}-Ergebnissen. \\textit{Links}: Die Grenzen zwischen den \\textit{bins} liegen bei 0, 1, 2, usw., 15, 16. \\textit{Mitte}: Grenzen bei 0, 2, 4, usw., 15, 16. \\textit{Rechts}: Grenzen bei 0, 4, 8, 12, 16. Sowohl die Grafik links als auch die in der Mitte halte ich hier für sinnvoll; die Grafik rechts ist nach meinem Geschmack ein bisschen zu grob. Eine goldene Regel für die Wahl der Breite der \\textit{bins} gibt es nicht. Daher gilt: Mit den Einstellungen herumspielen und eine nützliche Grafik auswählen.\\label{fig:histogram}", echo = FALSE, fig.width = 8, fig.height = 2, out.width="\\textwidth">>=
grenzen <- seq(from = 0, to = 16, by = 1)
p1 <- ggplot(data = d,
       aes(x = Norwegian)) +
  # Grenzen selbst festlegen, hier etwa bei
  # 0, 4, 8, 12, 16 und 20
  geom_histogram(breaks = grenzen,
                 fill = "lightgrey",
                 colour = "black") +
  xlab("Ergebnisse cloze-Test Norwegisch") +
  scale_x_continuous(breaks = seq(from = 0, to = 16, by = 2)) +
  scale_y_continuous(breaks = seq(0, 100, 2)) +
  ylab("Anzahl Studierende")

grenzen <- seq(from = 0, to = 16, by = 2)
p2 <- ggplot(data = d,
       aes(x = Norwegian)) +
  # Grenzen selbst festlegen, hier etwa bei
  # 0, 4, 8, 12, 16 und 20
  geom_histogram(breaks = grenzen,
                 fill = "lightgrey",
                 colour = "black") +
  xlab("Ergebnisse cloze-Test Norwegisch") +
  scale_x_continuous(breaks = grenzen) +
  scale_y_continuous(breaks = seq(0, 100, 2)) +
  ylab("Anzahl Studierende")

grenzen <- seq(from = 0, to = 16, by = 4)
p3 <- ggplot(data = d,
       aes(x = Norwegian)) +
  # Grenzen selbst festlegen, hier etwa bei
  # 0, 4, 8, 12, 16 und 20
  geom_histogram(breaks = seq(from = 0, to = 16, by = 4),
                 fill = "lightgreen",
                 colour = "darkgreen") +
  xlab("Ergebnisse cloze-Test Norwegisch") +
  scale_x_continuous(breaks = grenzen) +
  scale_y_continuous(breaks = seq(0, 100, 2)) +
  ylab("Anzahl Studierende")
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
@

\mypar{Aufgabe}
  Schauen Sie sich das mittlere Histogramm in Abbildung \ref{fig:histogram}
  genauer an. Dieses Histogramm verwendet acht Intervalle.
  Es handelt sich hierbei um halboffene Intervalle,
  d.h., genau eines der Endpunkte ist jeweils im Intervall eingeschlossen.
  Handelt es sich aber um die Intervalle
  \[
    (0, 2], (2, 4], \dots, (14, 16],
  \]
  in denen der linke Endpunkt nicht zum Intervall gehört,
  oder um die Intervalle
  \[
    [0, 2), [2, 4), \dots, [14, 16),
  \]
  in denen der rechte Endpunkt nicht zum Intervall gehört?
  (Hinweis: Schauen Sie sich auch Abbildung \ref{fig:dotchart} nochmals an.)
  
  Zeichnen Sie nun das Histogramm nochmals, aber so, dass der jeweils
  andere Endpunkt zum Intervall gehört. 
  Dazu können Sie der \texttt{geom\_histogram()}-Funktion
  einem weiteren Parameter \texttt{closed} übergeben. Schlagen Sie 
  auf der Hilfeseite dieser Funktion nach, welche die möglichen Werte
  für diesen Parameter sind.
\parend

In den Histogrammen bisher stand die Anzahl
Beobachtungen pro \textit{bin} auf der $y$-Achse.
Wenn man unterschiedliche Histogramme (z.B.\ von unterschiedlichen
Gruppen) vergleichen möchte, kann es sinnvoller sein, diese
Zahlen zu einer Art relative Frequenz umzurechnen.
Auch wenn die Bezeichnung in diesem Kontext nicht ganz stimmt,
werden wir diese \term{Wahrscheinlichkeitsdichten} nennen.
Diese werden so berechnet, dass die Gesamtfläche, die das Histogramm
einnimmt, 1 beträgt. Anders gesagt: Man nimmt die Höhe jedes \textit{bins},
also die Wahrscheinlichkeitsdichte, und multipliziert diese mit seiner Breite,
um die Oberfläche der Bälkchen zu berechnen. Die Höhe wird so festgelegt,
dass wenn man alle Oberflächen addiert, die Summe 1 beträgt. Die Form
des Histogramms ist aber gleich, egal ob man mit den Anzahlen oder den
Wahrscheinlichkeitsdichten arbeitet.

Abbildung \ref{fig:histogramdensity} zeigt ein Beispiel.
Die Breite jedes \textit{bins} in der rechten Grafik beträgt 4.
Die Höhen sind etwa 0.09, etwa 0.105, etwa 0.045 und fast 0.015, sodass
$(4 \cdot 0.09) + (4 \cdot 0.105) + (4 \cdot 0.045) + (4 \cdot 0.015) \approx 1$.
<<fig.cap = "Drei Histogramme mit der \\texttt{Norwegian}-Ergebnissen. Statt der absoluten Anzahl Beobachtungen pro \\textit{bin} stehen hier Wahrscheinlichkeitsdichten auf der y-Achse.\\label{fig:histogramdensity}", echo = FALSE, fig.width = 8.5, fig.height = 2.2, out.width="\\textwidth">>=
grenzen <- seq(from = 0, to = 16, by = 1)
p1 <- ggplot(data = d,
       aes(x = Norwegian,
           y = after_stat(density))) +
  # Grenzen selbst festlegen, hier etwa bei
  # 0, 4, 8, 12, 16 und 20
  geom_histogram(breaks = grenzen,
                 fill = "lightgrey",
                 colour = "black") +
  xlab("Ergebnisse cloze-Test Norwegisch") +
  scale_x_continuous(breaks = seq(from = 0, to = 16, by = 2)) +
  ylab("Wsk.-Dichte")

grenzen <- seq(from = 0, to = 16, by = 2)
p2 <- ggplot(data = d,
       aes(x = Norwegian,
           y =  after_stat(density))) +
  # Grenzen selbst festlegen, hier etwa bei
  # 0, 4, 8, 12, 16 und 20
  geom_histogram(breaks = grenzen,
                 fill = "lightgrey",
                 colour = "black") +
  xlab("Ergebnisse cloze-Test Norwegisch") +
  scale_x_continuous(breaks = grenzen) +
  ylab("Wsk.-Dichte")

grenzen <- seq(from = 0, to = 16, by = 4)
p3 <- ggplot(data = d,
       aes(x = Norwegian,
           y =  after_stat(density))) +
  # Grenzen selbst festlegen, hier etwa bei
  # 0, 4, 8, 12, 16 und 20
  geom_histogram(breaks = seq(from = 0, to = 16, by = 4),
                 fill = "lightgrey",
                 colour = "black") +
  xlab("Ergebnisse cloze-Test Norwegisch") +
  scale_x_continuous(breaks = grenzen) +
  ylab("Wsk.-Dichte")
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
@

Um die Grafiken in Abbildung \ref{fig:histogramdensity} selber zu zeichnen, ersetzen
Sie in den Befehlen oben
<<eval = FALSE>>=
ggplot(data = d,
       aes(x = Norwegian))
@
durch
<<eval = FALSE>>=
ggplot(data = d,
       aes(x = Norwegian,
           y = after_stat(density)))
@

\mypar[Wahrscheinlichkeitsdichten]{Aufgabe}
  Nehmen Sie an, wir transformieren die \texttt{Norwegian}-Ergebnisse
  aus Abbildung \ref{fig:histogramdensity}, indem wir diese Ergebnisse
  zu Prozentsätzen umgestalten: Ein Ergebnis von 10 entspricht einem
  Prozentsatz von 50; ein Ergebnis von 14 einem Prozentsatz von 70.
  Wir zeichnen ein Histogramm auf der Basis dieser Prozentsätzen
  und verwenden hierbei vier \textit{bins}, genau wie im rechten Histogramm
  in Abbildung \ref{fig:histogramdensity}. Das heisst, wir verwenden 
  vier Intervalle der Breite 25.
  Wie wird sich dies auf die Dichtewerte auswirken?
  Beantworten Sie diese Frage, ohne das Histogram zu zeichnen.
\parend

\section{Mittelwerte}
Es ist unpraktisch, in einem Bericht alle einzelnen
beobachteten Werte aufzulisten.\footnote{Aber es ist sehr sinnvoll,
den Datensatz online verfügbar zu stellen und im Bericht auf ihn
zu verweisen; siehe \citet{Klein2018} und \citet{Levenstein2018}! Eine benutzerfreundliche Website, wo Sie dies machen können, ist \url{https://osf.io}; siehe \citet{Soderberg2018} für eine Anleitung.} Wenn möglich probiert
man diese Informationen daher zu komprimieren, indem
man berichtet, welcher Wert typisch für diese Beobachtungen
ist und wie sehr die einzelnen Beobachtungen von diesem
typischen Wert abweichen.
Die Frage nach dem typischen Wert betrifft die \term{zentrale Tendenz}
der Beobachtungen und wird anhand von einem Mittelwert
beantwortet. Die Frage nach den Abweichungen betrifft die
\term{Streuung} der Beobachtungen und wird anhand von Streuungsmassen
beantwortet. Zuerst widmen wir uns den Mittelwerten.

\subsection{Das arithmetische Mittel}
Wenn man vom `Durchschnitt' oder `Mittelwert' spricht,
meint man meistens das (arithmetische) Mittel.
Eigentlich sind `Durchschnitt' und `Mittelwert' aber Hyperonyme,
denn es gibt ausser dem Mittel noch andere Durchschnittsmasse.

Um das Mittel einer endlichen Population zu berechnen, addiert
man alle $N$ Werte und teilt man die Summe durch die Anzahl Werte ($N$).
Das Populationsmittel kürzt man in Formeln meistens als $\mu$ ab.
In Gross-Sigmanotation schaut die Formel so aus:
\begin{equation*}
\mu = \frac{1}{N} \sum_{i = 1}^{N} x_i.
\end{equation*}

Wir können eine R-Funktion schreiben, die diese Formel umsetzt;
siehe auch Abschnitt \ref{sec:rprogramming}:
<<>>=
my_mean <- function(x) {
  N <- length(x)
  my_sum <- 0
  for (i in 1:N) {
    my_sum <- my_sum + x[i]
  }
  return(my_sum / N)
}
my_mean(d$Norwegian)
@
Für das Berechnen von Summen und Mitteln gibt es natürlich eingebaute R-Funktionen,
sodass wir diese selbst geschriebene Funktion gar nicht brauchen:
<<>>=
sum(d$Norwegian)
mean(d$Norwegian)
@

Die \texttt{tidyverse}-Lösung brauchen wir hier im Prinzip
nicht, aber sie zeigt nochmals, wie die \texttt{summarise()}-Funktion
funktioniert.
<<>>=
d |>
  summarise(mittel_norwegisch = mean(Norwegian))
@

Das Mittel hat ein paar nützliche mathematische Eigenschaften,
denen es seine Omnipräsenz verdankt. Eine erste praktische Eigenschaft
ist seine \term{Linearität}: Seien $X, Y$ zwei Vektoren der gleichen Länge $N$
und mit Mitteln $\mu_X$ bzw.\ $\mu_Y$ und $a, b$ zwei Konstanten.
Wir definieren nun einen dritten Vektor $Z$ der Länge $N$ durch
$Z := aX + bY$. Damit ist gemeint, dass $Z_i = aX_i + bY_i$ für $i = 1, \dots, N$.
Das Mittel von $Z$ ist nun gerade
\[
  \mu_Z = a\mu_X + b\mu_Y.
\]

Eine zweite praktische Eigenschaft ist die folgende. Sei $X$ ein Vektor der
Länge $N_X$ mit Mittel $\mu_X$ und $Y$ ein Vektor der Länge $N_Y$ mit Mittel
$\mu_Y$. Wir definieren einen dritten Vektor $Z$ der Länge $N_X + N_Y$, indem
wir $X$ und $Y$ aufreihen: $Z := (X_1, \dots, X_{N_X}, Y_1, \dots, Y_{N_Y})$.
Das Mittel von $Z$ ist nun das \term{gewichtete Mittel} der Mittel von $X$ und $Y$:
\[
  \mu_Z = \frac{N_X\mu_X + N_Y\mu_Y}{N_X + N_Y}.
\]

Eine dritte praktische Eigenschaft betrifft den zentralen Grenzwertsatz, den wir uns später anschauen.
Ein wesentlicher Nachteil des Mittels ist aber, dass er stark von Ausreissern beeinflusst wird.
Nimmt man zum Beispiel die Daten aus Abbildung \vref{fig:outlier} und berechnet man ihr Mittel,
dann ist das Ergebnis 4.25 -- ein ziemlich untypischer Wert für die Beobachtungen,
sind doch 6 der 7 Beobachtungen unter 2.6 und 1 über 17.5. Lässt man den Ausreisser weg,
ist das Mittel 2.02, was der Tendenz des Hauptanteils der Beobachtungen besser entspricht.

\subsection{Der Median}
Ein anderer beliebter Mittelwert ist der \term{Median}.
Es handelt sich hier buchstäblich um den Wert in der Mitte.
Genauer ist $m_X$ ein Median des Vektors $X$, wenn 
für mindestens der Hälfte der Werte $x$ von $X$ gilt, dass
$m_X \leq x$, und wenn für mindestens der Hälfte der Werte in $X$ gilt,
dass $m_X \geq x$.
Ein Vektor mit einer geraden Anzahl Einträgen kann mehrere
(sogar unendlich viele) Mediane haben.
In diesem Fall bezeichnet man als \emph{den} Median das Mittel des
grössten und des kleinsten solchen Wertes.

Zum Berechnen des Medians ordnet man die Daten
von klein nach gross und nimmt man den mittleren Wert.
Gibt es eine gerade Anzahl Beobachtungen, gibt es zwei
mittlere Werte. Diese Werte, sowie auch alle Zahlen zwischen ihnen,
sind Mediane. Um \emph{den} Median zu berechnen, nimmt man einfach das Mittel
beider mittlerer Werte.

Zuerst die komplizierte Berechnungsmethode,
um den Vorgang zu illustrieren: Mit \texttt{arrange()}
die Daten von klein nach gross ordnen und dann den mittleren Wert
nehmen. Da es 23 Beobachtungen gibt,
ist der 12.\ Wert der mittlere
(es gibt 11 kleinere und 11 grössere):
<<>>=
d |>
  select(Norwegian) |>
  arrange(Norwegian) |>
  slice(12)
@

Einfacher geht es mit \texttt{median()}:
<<>>=
median(d$Norwegian)
@

Mit der \texttt{summarise()}-Funktion können wir eine Zusammenfassungstabelle mit mehreren Werten aufstellen:
<<eval = TRUE>>=
d |>
  summarise(mittel_norwegisch = mean(Norwegian),
            median_norwegisch = median(Norwegian))
@

Der Median ist weniger ausreisserempfindlich als das Mittel. Berechnet
man für die Werte aus Abbildung \ref{fig:outlier} den Median mit
dem Ausreisser, ist das Ergebnis 2.09; ohne ist es 2.04.

Der Median ist kein linearer Operator. Beispielsweise ist der Median
von $X := (1, 2, 3)$ klar $2$, während der Median von $Y := (0, 1, 0)$
$0$ ist. Jedoch ist der Median von $X + Y = (1, 3, 3)$ $3 \neq 2+0$.
Es ist auch nicht möglich, anhand der Mediane von zwei Vektoren
den Median des zusammengelegten Vektors bestimmen.
Für den Median gibt es unter bestimmten Bedingungen eine (schwierigere)
Variante des zentralen Grenzwertsatzes; siehe \citet[][Satz 4.6]{Duembgen2016}.

Grosse Unterschiede zwischen dem Mittel und
dem Median sind öfters
Ausreissern oder asymmetrischen Verteilungen zuzuschreiben.
So oder so gilt: \textbf{Keine Mittelwerte berechnen,
ohne die Daten zuerst
grafisch darzustellen!}
Die Berechnung mag stimmen, aber unter Umständen ist sie nicht
\emph{sinnvoll}.

\subsection{Der Modus}
Den Modus trifft man weniger oft an, aber er ist eine ganz einfache
Art und Weise, um den `typischen Wert' zu definieren: Es handelt
sich schlicht und einfach um den Wert, der am häufigsten vorkommt.
Eine Modusfunktion gibt es nicht, aber wir können mit \texttt{count()} für jeden Wert
zählen, wie oft er vorkommt. Mit \texttt{arrange(desc(n))} sortieren
wir diese Anzahlen in absteigender Reihenfolge:
<<>>=
d |>
  count(Norwegian) |>
  arrange(desc(n))
@
Zwei Werte kommen am häufigsten vor: 5 und 6 kommen beide 4 Mal vor. Die Modi der Norwegischdaten sind also 5 und 6.

Ein wesentlicher Nachteil des Modus ist, dass bei feinkörnigen
Daten jede Beobachtung
eh nur ein oder zwei Mal vorkommt, sodass es nicht sinnvoll ist,
ihn zu berechnen.

\subsection{Andere Mittelwerte}
Es existieren noch andere Mittelwerte
z.B.\ das \term{harmonische Mittel},
das \term{geometrische Mittel},
das \term{winsorisierte Mittel},
das \term{getrimmte Mittel}
und das \term{gewichtete Mittel}.
Diese seien hier bereits der Vollständigkeit halber erwähnt,
werden aber noch nicht weiter erläutert.
Gerade das winsorisierte und das getrimmte Mittel
dienen vor allem dazu, das Mittel einer Population
zu schätzen, wenn man bloss mit einer Stichprobe arbeitet.

\mypar{Aufgabe}\label{ex:stocker}
Die Datei \texttt{stocker2017.csv} enthält einen Teil der Daten aus einer
  on-line-Studie von \citet{Stocker2017}.
  160 Versuchspersonen wurden gebeten, die Glaubwürdigkeit von Aussagen von
  SprecherInnen mit unterschiedlichen Akzenten (Englisch, Französisch, Deutsch und Italienisch)
  mithilfe eines \textit{sliders} auf einer Skala von 0 bis 100 zu bewerten.
  Diese Daten stehen in der \texttt{score}-Spalte.

    \begin{enumerate}
      \item Lesen Sie diese Datei in R ein. Kontrollieren Sie, ob dies geklappt hat.

      \item Berechnen Sie das Mittel und den Median der \texttt{score}-Daten.
      Sind sich diese Mittelwerte ähnlich?

      \item Stellen Sie die \texttt{score}-Daten in einem Histogramm mit 10 \textit{bins} dar.
      Beschreiben Sie die Form dieses Histogramms.

      \item Zeichnen Sie ein Histogramm mit 100 bins.
      Beschreiben Sie dieses Histogramm.
      Sind das Mittel und der Median gute Masse für die zentrale Tendenz in diesen Daten?

      \item Welcher Wert ist der dritthäufigste? Warum, denken Sie?

      \item Was ist bei den viert-, fünft-, sechst- usw. -häufigsten Werten auffällig? \parend
    \end{enumerate}


\mypar[harmonisches Mittel]{Aufgabe}
Es sei $x = (x_1, x_2, \dots, x_n)$ ein Vektor
    mit strikt positiven Zahlen. Das \textbf{harmonische Mittel}
    $H$ dieser Zahlen ist nun definiert als
    \[
      H = \frac{n}{\sum_{i=1}^n \frac{1}{x_i}}.
    \]
    
    Schreiben Sie eine eigene R-Funktion \texttt{harmonic\_mean()},
    welche einen Vektor mit einer beliebigen Anzahl strikt positiver
    Zahlen als Parameter erhält und sein harmonisches Mittel ausspuckt.
<<echo = FALSE>>=
harmonic_mean <- function(x) {
  H <- length(x) / (sum(1/x))
  return(H)
}
@

    Hinweis: Wenn Sie den ersten Kilometer gegen 5 Kilometer/Stunde
    überbrücken, den zweiten mit 10 Kilometern/Stunde
    und den dritten mit 2 Kilometern/Stunde, dann haben Sie insgesamt
    3 Kilometer in 48 Minuten überbrückt. (Man rechne nach.)
    Dies entspricht einer durchschnittlichen Geschwindigkeit von 3.75
    Kilometern/Stunde.
    Diese durchschnittliche Geschwindigkeit können
    Sie mit dem harmonischen Mittel berechnen. Wenn Ihre Funktion
    gut geschrieben ist, sollte der folgende Befehl die Antwort 3.75 ergeben:
<<>>=
harmonic_mean(c(5, 10, 2))
@
\parend

\section{Streuungsmasse}
Wenn man nur einen oder ein paar Mittelwerte berichtet,
bleibt die Frage unbeantwortet, wie stark die einzelnen
Beobachtungen davon abweichen.
Mit Streuungmassen versucht man diese Abweichnung numerisch
auszudrücken.

\subsection{Spannweite}
Ein einfaches Streuungsmass ist die \term{Spannweite}. Man berechnet
lediglich den niedrigsten und den höchsten Wert und berichtet diese oder
den Unterschied zwischen ihnen:
<<>>=
min(d$Norwegian)
max(d$Norwegian)
range(d$Norwegian)
@

Dieses Mass wird aus gutem Grund selten verwendet:
Es ist extrem ausreisserempfindlich.
Ausserdem unterschätzt die Spannweite einer
Stichprobe systematisch die Spannweite der Population,
aus der sie stammt.\footnote{Diese Tatsache scheint
übrigens in der Zweit\-sprachs\-erwerbs\-forschung
nicht allen Forschenden bekannt zu sein,
die diesem Streuungsmass eine zentrale Rolle in ihrer Forschung zuteilen
\citep{Vanhove2019b}.}
(Mit Stichproben beschäftigen wir uns in späteren Kapiteln.)

\subsection{Summe der Quadrate}\label{sec:sumsofsquares}
Wenn wir alle Beobachtungen ins Streuungsmass einfliessen lassen wollen,
scheint es auf den ersten Blick sinnvoll, die Unterschiede zwischen den beobachteten
Werten und dem Mittel zu berechnen und diese Unterschiede beieinander aufzuzählen:
$(x_1 - \mu) + (x_2 - \mu) + \dots$. Diese Summe ist aber immer 0.
Um das zu sehen, überlege man sich Folgendes:
\[
  (x_1 - \mu) + (x_2 - \mu) + \dots + (x_N - \mu) = (x_1 + x_2 + \dots + x_N) - N\mu.
\]
Nun wird $\mu$ berechnet als $\frac{1}{N}(x_1 + x_2 + \dots + x_N)$,
sodass $N\mu = x_1 + x_2 + \dots + x_N$. Daher gilt
\[
  (x_1 + x_2 + \dots + x_N) - N\mu = (x_1 + x_2 + \dots + x_N) - (x_1 + x_2 + \dots + x_N) = 0.
\]
Man kann auch direkt die Linearität des Mittels anwenden, um zum gleichen Schluss zu kommen.

Um dieses Problem zu lösen, werden die Unterschiede zwischen den beobachteten
Werten und dem Mittel quadriert, bevor sie beieinander aufgezählt werden.
Dadurch werden sie alle nicht-negativ, sodass ihre Summe nicht länger 0 ist.
Diese \term{Summe der Quadrate} wird in Formeln als $d^2$ oder $SS$ (\textit{sum of squares}) abgekürzt:
\begin{equation*}
d^2 = \sum_{i = 1}^{N} (x_i - \mu)^2.
\end{equation*}
<<>>=
sum((d$Norwegian - mean(d$Norwegian))^2)
@
Achten Sie auf die Stelle der Klammern und der Quadrierung: Die Unterschiede müssen quadriert werden, nicht die Summe oder das Mittel.
<<>>=
# Falsch: Hier wird die Summe quadriert.
sum((d$Norwegian - mean(d$Norwegian)))^2

# Falsch: Hier wird das Mittel quadriert.
sum((d$Norwegian - mean(d$Norwegian)^2))
@

Vielleicht fragen Sie sich, wieso man hier mit quadrierten Unterschieden arbeitet.
Wäre es nicht einfacher, mit den absoluten Unterschieden zu rechnen?
Streuungsmasse, die auf den absoluten Unterschieden basieren, gibt es tatsächlich,
aber das Arbeiten mit quadrierten Unterschieden bietet mathematische Vorteile
(z.B. zentralen Grenzwertsatz).

\mypar[Gleitkommazahlen]{Bemerkung}
Wir haben zwar gezeigt, dass immer
\[
  \sum_{i = 1}^N (x_i - \mu) = 0
\]
gilt.
Aber wenn wir diese Berechnung in R kontrollieren,
erhalten wir ein anderes Ergebnis:
<<>>=
sum(d$Norwegian - mean(d$Norwegian))
@
8.8818e-15 bedeutet $8.8818 \cdot 10^{-15}$, also 0.0000000000000088818.
Aber eigentlich
sollte das Ergebnis genau 0 sein, nicht \emph{fast} 0.
Das Problem liegt bei der Genauigkeit, mit der ein Computer mit Zahlen
umgeht. Wir werden auf dieses Problem hier nicht 
näher eingehen, da es für uns nicht von riesiger Bedeutung ist.
Jedoch ist dies ein Beispiel, wo das Testen auf Gleichheit mit \texttt{==}
nicht das gewünschte Ergebnis liefert, aber \texttt{all.equal()} schon:
<<>>=
sum(d$Norwegian - mean(d$Norwegian)) == 0
all.equal(sum(d$Norwegian - mean(d$Norwegian)), 0)
@
\parend

\subsection{Varianz}
Ein Problem mit $d^2$ ist, dass Datensätze unterschiedlicher Grösse nicht vergleichbar
sind: Je mehr Beobachtungen es gibt, desto grösser ist $d^2$. 
Das Mass $d^2$ drückt also sowohl
die Grösse des Datensatzes als auch die Streuung der Beobachtungen aus,
was unerwünscht ist. Die Lösung liegt auf der Hand: $d^2$ teilen durch die Anzahl Beobachtungen.
Dies ergibt die \term{Populationsvarianz} ($\sigma^2$):
\begin{equation}
\sigma^2 = \frac{1}{N} \sum_{i = 1}^{N} (x_i - \mu)^2.
\label{eq:popvar}
\end{equation}

In der Regel müssen wir die Varianz einer Stichprobe, nicht jene einer
Population berechnen. Diese wird leicht anders berechnet;
siehe Kapitel \ref{ch:stichproben}.

\mypar[Funktion für die Populationsvarianz]{Aufgabe}\label{ex:popvar}
In R gibt es keine eingebaute Funktion, um die Populationsvarianz 
einer endlichen Population zu berechnen.
Schreiben Sie eine Funktion \texttt{pop\_var()}, um diese Lücke zu füllen.
\parend

<<echo = FALSE>>=
pop_var <- function(x) {
  mean((x - mean(x))^2)
}
@


\subsection{Standardabweichung}
Varianzen sind nicht einfach zu interpretieren, da sie aufgrund
der Quadrierung in der Berechnung in quadrierten Einheiten
ausgedrückt werden (z.B.\ quadrierte Testergebnisse, quadrierte
Sprecher per Sprache). Wir können aber die Wurzel der Varianz nehmen,
was die Populationsstandardabweichung ergibt ($\sigma$):
\begin{equation*}
\sigma = \sqrt{\sigma^2} = \sqrt{\frac{1}{N} \sum_{i = 1}^{N} (x_i - \mu)^2}.
\end{equation*}

In R mit der selbst geschriebenen \texttt{pop\_var()}-Funktion (Aufgabe \ref{ex:popvar}):
<<>>=
pop_var(d$Norwegian) |>
  sqrt()
@

Standardabweichungen und Varianzen
kann man (wie Mittelwerte) nicht absolut interpretieren:
Eine Standardabweichung von 0.4 ist je nach der Art von Daten
klein, gross oder unauffällig, und dies gilt
auch für Standardabweichungen
von 8'000. So wäre etwa eine Standardabweichung
von 13 unauffällig, wenn es
sich um in Zentimetern gemessenen Körpergrössen
von Menschen handeln würde;
erstaunlich klein, wenn die Körpergrössen
in Millimetern ausgedrückt wären;
und ziemlich gross, wenn sie in Zoll ausgedrückt wären.

In der Regel müssen wir die Standardabweichung einer
Stichprobe, nicht jene einer
Population berechnen. Diese wird leicht anders berechnet;
siehe Kapitel \ref{ch:stichproben}.

\mypar[Daten nicht nur numerisch zusammenfassen!]{Bemerkung}
Stellen Sie sich vor, dass Sie in einer Studie lesen,
dass 39 Versuchspersonen eine Frage auf einer 6er-Skala
von 0 bis 5 beantwortet haben und das Mittel der
Antworten 2.43 betrug.
Vielleicht stellen Sie sich dann darunter vor, dass die
meisten Versuchspersonen sich für `2' oder `3' entschieden.
Dies muss aber nicht der Fall sein: Hinter diesem Mittelwert
können sich viele andere Datenmuster verstecken, die zu
anderen Schlussfolgerungen führen sollten.
Vielleicht sind sich die Versuchspersonen einig
in ihrer Gleichgültigkeit, weshalb sie alle Antworten
in der Mitte der Skala wählen. Oder vielleicht handelt
es sich um ein sehr kontroverses Thema mit überzeugten
Gegnern und Befürwortern aber ohne eine moderate Mitte.
Oder vielleicht sind alle Arten von Meinung etwas vertreten;
siehe Abbildung \ref{fig:samemean}.

<<fig.cap = "Hinter dem gleichen Mittelwert kann sich eine Vielzahl von unterschiedlichen Mustern verstecken. Diese drei Grafiken zeigen alle 39 Beobachtungen einer Variablen mit einem Mittel von 2.43 (nach Rundung).\\label{fig:samemean}", echo = FALSE, fig.width = 6, fig.height = 2, out.width = "\\textwidth">>=
op <- par(no.readonly = TRUE)
par(mfrow = c(1, 3), las = 1,
    oma = c(0, 0, 1, 0),
    bg = "white")

x <- c(20, 0, 0, 0, 0, 19)
names(x) <- 0:5
barplot(x, ylim = c(0, 40),
        xlab = "Antwort",
        ylab = "Anzahl")

x <- c(7, 6, 4, 8, 13, 1)
names(x) <- 0:5
barplot(x, ylim = c(0, 40),
        xlab = "Antwort",
        ylab = "Anzahl")

x <- c(0, 0, 22, 17, 0, 0)
names(x) <- 0:5
barplot(x, ylim = c(0, 40),
        xlab = "Antwort",
        ylab = "Anzahl")
title("Alle Grafiken: Mittel ≈ 2.43",
      outer = TRUE)
par(op)
@

Wenn ein Streuungsmass berichtet wird, schränkt
sich die Anzahl möglicher Muster zwar ein, aber trotzdem
können sich hinter einem Mittel und einer Standardabweichung
mehrere Verteilungen verstecken
(Abbildung \ref{fig:samemeansd}).\footnote{Diese Verteilungen wurden generiert anhand des R-Codes unter \url{http://bayesfactor.blogspot.ch/2016/03/how-to-check-likert-scale-summaries-for.html}.}\parend

<<fig.cap = "Sogar wenn man das Mittel und die Standardabweichung kennt, weiss man noch nicht, welches Muster sich hinter diesen Zahlen versteckt. In all diesen Grafiken beträgt das Mittel nach Rundung 2.43 und die Standardabweichung 1.05.\\label{fig:samemeansd}", echo = FALSE, fig.width = 6, fig.height = 4, out.width = "\\textwidth">>=
par(mfrow = c(2, 3), las = 1,
    oma = c(0, 0, 1, 0),
    bg = "white")

x <- c(0, 0, 33, 0, 1, 5)
names(x) <- 0:5
barplot(x, ylim = c(0, 40),
        xlab = "Antwort",
        ylab = "Anzahl")

x <- c(0, 10, 7, 18, 3, 1)
names(x) <- 0:5
barplot(x, ylim = c(0, 40),
        xlab = "Antwort",
        ylab = "Anzahl")

x <- c(1, 10, 2, 23, 3, 0)
names(x) <- 0:5
barplot(x, ylim = c(0, 40),
        xlab = "Antwort",
        ylab = "Anzahl")

x <- c(0, 9, 11, 12, 7, 0)
names(x) <- 0:5
barplot(x, ylim = c(0, 40),
        xlab = "Antwort",
        ylab = "Anzahl")

x <- c(5, 1, 5, 28, 0, 0)
names(x) <- 0:5
barplot(x, ylim = c(0, 40),
        xlab = "Antwort",
        ylab = "Anzahl")

x <- c(3, 4, 7, 24, 0, 1)
names(x) <- 0:5
barplot(x, ylim = c(0, 40),
        xlab = "Antwort",
        ylab = "Anzahl")

title("Alle Grafiken: Mittel ≈ 2.43, SD ≈ 1.05",
      outer = TRUE)
par(op)
@

\begin{framed}
\noindent \textbf{Merksatz!} Stellen Sie Ihre Daten auch grafisch dar, sodass Sie und Ihre Leserschaft
wissen, wie diese überhaupt aussehen. Mittelwerte und Streuungsmasse
erzählen nicht die ganze Geschichte.
\end{framed}


\section{Kerndichteschätzungen}
In unserem Norwegischbeispiel gibt es es nur eine geringe Anzahl Beobachtungen
und ausserdem ist die Variable nicht sehr feinkörnig.
Eine sehr feinkörnige Variable wäre eine Variable mit sehr vielen möglichen Ergebnissen
und höchstens einem Beleg pro möglichen Wert.

Was würde nun passieren, wenn
wir eine grosse Anzahl Beobachtungen (z.B.\ 100'000) von einer sehr feinkörnigen
Variablen erheben würden, diese Beobachtungen in einem
Histogramm mit Wahrscheinlichkeitsdichten (siehe Abschnitt \ref{sec:histogram}) darstellen würden
und die Anzahl \textit{bins} immer vergrössern würden?
Wenn die Anzahl \textit{bins} gross wird, können wir sie kaum mehr
mehr voneinander unterscheiden, wie Abbildung \ref{fig:density1} zeigt.
Ausserdem werden wir irgendwann nur noch \textit{bins}, die entweder
eine oder keine einzige Beobachtung beinhalten, haben.

<<fig.cap = "Vier Histogramme der gleichen feinkörnigen Variablen.\\label{fig:density1}", fig.width = 8, fig.height = 2, out.width="\\textwidth", echo = FALSE>>=
n <- 100000
df <- data.frame(x = c(rnorm(n/4, 15, 1),
                   rnorm(n/4, 10, 1),
                   rlnorm(n/4, 2, 0.15),
                   20*rbeta(n/4, 2, 6)))

p1 <- ggplot(df, aes(x = x,
                     y = after_stat(density))) +
  geom_histogram(bins = 10,
                 fill = "white",
                 colour = "grey") +
  ylab("Wsk.-Dichte") +
  ggtitle("10 bins")
p2 <- ggplot(df, aes(x = x,
                     y = after_stat(density))) +
  geom_histogram(bins = 25,
                 fill = "white",
                 colour = "grey") +
    ylab("Wsk.-Dichte") +
  ggtitle("25 bins")
p3 <- ggplot(df, aes(x = x,
                     y = after_stat(density))) +
  geom_histogram(bins = 50,
                 fill = "white",
                 colour = "grey") +
    ylab("Wsk.-Dichte") +
  ggtitle("50 bins")
p4 <- ggplot(df, aes(x = x,
                     y = after_stat(density))) +
  geom_histogram(bins = 100,
                 fill = "white",
                 colour = "grey") +
    ylab("Wsk.-Dichte") +
  ggtitle("100 bins")

gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 4)
@

In solchen Fällen arbeitet man stattdessen mit Kerndichtschätzungen.
Die Berechnungsmethode braucht uns hier nicht zu interessieren (es gibt mehrere); 
grundsätzlich handelt es sich um ein geglättetes Histogramm, bei dem die Wahrscheinlichkeitsdichten
jedes Bälkchens mit einer Kurve verbunden werden und die Bälkchen selber nicht mehr dargestellt werden; siehe Abbildung \ref{fig:density2}.

<<fig.cap = "Eine Kerndichteschätzung der gleichen Variablen wie in Abbildung \\ref{fig:density1}.\\label{fig:density2}", fig.width = 4, fig.height = 3, out.width=".4\\textwidth", echo = FALSE>>=
ggplot(df, aes(x = x)) +
  geom_density() +
    ylab("Wsk.-Dichte")
@

% Aber Vorsicht! Wahrscheinlichkeitsdichte ist nicht gleich Wahrscheinlichkeit.
% In Abbildung \ref{fig:density2} ist die
% Wahrscheinlichkeit, dass ein Wert von genau 10
% beobachtet wird, nicht fast 12\%, sondern verschwindend gering.
% Wenn man bloss genügend Dezimalstellen in Betracht zieht (z.B.\ 10.00000001 oder
% 9.9999999999), ist jeder einzelne Wert ja verschwindend unwahrscheinlich. Wir können
% deswegen keine sinnvollen Wahr\-schein\-lich\-keits\-aus\-sagen über spezifische Werte
% machen, sondern nur über Intervalle. Dies machen wir in den nächsten Kapiteln.

Eine Kerndichteschätzung können Sie mit dem Befehl \texttt{geom\_density()} zeichnen; 
siehe die Beispiele unter \url{https://ggplot2.tidyverse.org/reference/geom_density.html}.
Den Beispielen auf dieser Seite kann man entnehmen, dass man die Kerndichte
einer Variablen auf mehrere Arten schätzen kann, sodass
es nicht \emph{die} Kerndichteschätzung einer bestimmten
Variablen gibt. Vergleichen Sie dazu die erste, dritte und
vierte Grafik, die alle Kerndichteschätzungen der gleichen
Variablen darstellen.

\section{Verteilungs- und Quantilfunktionen}\label{sec:descriptive_cumprop}
Für einen numerischen Vektor $X$ drückt die \term{Verteilungsfunktion} $F_X$ 
für jede Zahl aus, welche Proportion der Einträge von $X$ nicht grösser als dieser
Wert ist. Das heisst,
\[
  F_X(r) := \frac{1}{N}\sum_{i = 1}^N \mathbb{1}(X_i \leq r),
\]
wobei $N$ die Anzahl Einträge in $X$ ist und $\mathbb{1}(X_i \leq r) = 1$,
falls $X_i \leq r$ und $\mathbb{1}(X_i \leq r) = 0$ sonst.

Abbildung \ref{fig:cumprop} zeigt die Verteilungsfunktion der norwegischen
Lesedaten ($F_N$, links) und der feinkörnigen Variablen $x$ aus dem letzten Abschnitt
($F_x$, rechts). Diese Abbildung wurde mit der Funktion \texttt{ecdf()} gezeichnet.
Obwohl man ähnliche Grafiken mit \texttt{ggplot2} zeichnen kann, zeichne ich
Verteilungsfunktionen lieber ohne, da \texttt{ggplot2} defaultmässig Verteilungsfunktionen
stetig (lückenlos) aussehen lässt. Die linke Verteilungsfunktion ist aber klar
nicht-stetig; auch die rechte Grafik ist nicht-stetig, nur sind die Lücken kleiner.
<<fig.cap = "Die Verteilungsfunktionen der norwegischen Lesedaten (links) und der feinkörningen Variablen aus Abbildung \\ref{fig:density2}.\\label{fig:cumprop}", fig.width = 6, fig.height = 3, out.width=".7\\textwidth", echo = FALSE>>=
par(mfrow = c(1, 2))
plot(ecdf(d$Norwegian),
     xlab = "r", ylab = bquote(F[N](r)), main = "")
segments(-10, 0.72, 7, col = "blue", lty = "dashed")
segments(7, 0.72, 7, 0, col = "blue", lty = "dashed")
plot(ecdf(df$x),
     xlab = "r", ylab = bquote(F[x](r)), main = "")
par(op)
@

Die \term{Quantilfunktion} $F_X^{-1}$ beantwortet die umgekehrte Frage.
Für eine Proportion $p \in (0, 1)$ ist $F_X^{-1}(p)$ der kleinste Wert $q$,
sodass $F_X(q) \geq p$. Die blauen Strichellinien in Abbildung \ref{fig:cumprop} (links) 
zeigen, dass 
der kleinste Wert, für den gilt, dass mindestens 72\% der Lesetestergebnisse
unter ihm liegen, 7 ist. Also $F_N^{-1}(0.72) = 7$.

In R gibt es zwar eine \texttt{quantile()}-Funktion, aber diese ist eigentlich
dafür gedacht, dass man anhand einer Stichprobe die Quantile der Population
schätzt. Wie Sie der Hilfeseite dieser Funktion entnehmen können, gibt
es verschiedene Möglichkeiten, wie diese Schätzung ausgeführt werden kann.
Der Vollständigkeit halber gebe ich hier eine R-Funktion an, die 
das $p$-te Quantil einer endlichen Population $x$ berechnet:
<<>>=
pop_quantile <- function(x, p) {
  x <- x[!is.na(x)]
  cum_prop <- cumsum(table(x)) / length(x)
  return(which(cum_prop > p)[1] |> names() |> as.numeric())
}
pop_quantile(d$Norwegian, 0.43)
pop_quantile(d$Norwegian, 0.72)
@

\section{Weiterführende Literatur}
\citet{Huff1954} (\textit{How to lie with statistics})
ist ein kurzes und sehr lesbares Büchlein. Es behandelt
unter anderem die unterschiedlichen Mittelwerte und wie
diese manipulativ eingesetzt werden.

In diesem Skript werden zwar mehrere nützliche Arten von
Grafiken vorgestellt, aber eine ausführlichere Behandlung
finden Sie bei \citet{Healy2019}. Dieses Buch ist auch
kostenlos verfügbar unter \url{https://socviz.co/}.