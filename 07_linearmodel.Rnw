\chapter{Ein anderer Blick aufs Mittel}\label{ch:linmod}
In diesem und den folgenden Kapiteln behandeln wir das \term{allgemeine lineare Modell}
(\textit{general linear model}).\footnote{Nicht zu verwechseln
mit dem \term{verallgemeinerten linearen Modell}
(\textit{generalized linear model}).
Dies ist eine Erweiterung des allgemeinen linearen Modells, 
mit der wir uns in diesem Kurs nur kurz befassen; 
siehe Kapitel \ref{ch:logistic}.}
Dies ist eine Methode,
um zu auszudrücken, wie ein oder mehrere \term{Prädiktoren}
mit dem \term{\textit{outcome}} zusammenhängen.
Oft redet man statt von Prädiktoren und outcomes
von unabhängigen bzw.\ abhängigen Variablen,
aber ich finde die Begriffe Prädiktor und outcome
deutlicher.

In diesem Kapitel werden einige Schlüsselkonzepte des
allgemeinen linearen Modells erläutert, indem wir das
Mittel einer Population auf eine andere Art und Weise
schätzen als wir es bisher gemacht haben.
In den darauf folgenden Kapiteln werden die
Modelle graduell komplexer, aber die Basisprinzipien
aus diesem Kapitel werden noch immer zutreffen.

\section{Ein Modell für die GJT-Daten}
Lasst uns kurz alles über Mittelwerte vergessen.
Wir erhalten Daten (hier: die GJT-Werte von \citet{DeKeyser2010},
siehe Kapitel \ref{ch:uncertainty}) und sollten
diese sinnvoll beschreiben. Sinnvoller als einfach
alle Datenpunkte aufzulisten, wäre, die Datenpunkte
in zwei Teile zu zerlegen: einen systematischen Teil, der die
Gemeinsamkeiten zwischen allen Werten ausdrückt,
und einen unsystematischen Teil, der die individuellen Unterschiede
zwischen diesen Gemeinsamkeiten und den Werten ausdrückt:
\[
\textrm{Wert einer Beobachtung} = \textrm{Gemeinsamkeit} + \textrm{Abweichung}.
\]
Um die Notation übersichtlich zu halten, wird diese Gleichung
meistens so geschrieben:
\begin{equation}\label{eq:beta0}
 y_i = \beta_0 + \varepsilon_i,
\end{equation}
für $i = 1, \dots, n$.
Hier ist $y_i$ die $i$-te Beobachtung im Datensatz,
$\beta_0$ stellt die Gemeinsamkeit zwischen allen
Werten in der Population dar
und $\varepsilon_i$ drückt aus, wie stark
die $i$-te Beobachtung von diesem Populationswert abweicht.
Die $\varepsilon_i$-Werte nennt man auch die \textbf{Residuen}
oder den \textbf{Restfehler}.
Wir schreiben $\beta_0$ statt einfach $\beta$,
weil wir nachher die Gemeinsamkeit
zwischen den $y$-Werten mithilfe mehrerer
$\beta$s ausdrücken werden.

In der Regel interessieren wir uns mehr für die $\beta$s
als für die $\varepsilon$s.
Da uns aber nicht die ganze Population
zur Verfügung steht, müssen wir uns mit einer Schätzung
von $\beta_0$ begnügen.
In Gleichung \ref{eq:beta0} ist $\beta_0$ ein Parameter
mit einem bestimmten, aber in der Regel unbekannten Wert;
für Schätzungen dieses Parameters wird die Notation
$\widehat{\beta}_0$ verwendet.
Da $\widehat{\beta}_0$ bloss eine Schätzung ist,
wird der Restfehler ebenfalls bloss geschätzt:
\[
  y_i = \widehat{\beta}_0 + \widehat{\varepsilon}_i,
\]

Äquivalent dazu kann man auch
\[
  \widehat{\varepsilon}_i = y_i - \widehat{\beta}_0,
\]
für $i = 1, \dots, n$, schreiben.

Wie können wir nun $\widehat{\beta}_0$ berechnen
(bzw.\ $\beta_0$ schätzen)?
Im Prinzip geht die Gleichung für jeden $\beta_0$-Wert
auf, denn wir können uns die $\widehat{\varepsilon}$-Werte
eben so aussuchen, wie es uns passt.
Die ersten beiden GJT-Werte im Datensatz sind
151 und 182. Falls wir nun für $\beta_0$ einen beliebigen
Wert wählen würden, z.B.\ 1823, 
so würden wir einfach $\widehat{\varepsilon}_1 = -1672$ und $\widehat{\varepsilon}_2 = -1641$ wählen und die Gleichung ginge auf:
\[
y_1 = 151 = 1823 - 1672,
\]
\[
y_2 = 182 = 1823 - 1641.
\]
Falls wir aber für $\beta_0$ den
Wert 14 wählen würden, 
so würden wir $\widehat{\varepsilon}_1 = 137$ und $\widehat{\varepsilon}_2 = 168$
wählen und die Gleichung ginge ebenfalls auf:
\[
y_1 = 151 = 14 + 137,
\]
\[
y_2 = 182 = 14 + 168.
\]
Wir brauchen also eine prinzipielle Methode, um
$\beta_0$ zu schätzen.

\section{Die Methode der kleinsten Quadrate}
Die Frage stellt sich, was die optimale Art und Weise
ist, um $\widehat{\beta}_0$ zu bestimmen.
Die wenig überraschende Antwort lautet: Es hängt davon
ab, was man unter `optimal' versteht.
Eine sinnvolle Definition von `optimal' ist,
dass $\beta_0$ so geschätzt werden soll, dass
die Summe der absoluten Restfehler
($\sum_{i = 1}^{n} |\widehat{\varepsilon}_i|$) möglichst klein ist.
Wenn wir für $\widehat{\beta}_0$ den Wert 135 wählen,
beträgt die Summe der absoluten Restfehler 1993.
Hier gehen wir davon aus, dass der Datensatz
\texttt{dekeyser2010.csv} den Objektnamen \texttt{d} hat.

<<>>=
sum(abs(d$GJT - 135))
@

Wenn wir stattdessen den Wert 148 wählen, beträgt die Summe der absoluten Restfehler 1799.
<<>>=
sum(abs(d$GJT - 148))
@

Wenn wir `optimal' so definieren, ist 148 also die bessere
Schätzung von $\beta_0$. Diese Übung können wir für jede Menge
Kandidatwerte durchprobieren und dann den optimalen Wert wählen.
Dies ist die \term{Methode der kleinsten absoluten Abweichungen}.
Wie Abbildung \ref{fig:optimisation} (links)
zeigt, sind $\beta_0$-Schätzungen
zwischen 150 und 151 in diesem Sinne optimal.
Nicht zufällig sind alle Werte im Intervall $[150, 151]$
Mediane der GJT-Werte; \emph{der} Median ist entsprechend 150.5.
Tatsächlich: Wenn man $\beta_0$
mit der Methode der kleinsten absoluten Abweichungen schätzt, ist
das Ergebnis ein Median der Stichprobe.
Den Beweis schenken wir uns; siehe \citet{Schwertman1990}.
Dass wir es hier mit einer Schätzung zu tun haben,
wird klar, wenn man sich überlegt, dass diese Methode
ein anderes Ergebnis liefern könnte, wenn man eine neue
Stichprobe aus der gleichen Population zieht.

<<echo = FALSE, fig.cap = "Links: Wenn der Parameter mit der Methode der kleinsten absoluten Abweichungen geschätzt wird, ist die Lösung gleich dem Median der Stichprobe. Rechts: Wenn der Parameter mit der Methode der kleinsten Quadrate geschätzt wird, ist die Lösung gleich dem Mittel der Stichprobe. \\label{fig:optimisation}", echo = FALSE, fig.width = 8, fig.height = 2.3, out.width=".9\\textwidth">>=
sum_of_squares <- function(x, m) {
  sum((x - m)^2)
}
df <- data.frame(beta0 = seq(145, 155, by = 0.01))
df$SS <- NA
for (i in 1:nrow(df)) {
  df$SS[i] <- sum_of_squares(x = d$GJT, m = df$beta0[i])
}
p1 <- ggplot(df,
       aes(x = beta0, y = SS)) +
  geom_line() +
  xlab(expression(widehat(beta[0]))) +
  ylab(expression(paste(Sigma, epsilon[i]^2))) +
  geom_vline(xintercept = mean(d$GJT), linetype = 2) +
  ggtitle("Methode der kleinsten\nQuadrate",
          "senkrechte Linie = Mittel der GJT-Werte")

sum_of_deviations <- function(x, m) {
  sum(abs(x - m))
}

df$SD <- NA
for (i in 1:nrow(df)) {
  df$SD[i] <- sum_of_deviations(x = d$GJT, m = df$beta0[i])
}

p2 <- ggplot(df,
       aes(x = beta0, y = SD)) +
  geom_line() +
  xlab(expression(widehat(beta[0]))) +
  ylab(expression(paste(Sigma, abs(epsilon[i])))) +
  geom_vline(xintercept = median(d$GJT), linetype = 2) +
    ggtitle("Methode der kleinsten absoluten\nAbweichungen",
          "senkrechte Linie = Median der GJT-Werte")

gridExtra::grid.arrange(p2, p1, ncol = 2)
@

Eine andere sinnvolle Definition von `optimal' ist,
dass $\beta_0$ so geschätzt werden soll, dass
die Summe der quadrierten Restfehler
($\sum_{i = 1}^{n} \widehat{\varepsilon}_i^2$) möglichst klein ist.
Dies ist die \textbf{Methode der kleinsten Quadrate}.
Verglichen mit der Methode der kleinsten absoluten Abweichungen
fallen grosse Residuen noch mehr ins Gewicht.
Anders gesagt: Grosse Abweichungen (darunter auch Ausreisser) üben
einen stärkeren Einfluss auf das Ergebnis aus.
Wie Abbildung \ref{fig:optimisation} (rechts) zeigt,
ist die optimal $\beta_0$-Schätzung laut der Methode
der kleinsten Quadrate 150.78 -- nicht zufällig das Mittel
der Stichprobe!

Während wir in Kapitel 3 die Summe der Quadrate als eine
Funktion des Mittels betrachtet haben, kann man die Rollen
auch umkehren: Das Mittel ist jener Wert, der die Summe
der Quadrate minimiert. Ebenso sind die Mediane jene Werte,
die die Summe der absoluten Abweichungen minimieren.
Die Modi sind übrigens die Werte, 
die die Summe der binären Abweichungen minimieren.
Sind $y_i$ und $\widehat{\beta}_0$ einander gleich, 
beträgt die binäre Abweichung 0, sonst 1.

Rechnerisch ist die Methode der kleinsten Quadrate am einfachsten,
und die Parameter in allgemeinen linearen Modellen werden daher
meistens mit dieser Methode geschätzt
(\textit{ordinary least squares}, OLS).
Aber dies ist keine Notwendigkeit.
In bestimmten Bereichen trifft man ab und zu Modellierungsverfahren an,
die andere Optimierungskriterien hantieren.

\mypar[Mittel minimiert die Summe der Quadrate]{Bemerkung}
  Dass das Stichprobenmittel die Summe der Quadrate minimiert, lässt sich 
  relativ einfach beweisen.
  
  \begin{proof}
  Die Summe der Quadrate ist eine Summe von quadratischen Funktionen in $\widehat{\beta}_0$
  und daher selber eine quadratische Funktion in $\widehat{\beta}_0$:
  \[
    \sum_{i=1}^n \widehat{\varepsilon}_i^2
    = \sum_{i=1}^n (y_i - \widehat{\beta}_0)^2
    = \sum_{i=1}^n y_i^2 - 2y_i\widehat{\beta}_0 + \widehat{\beta}_0^2.
  \]
  Aus der Schule wissen wir, dass eine quadratische Funktion dort ihr Minimum
  oder Maximum erreicht, wo ihre Ableitung 0 beträgt. Die quadratische Funktion,
  mit der wir es zu tun haben, hat einen positiven Leitkoeffizienten, also 
  erreicht sie nur ein Minimum. Wir berechnen die Ableitung:
  \begin{align*}
    \frac{\df}{\df \widehat{\beta}_0}  \sum_{i=1}^n \left(y_i^2 - 2y_i\widehat{\beta}_0 + \widehat{\beta}_0^2\right)
    &=  \sum_{i=1}^n \frac{\df}{\df \widehat{\beta}_0}\left(y_i^2 - 2y_i\widehat{\beta}_0 + \widehat{\beta}_0^2\right) \\
    &= \sum_{i=1}^n \left(-2y_i + 2\widehat{\beta}_0\right).
  \end{align*}
  Dieser Ausdruck ist genau dann gleich 0, wenn 
  \[
    \sum_{i=1}^n 2y_i = \sum_{i=1}^n 2\widehat{\beta}_0,
  \]
  also wenn
  \[
    \sum_{i=1}^n y_i = n\widehat{\beta}_0,
  \]
  also wenn
  \[
    \widehat{\beta}_0 = \frac{1}{n}\sum_{i=1}^n y_i.
  \]
  Dabei ist der letzte Ausdruck genau die Definition des Stichprobenmittels.
  \end{proof}

\section{Lineare Modelle in R}
Mit der \texttt{lm()}-Funktion können lineare Modelle aufgebaut werden.
Ihre Parameter werden anhand der Methode der kleinsten Quadrate geschätzt.
Innerhalb der Funktion braucht es eine Formel mit dem outcome
vor und den Prädiktoren nach der Tilde. In diesem Fall gibt es keinen
Prädiktor. Stattdessen wird \texttt{1} verwendet.

<<echo = FALSE>>=
options(digits = 7)
@


<<>>=
mod.lm <- lm(GJT ~ 1, data = d)
@

Die geschätzten $\beta$s kann man abrufen, indem
man den Namen des Modells (hier: \texttt{mod.lm})
eintippt.

<<>>=
mod.lm
@

Auch mit \texttt{coef()} erhält man die $\beta$-Schätzungen (hier nur $\beta_0$).

<<>>=
coef(mod.lm)
@

Dass mal 150.8 und mal 150.7763 angezeigt wird, liegt lediglich daran, dass das der Output
der letzten zwei Befehle strenger bzw.\ lockerer gerundet wird.

Mit \texttt{predict()} erhält man einen Vektor mit $n$ Werten (hier: $n = 76$),
der die `vorhergesagten' $y$-Werte enthält.
(Ich mag den Begriff `vorhergesagt' hier nicht.)
Es handelt sich um die $y$-Werte abzüglich der $\hat{\varepsilon}$-Werte: $\widehat{y_i} = y_i - \widehat{\varepsilon}_i$.
<<>>=
predict(mod.lm)
@
In unserem Fall sind dies lediglich 76 Wiederholungen von $\widehat{\beta}_0$. Der Grund ist dieser:
 \[
 y_i = \widehat{\beta}_0 + \widehat{\varepsilon}_i.
 \]
Also 
 \[
 \widehat{\varepsilon}_i = y_i - \widehat{\beta}_0, 
 \]
also 
 \[
 \widehat{y_i} = y_i - \widehat{\varepsilon}_i = y_i - (y_i - \widehat{\beta}_0) = \widehat{\beta}_0.
 \]

Die Residuen kann man mit der \texttt{resid()}-Funktion abfragen.

<<eval = FALSE>>=
# Output hier nicht gezeigt
resid(mod.lm)
@

\section{Unsicherheit in einem allgemeinen linearen Modell quantifizieren}

\subsection{Der Bootstrap}
Genauso wie im letzten Kapitel können wir den Bootstrap
verwenden, um die Unsicherheit in der Schätzung von $\beta_0$
zu quantifizieren. Da in diesem Fall $\widehat{\beta}_0$ gleich dem
Stichprobenmittel ist,
ergibt dies natürlich die gleiche Lösung wie vorher. Aber es gibt
mir die Gelegenheit, zu zeigen, wie man auch für komplexere
lineare Modelle den Bootstrap verwenden kann.

Die Logik ist wiederum, dass wir die Stichprobe stellvertretend
für die Population einsetzen. Aber anstatt Bootstrap-Stichproben
aus der Stichprobe zu ziehen, ziehen wir diesmal Bootstrap-Stichproben
aus den Residuen ($\widehat{\varepsilon}$). Diese kombinieren wir dann
mit $\widehat{\beta}_0$, um die Bootstrap-Stichproben zu generieren.
Wenn wir uns nur fürs Mittel interessieren, hat diese Methode
überhaupt keinen Mehrwert, denn sie ist mathematisch der zuerst
besprochenen Bootstrap-Methode gleich. Aber sie ist pädagogisch wertvoller
(und manchmal auch statistisch besser),
wenn wir später mehrere $\beta$s haben werden.
Konkret:
\begin{enumerate}\label{bootstrapoverview}
 \item Man berechnet $\widehat{\beta}_0$ und erhält dazu auch noch
 einen Vektor $\hat{\varepsilon}$, der die Werte
 $\widehat{\varepsilon}_1$ bis $\widehat{\varepsilon}_n$ enthält.
 \item Man zieht eine Bootstrap-Stichprobe aus $\hat{\varepsilon}$ (\textit{sampling with replacement}).
 Diese kann man als $\hat{\varepsilon}^{*}$ bezeichnen.
 Dieser Vektor enthält ebenso $n$ Werte,
 wobei bestimmte $\widehat{\varepsilon}_i$ eventuell
 nicht vorkommen, andere dafür mehrmals.
 \item Man kombiniert $\widehat{\beta}_0$ und $\hat{\varepsilon}^{*}$. Dies ergibt
 eine neue Reihe von $y$-Werten: $y_i^{*} = \widehat{\beta}_0 + \widehat{\varepsilon}_i^{*}$.
 \item Man schätzt nun auf der Basis von $y^{*}$ erneut den Parameter von Interesse ($\widehat{\beta}_0^{*}$).
 \item Man führt Schritte 2--4 ein paar tausend Mal aus und erhält so die Verteilung
 der gebootstrappten $\beta_0$-Schätzungen.
\end{enumerate}

Der unten stehende R-Code implementiert diese Schritte.
<<cache = TRUE>>=
n_bootstrap <- 20000
bs_b0 <- vector(length = n_bootstrap)
residuals <- resid(mod.lm)
predictions <- predict(mod.lm)

for (i in 1:n_bootstrap) {
  bs_residual <- sample(residuals, replace = TRUE)
  bs_outcome <- predictions + bs_residual
  bs_mod <- lm(bs_outcome ~ 1)
  bs_b0[i] <- coef(bs_mod)[1]
}
@

Wir können jetzt wieder die Verteilung der
gebootstrappten Parameterschätzungen visualisieren,
und ihre Standardabweichung und 2.5.\ und 97.5.\ Quantile
berechnen. Die Ergebnisse sind natürlich identisch
mit jenen aus Kapitel \ref{ch:uncertainty}.

<<eval = FALSE>>=
# Histogramm (nicht gezeigt)
hist(bs_b0)
@

<<>>=
sd(bs_b0)
quantile(bs_b0, probs = c(0.025, 0.975))
@

\subsection{Ein anderer Bootstrap}\label{semiparametricbootstrap}
Beim Bootstrap, den wir soeben besprochen haben,
sind wir davon ausgegangen, dass die Residuen
in der Stichprobe genau so verteilt sind wie die
Residuen in der Population. Ein Nachteil dieser
Annahme ist, dass wir dadurch die Feinkörnigkeit
der Residuen in der Population wohl unterschätzen.
Beispielsweise gibt es für das \texttt{mod.lm}-Modell
ein Residuum von $-14.78$ und ein Residuum von
$-12.78$, aber keines von $-13.78$.
Ein Residuum von $-14.78$ entspricht einer Beobachtung
von $150.78 - 14.78 = 136$; ein Residuum von $-13.78$
entspräche einer Beobachtung von $150.78 - 13.78 = 137$.
Nach unserer Annahme gäbe es in der Population also
keine Versuchspersonen mit einem GJT-Ergebnis von 137.

Dies ist natürlich eine etwas komische Annahme;
in der Regel hat sie aber kaum einen Einfluss auf die
Inferenzen. Aber eine Alternative wäre,
davon auszugehen, dass die Residuen in
der Population normalverteilt sind. Normalverteilungen
sind unendlich feinkörnig, sodass diese Annahme sozusagen
den Gegenpol der ersten Annahme darstellt.
Das Mittel der Residuen beträgt 0, sodass
wir nur die Standardabweichung der normalverteilten Restfehler
in der Population
finden müssen. Diese wird durch die Standardabweichung
der Restfehler in der Stichprobe geschätzt.
Dafür können wir hier zwei Funktionen verwenden:

<<>>=
sd(resid(mod.lm))
sigma(mod.lm)
@

In diesem Beispiel (lineares Modell ohne Prädiktoren)
sind diese Werte identisch, aber sobald Prädiktoren
im Spiel sind, liefert \texttt{sigma()} die bessere Schätzung
der Standardabweichung der Residuen. Sie wird so berechnet:
\begin{equation}\label{eq:sigmap}
\widehat{\sigma}_{\varepsilon} = \sqrt{\frac{1}{n - p} \sum_{i = 1}^{n} \widehat{\varepsilon}_i^2},
\end{equation}
wo $p$ die
Anzahl geschätzten $\beta$s ist.
In diesem Fall ist $p = 1$, sodass die Gleichung die
Stich\-proben\-standard\-ab\-weichung der Residuen ergibt:
<<>>=
sqrt(sum(resid(mod.lm)^2)/(length(resid(mod.lm)) - length(coef(mod.lm))))
@
Hier teilt man durch $n-p$ aus dem gleichen Grund,
weshalb man bei der Standardabweichung der Beobachtungen
in der einer Stichprobe durch $n-1$ teilt: um eine systematische
Unterschätzung zum grössten Teil entgegenzuwirken.

Anstatt für jede Bootstrapstichprobe die Residuen
durch \textit{sampling with replacement} zu generieren,
werden sie hier zufällig aus einer Normalverteilung
mit $\mu = 0$ und $\sigma = \widehat{\sigma}_{\varepsilon}$ generiert:

<<cache = TRUE>>=
n_bootstrap <- 20000
bs_b0 <- vector(length = n_bootstrap)
sd_residuals <- sigma(mod.lm)
predictions <- predict(mod.lm)

for (i in 1:n_bootstrap) {
  bs_residual <- rnorm(n = length(predictions), mean = 0, sd = sd_residuals)
  bs_outcome <- predictions + bs_residual
  bs_mod <- lm(bs_outcome ~ 1)
  bs_b0[i] <- coef(bs_mod)[1]
}
@

<<eval = FALSE>>=
# Histogramm (nicht gezeigt)
hist(bs_b0)
@


<<>>=
# geschätzter Standardfehler
sd(bs_b0)

# 95% Konfidenzintervall
quantile(bs_b0, probs = c(0.025, 0.975))
@

Diese Art von Bootstrap -- bei der wir davon ausgehen,
dass die Residuen eine bestimmte Verteilung haben,
und wir die relevanten Parameter dieser Verteilung
anhand der Stichprobe schätzen -- nennt man
einen \term{parametrischen Bootstrap}.
Den Bootstrap aus dem letzten Abschnitt -- bei der man
Bootstrapstichproben der Modellresiduen
mit den Modellvorhersagen kombiniert und die relevanten
Parameter anhand dieser neuen Werte schätzt -- nennt
man einen \term{semiparametrischen Bootstrap}.
Den Bootstrap aus dem letzten Kapitel -- bei der man
Bootstrapstichproben aus dem ursprünglichen Datensatz
generiert -- nennt man einen \term{nichtparametrischen Bootstrap}.

Man bemerke hier übrigens, dass sowohl die Annahme,
dass die Residuen in der Population genau so wie in der Stichprobe
verteilt sind, als auch die Annahme, dass sie normalverteilt und
unendlich feinkörnig sind, hier gar nicht stimmen können,
da bei dieser Studie nur Ganzzahlen zwischen 0 und 204 hätten
vorkommen können. Die Tatsache, dass wir unter unterschiedlichen
Annahmen zum gleichen Ergebnis kommen, deutet bereits darauf hin,
dass bei dieser Stichprobengrosse und bei dieser Art von Datenverteilung 
die Schätzung der Unsicherheit eines
Mittels nicht massgeblich von solchen Annahmen über die genaue Verteilung der
Residuen in der Population abhängt.

\mypar[Rundung im Bootstrap]{Bemerkung} Vielleicht möchten wir zwar die
Annahme treffen, dass die Residuen grob gesagt normalverteilt sind, aber nicht,
dass die Beobachtungen beliebig feinkörnig sein können. Mit dem unten stehenden
Code wird erneut ein parameterischer Bootstrap durchgeführt, aber die generierten Beobachtungen werden
auf Ganzzahlen gerundet. Ausserdem werden allfällige generierte Beobachtungen,
die ausserhalb des zulässigen Bereichs der \texttt{GJT}-Variablen liegen,
auf 204 bzw.\ auf 0 gesetzt.
<<cache = TRUE>>=
n_bootstrap <- 20000
bs_b0 <- vector(length = n_bootstrap)
sd_residuals <- sigma(mod.lm)
predictions <- predict(mod.lm)

for (i in 1:n_bootstrap) {
  bs_residual <- rnorm(n = length(predictions), mean = 0, sd = sd_residuals)
  bs_outcome <- predictions + bs_residual
  
  # runden + einschränken
  bs_outcome <- round(bs_outcome)
  bs_outcome[bs_outcome > 204] <- 204
  bs_outcome[bs_outcome < 0] <- 0

  bs_mod <- lm(bs_outcome ~ 1)
  bs_b0[i] <- coef(bs_mod)[1]
}

# geschätzter Standardfehler
sd(bs_b0)

# 95% Konfidenzintervall
quantile(bs_b0, probs = c(0.025, 0.975))
@

\parend

\subsection{Mit $t$-Verteilungen}
Wenn man ohnehin davon ausgehen will, dass die Residuen
normalverteilt sind, kann man den geschätzten
Standardfehler und das Konfidenzintervall
auch analytisch herleiten.
Die Formeln, die man dazu braucht, werden hier
nicht gezeigt, denn sie haben kaum einen didaktischen
Mehrwert.
In R kann man die \texttt{summary()}-Funktion
verwenden, um den geschätzten Standardfehler zu berechnen
(\texttt{Std. Error}):
<<>>=
summary(mod.lm)
@
$\beta_0$ heisst hier \texttt{(Intercept)}.
Was \texttt{t value} und \texttt{Pr(>|t|)} bedeuten,
werden wir erst in einem späteren Kapitel besprechen.

Das 95\%-Konfidenzintervall kann mit \texttt{confint()}
berechnet werden.
<<>>=
confint(mod.lm)
@

Natürlich kann man auch gerne andere Intervalle berechnen,
z.B.\ 80\%-Konfidenzintervalle:
<<>>=
confint(mod.lm, level = 0.80)
@

\mypar{Aufgabe}
Warum wird bei der \texttt{summary()}-Funktion
schon der Median aber nicht das Mittel der Residuen gezeigt?
\parend

\section{Exkurs: Maximum-Likelihood-Schätzung}\label{sec:ml}
Bei der Methode der kleinsten Quadrate (und Varianten darauf) erhält man die
Parameterschätzungen,
indem man die resultierenden geschätzten Restfehler minimiert.
Bei gewissen komplexeren Modellen
(zum Beispiel dem verallgemeinerten linearen Modell)
kann diese Schätzmethode nicht eingesetzt werden und verwendet man
stattdessen die sog.\ \term{Maximum-Likelihood-Methode} (oder eine Variante darauf).
Das Prinzip der Maximum-Likelihood-Schätzung soll hier nun anhand der GJT-Daten
erläutert werden.
Diesen Abschnitt können Sie vorübergehend ruhig überspringen,
bis Sie auf Ihrer Reise durch die Statistik einmal über die Maximum-Likelihood-Methode
stolpern.

Nehmen wir an, dass die Beobachtungen
$y_1, \dots, y_n$ i.i.d.\ normalverteilt sind mit Mittel $\mu$ und Varianz $\sigma^2$.
Die Wahrscheinlichkeitsdichte $p(y_i)$ einer solchen Beobachtung ist nun gegeben
durch
\[
  p(y_i) = \frac{1}{\sqrt{2\pi} \sigma}\exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right).
\]
Wegen der Unabhängigkeit der Beobachtungen ist die Wahrscheinlichkeitsdichte
$p(y_1, \dots, y_n)$
des Zufallsvektors $(y_1, \dots, y_n)$ gleich dem Produkt der einzelnen
Wahrscheinlichkeitsdichten.
(Dies wurde in diesem Skript zwar nicht gezeigt, aber es ergibt sich aus
Definition \vref{def:unab2}.)
Also
\begin{equation}\label{eq:mle}
  p(y_1, \dots, y_n) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi} \sigma}\exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right).
\end{equation}
Wenn wir diesen Ausdruck als eine Funktion von $\mu$ auffassen, so nennen
wir sie die \term{Likelihood-Funktion}.
Eine sinnvolle Art und Weise, $\mu$ anhand der Daten zu schätzen,
ist, jenen Wert $\widehat{\mu}$ zu wählen, der die Likelihood-Funktion maximiert.
Das Produkt macht dies etwas mühsam, aber wir können den Logarithmus nehmen
und das Produkt als Summe umschreiben, denn $\log ab = \log a + \log b$.
Dies ergibt die \term{log-Likelihood-Funktion}.
Da der Logarithmus streng monoton wächst, maximiert jener Wert,
der die log-Likelihood-Funktion maximiert, auch die Likelihood-Funktion.
Folglich bestimmen wir $\widehat{\mu}$ wie folgt:
\begin{align*}
  \widehat{\mu}
  &= \argmax_{\mu} \left(\prod_{i = 1}^n \frac{1}{\sqrt{2\pi} \sigma}\exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right)\right)
  & [\textrm{maximiere Likelihood-Funktion}]\\
  &= \argmax_{\mu} \left(\prod_{i = 1}^n \exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right)\right)
  & [\textrm{Konstanten spielen keine Rolle}]\\
  &= \argmax_{\mu} \log\left(\prod_{i = 1}^n \exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right)\right)
  & [\textrm{Logarithmus}]\\
  &= \argmax_{\mu} \sum_{i = 1}^n \log\left(\exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right)\right)
  & [\textrm{Produkt als Summe umschreiben}]\\
  &= \argmax_{\mu} - \sum_{i = 1}^n \frac{(y_i - \mu)^2}{2\sigma^2}
  & [\log(\exp x) = x]\\
  &= \argmax_{\mu} -\sum_{i = 1}^n (y_i - \mu)^2
  & [\textrm{Konstanten spielen keine Rolle}]\\
  &= \argmin_{\mu} \sum_{i = 1}^n (y_i - \mu)^2.
  & [\textrm{$-x$ maximieren = $x$ minimieren}]
\end{align*}

Wir erhalten also das genau gleiche Optimierungsproblem wie bei der Methode
der kleinsten Quadrate!
Die Lösung ist folglich auch dieselbe:
\[
  \widehat{\mu} = \frac{1}{n} \sum_{i = 1}^n y_i.
\]

Allgemeiner gilt beim allgemeinen linearen Modell,
dass wir die gleichen $\beta$-Schätzungen erhalten,
egal ob wir die Methode der kleinsten Quadrate verwenden (ohne Annahmen
über die Verteilung der Daten zu machen)
oder die Maximum-Likelihood-Methode verwenden (unter der Annahme
von i.i.d.\ normalverteilten Daten).
Bei gewissen komplexeren Modellen kann man die Methode der kleinsten
Quadrate nicht heranziehen, aber die Maximum-Likelihood-Methode schon.

\section{Fazit}
\begin{itemize}
\item Datenpunkte kann man als eine Kombination einer
Gemeinsamkeit aller Datenpunkte in der Population und einer
spezifischen Abweichung verstehen.
\item In der Regel ist die Gemeinsamkeit von Interesse;
diese wird aber anhand der Abweichungen und eines
Optimierungskriteriums geschätzt.
\item Das am meisten verwendetete Optimierungskriterium
ist die Methode der kleinsten Quadrate, die im `univariaten' Fall
(= wenn man nur mit einer Variablen arbeitet) das Mittel ergibt,
aber es existieren auch andere Methoden.
Die Maximum-Likelihood-Methode unter Annahme von i.i.d.\ normalverteilten Daten
ergibt die gleiche Lösung wie die Methode der kleinsten Quadrate.
\item Die Unsicherheit in der Parameterschätzung kann
mithilfe des Bootstraps oder anhand weiterer Annahmen geschätzt werden.
\end{itemize}