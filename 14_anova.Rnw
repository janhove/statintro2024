\chapter{Varianzanalyse}\label{ch:anova}
Mit einem Zweistichproben-$t$-Test kann man zwei Gruppen miteinander vergleichen.
Wenn wir statt zwei nun mehrere Gruppen
mithilfe von Signifikanztests
hinsichtlich eines outcomes vergleichen möchten,
läge es auf der Hand, mehrere $t$-Tests (oder Alternative dazu, wie Randomisierungstests) einzusetzen.
Wenn man zum Beispiel drei Gruppen vergleichen
möchte, könnte man zuerst Gruppen A und B vergleichen,
dann Gruppen A und C und zu guter Letzt Gruppen B und C.
Das Problem mit diesem Vorgehen ist, dass
die Wahrscheinlichkeit, \emph{irgendeinen}
signifikanten Unterschied zu finden, wenn die
Nullhypothese tatsächlich stimmt, grösser als $\alpha$
(in der Regel: $\alpha = 0.05$) ist.
Die Simulationen weiter unten stellen
dieses Problem unter Beweis. Danach folgen mögliche
Lösungen.

<<echo = FALSE>>=
set.seed(123456789)
@

Generieren wir zuerst Daten,
für die wir wissen, dass die Nullhypothese buchstäblich
stimmt. Wir gehen hier zwar von \textit{random sampling} aus,
aber auch wenn wir von \textit{random assignment} ausgingen,
würde sich am Problem und an der Lösung nichts ändern.
Mit dem folgenden Befehl wird ein Datensatz (tibble)
namens \texttt{d} kreiert. Dieser enthält
60 Beobachtungen von je zwei Variablen: \texttt{gruppe}
(drei Ausprägungen mit je 20 Beobachtungen)
und \texttt{ergebnis}. Letztere Variable wurde
aus einer Normalverteilung generiert, deren Eigenschaften
unabhängig von \texttt{gruppe} sind. Die Nullhypothese
stimmt also. Abbildung \ref{fig:anova_zufallsdaten}
stellt diese Daten grafisch dar.
<<cache = TRUE>>=
d <- tibble(
  gruppe = rep(c("A", "B", "C"), times = 20),
  ergebnis = rnorm(n = 3*20, mean = 10, sd = 3)
)
@

<<echo = FALSE, out.width=".6\\textwidth", cache = TRUE, fig.width = .8*6, fig.height = .8*2, fig.cap= "Zufällig generierte Daten: drei Zufallsstichproben von je 20 Beobachtungen aus der gleichen Normalverteilung. Bei Ihnen werden diese Daten natürlich anders aussehen.\\label{fig:anova_zufallsdaten}">>=
ggplot(d,
       aes(x = gruppe,
           y = ergebnis)) +
  geom_boxplot(outlier.shape = NA) +
  geom_point(shape = 1,
             position = position_jitter(width = 0.2, height = 0)) +
  xlab("Gruppe") +
  ylab("Ergebnis")
@

Wir könnten nun diesen Datensatz drei Mal aufspalten:
Ein Mal behalten wir nur die Daten aus den Gruppen
A und B, ein Mal behalten wir nur die Daten aus Gruppen
A und C, und ein Mal nur die Daten aus Gruppen B und C.
Wir führen dann jedes Mal einen $t$-Test nach Student aus.
Bei Ihnen werden der Output anders aussehen, da
die Daten zufällig generiert wurden.
Was den R-Code betrifft, bemerke man die
Verwendung von \texttt{\%in\%} sowie
des Platzhalters \texttt{\_}.
Mit dem letzteren übergibt man das tibble,
das man vor dem vorigen \texttt{|>} kreiert hat,
einem bestimmten Parameter in einer nächsten
Funktion (hier dem \texttt{data}-Parameter
in der \texttt{t.test()})-Funktion).
<<>>=
# A vs. B
d |>
  filter(gruppe %in% c("A", "B")) |>
  t.test(ergebnis ~ gruppe, data = _, var.equal = TRUE)

# A vs. C
d |>
  filter(gruppe %in% c("A", "C")) |>
  t.test(ergebnis ~ gruppe, data = _, var.equal = TRUE)

# B vs. C
d |>
  filter(gruppe %in% c("B", "C")) |>
  t.test(ergebnis ~ gruppe, data = _, var.equal = TRUE)
@

Wenn man sich in die Logik des Signifikanztestens
einschreibt, dann müsste man aufgrund dieser Ergebnisse
die Nullhypothese, dass die Daten in jeder Gruppe
aus der gleichen Verteilung stammen, ablehnen,
denn für den Vergleich zwischen Gruppen A und B
findet man ja einen $p$-Wert,
der kleiner als $\alpha = 0.05$ ist ($p = 0.017$).
Dies obwohl die Nullhypothese in dieser Simulation
buchstäblich stimmt.
Dass man Nullhypothesen, die tatsächlich stimmen,
ab und zu zu Unrecht ablehnt, ist klar -- und das
ist hier auch nicht das Problem. Vielmehr ist das Problem,
dass diese Fehlerquote (Fehler der ersten Art) angeblich
höchstens $\alpha = 0.05$ beträgt, aber eigentlich
wesentlich höher ist.

Diese Tatsache können wir mit einer Simulation illustrieren.
Mit den folgenden Befehlen wird das gleiche Szenario wie oben
(3 Gruppen mit je 20 Beobachtungen; Nullhypothese stimmt;
3 $t$-Tests) 5'000 durchlaufen. Jedes Mal wird der $p$-Wert
der einzelnen $t$-Tests gespeichert. Am Ende wird der 
jeweils kleinste der drei $p$-Werte berechnet.
<<cache = TRUE>>=
n_runs <- 5000
pval_ab <- vector(length = n_runs)
pval_ac <- vector(length = n_runs)
pval_bc <- vector(length = n_runs)

for (i in 1:n_runs) {
  sim_df <- tibble(
    gruppe = rep(c("A", "B", "C"), times = 20),
    ergebnis = rnorm(n = 3*20, mean = 10, sd = 3)
  )

  pval_ab[i] <- t.test(ergebnis ~ gruppe, data = sim_df |> 
                         subset(gruppe %in% c("A", "B")))$p.value

  pval_ac[i] <- t.test(ergebnis ~ gruppe, data = sim_df |> 
                         subset(gruppe %in% c("A", "C")))$p.value
  
  pval_bc[i] <- t.test(ergebnis ~ gruppe, data = sim_df |> 
                         subset(gruppe %in% c("B", "C")))$p.value
}

# kleinsten der 3 p-Werte auslesen
min_pval <- pmin(pval_ab, pval_ac, pval_bc)
@

Wenn die Nullhypothese stimmt, so müsste für jedes $\alpha \in (0,1)$ gelten,
dass höchstens $100\alpha$\% der $p$-Werte unter $\alpha$ liegen (Definition \ref{def:pvalue}).
Wenn Sie Histogramme der $p$-Werte der einzelnen
$t$-Tests zeichnen, werden Sie feststellen, dass
dies tatsächlich der Fall ist: Die $p$-Werte sind -- für diese Art von Daten und für diesen Test -- gleichverteilt auf $[0,1]$. 
Die Abweichungen von dieser Gleichverteilung, die Sie feststellen werden, sind zufallsbedingt;
wenn Sie die Anzahl Simulationen erhöhen, werden diese Abweichungen kleiner.
Wenn Sie nun aber die Verteilung der jeweils
kleinsten der drei $p$-Werte (\texttt{min\_pval})
zeichnen, werden Sie feststellen, dass diese \emph{nicht}
gleichverteilt sind; siehe Abbildung
\vref{fig:anova_pvalues}.

Wenn wir uns bei unseren Inferenzen darüber, ob sich die drei Gruppen voneinander
unterscheiden, nach dem kleinsten der drei $p$-Werte richten, würden wir nicht
in bloss höchstens $\alpha =$ 5\% der Fälle die Nullhypothese zu Unrecht ablehnen,
sondern in mehr als 13\% der Fälle, wenn drei Gruppen verglichen werden:
<<>>=
mean(min_pval <= 0.05)
@

 Dieser Befund ist allgemeingültig.
 Wenn man mehrere Signifikanztests verwendet, um eine Nullhypothese
 zu überprüfen, steigt die Wahrscheinlichkeit, dass mindestens einer
 von ihnen einen signifikanten $p$-Wert produziert -- auch wenn die Nullhypothese
 stimmt und jeder Test korrekt ausgeführt wurde.
 Die Wahrscheinlichkeit, dass man mindestens einen signifikanten $p$-Wert
 antrifft, wenn die Nullhypothese stimmt, nennt man die \term{familywise Type-I error rate};
 siehe Abbildung \vref{fig:familywise}.

 In Fällen wie im obigen Beispiel ist die Tatsache, dass man eine Nullhypothese
 mit mehreren Tests überprüft hat (\term{multiple comparisons}),
 allen klar. Ausserdem kann das Problem in solchen Fällen relativ leicht behoben
 werden. In anderen Fällen sind die \textit{multiple comparisons} weniger einfach
 aufzudecken bzw.\ schwieriger bei der Interpretation der Ergebnisse zu berücksichtigen --
 zum Beispiel, weil im Forschungsbericht nur eine Auswahl der ausgeführten Signifikanztests
 beschrieben wird. Siehe hierzu Kapitel \ref{ch:QRP}.

<<echo = FALSE, warning = FALSE, message = FALSE, out.width="\\textwidth", fig.width = 6*1.4, fig.height = 1.5*1.4, fig.pos = "tp", fig.cap = "Wenn die Nullhypothese stimmt, sollte der $p$-Wert aus einer Gleichverteilung stammen. Für die $p$-Werte aus den einzelnen $t$-Tests ist dies auch der Fall: Die Wahrscheinlichkeit, dass man einen $p$-Wert kleiner als $\\alpha = 0.05$ beobachtet, liegt tatsächlich bei 5\\%, wenn die Nullhypothese stimmt. Wenn man aber auf der Basis des jeweils kleinsten der drei $p$-Werte schliesst, ob sich die Gruppen unterscheiden, dann sind die relevanten $p$-Werte nicht gleich- sondern rechtsschief verteilt: Die Wahrscheinlichkeit, dass man \\emph{irgendeinen} $p$-Wert kleiner als 0.05 beobachtet, ist erheblich höher als 5\\%, auch wenn die Nullhypothese stimmt.\\label{fig:anova_pvalues}">>=
cols <- RColorBrewer::brewer.pal(3, "Set1")
par(mfrow = c(1, 4), las = 2, mar = c(4, 4, 3, 1) + 0.1)
hist(pval_ab, main = "Vergleich AB", xlab = "p",
     col = cols[2], freq = TRUE, ylab = "Anzahl",
     breaks = seq(0, 1, 0.1))
hist(pval_ac, main = "Vergleich AC", xlab = "p",
     col = cols[2], freq = TRUE, ylab = "Anzahl",
     breaks = seq(0, 1, 0.1))
hist(pval_bc, main = "Vergleich BC", xlab = "p",
     col = cols[2], freq = TRUE, ylab = "Anzahl",
     breaks = seq(0, 1, 0.1))
hist(min_pval, main = "Jeweils kleinster\nder 3 p-Werte", xlab = "min p",
     col = cols[1], freq = TRUE, ylab = "Anzahl",
     breaks = seq(0, 1, 0.1))
par(op)
@

<<echo = FALSE, out.width = ".7\\textwidth", fig.width = 6, fig.height = 3, fig.pos = "tp", fig.cap = "Wenn alle Nullhypothesen stimmen und mehrere unabhängige Signifikanztests durchgeführt werden, dann wird die Wahrscheinlichkeit, dass mindestens ein Test ein signifikantes Ergebnis produziert immer grösser ($y = 1 - (1 - 0.05)^\\textrm{Anzahl Tests}$). Diese Wahrscheinlichkeit ist der \\textit{familywise Type-I error rate}. Zwei Tests sind unabhängig voneinander, wenn das Ergebnis des einen Tests überhaupt keine Indizien darüber gibt, was das Ergebnis des anderen Tests sein wird.\\label{fig:familywise}">>=
n_tests <- seq(1, 30, by = 1)
error_rate <- 1 - 0.95^n_tests
df_error_rate <- tibble(n_tests, error_rate)
ggplot(df_error_rate,
       aes(x = n_tests,
           y = error_rate)) +
  geom_point() +
  geom_line() +
  xlab("Anzahl unabhängiger Tests") +
  ylim(0, 1) +
  ylab("Fehler der 1. Art (Wsk.)") +
  geom_hline(yintercept = 0.05, linetype = 2) +
  annotate(geom = "text",
           x = 15, y = 0.10,
           label = "angeblicher Fehler der 1. Art (α = 0.05)") +
  annotate(geom = "text",
           x = 15, y = 0.60, angle = 18,
           label = "tatsächlicher Fehler der 1. Art")
@

\mypar{Aufgabe}
  \begin{enumerate}
    \item Es seien $P_1, P_2, P_3$ unabhängige Zufallsvariablen mit
    einer Gleichverteilung auf $[0,1]$. Berechnen Sie die Wahrscheinlichkeit,
    dass mindestens eine dieser Zufallsvariablen einen Wert von höchstens $0.05$
    annimmt.
    
    Hinweis: Siehe Abbildung \ref{fig:familywise}. Gefragt ist übrigens eine
    Berechnung, keine Simulation oder Ähnliches.
    
    \item Der Wert, den Sie berechnet haben, ist grösser als $0.1364$.
    Dies liegt nicht daran, dass wir diese Zahl anhand einer Simulation geschätzt
    haben. Wieso unterscheiden sich diese Zahlen dann schon? \parend
  \end{enumerate}


\section{Erste Lösung: Randomisierungstest}
Die einfachste Art und Weise, das \textit{multiple comparisons}-Problem
zu lösen, ist, die Nullhypothese mit einem einzigen Test zu überprüfen
statt mit mehreren Tests. Am häufigsten wird hierzu Varianzanalyse (\textsc{anova}: \textit{analysis of variance})
bzw.\ der $F$-Test eingesetzt (siehe nächsten Abschnitt). Aber das Gleiche
kann man bewirken mit einem Randomisierungstest, der m.E.\ einfacher nachvollziehbar ist.
Wenn Sie sich Aufgabe \ref{ex:randtest_multiple} zur Brust genommen haben,
haben Sie diesen Test bereits implementiert. Der Deutlichkeit halber besprechen
wir diesen Ansatz hier nochmals.

Der Randomisierungstest geht von der gleichen Nullhypothese wie in Kapitel \ref{ch:logik} aus:
Die Versuchspersonen (oder was auch immer beobachtet wurde) wurden nach dem Zufallsprinzip
den Gruppen zugeordnet und die Unterschiede zwischen den Gruppenmitteln, die wir hier
gerade berechnen, sind lediglich
das Ergebnis dieser zufälligen Zuordnung.
<<>>=
d |>
  group_by(gruppe) |>
  summarise(mittel = mean(ergebnis))
@

Die entscheidende Frage ist nun:
Wie wahrscheinlich wäre es, dass sich die Mittel so stark
oder noch stärker voneinander unterscheiden würden, wenn diese Nullhypothese tatsächlich stimmt?
Um diese Frage zu beantworten, müssen wir zuerst in einer Zahl ausdrücken, wie stark
sich diese drei Mittel voneinander unterscheiden.
Die Varianz der Gruppenmittel bietet sich hierfür an.
<<>>=
d |>
  group_by(gruppe) |>
  summarise(mittel = mean(ergebnis)) |>
  select(mittel) |>
  var()
@

Die Varianz der Stichprobenmittel beträgt also 1.282.
Bei Ihnen wird das Ergebnis aufgrund der zufällig
generierten Daten anders aussehen.
Um die Verteilung der Varianzen der Stichprobenmittel
unter Annahme der Nullhypothese zu generieren,
können wir die Beobachtungen ein paar tausend Mal
zufällig den Gruppenbezeichnungen zuordnen
und jedes Mal die Varianz der Stichprobenmittel
der permutierten Daten berechnen.
Die Logik ist identisch mit jener der Permutationstests
aus dem letzten Kapitel, nur schauen wir uns die Varianz
der Gruppenmittel statt des Unterschieds zwischen den
zwei Gruppenmitteln an.

<<echo = FALSE>>=
set.seed(123456789)
@

<<cache = TRUE>>=
m <- 19999
var_mittel_H0 <- vector(length = m)

# Der Datensatz d wird hier als d_H0 kopiert,
# sodass er nachher nicht überschrieben wird.
d_H0 <- d

for (i in 1:m) {
  d_H0$gruppe <- sample(d_H0$gruppe)
  var_mittel_H0[i] <- tapply(d_H0$ergebnis, d_H0$gruppe, mean) |> var()
}
@

<<fig.width = 1.2*3, fig.height = 1.2*2, echo = FALSE, fig.cap = "Die Verteilung der Varianzen der Stichprobenmittel, wenn die Beobachtungen zufällig den Gruppen neu zugeordnet werden. Die Proportion der Varianzen, die mindestens so gross wie die beobachtete Varianz sind, stellt ein gültiger $p$-Wert dar.\\label{fig:distributionvaranova}">>=
df_var_H0 <- tibble(var_mittel_H0) |> 
  add_row(var_mittel_H0 = 1.282355)
ggplot(df_var_H0,
       aes(x = var_mittel_H0)) +
  geom_histogram(fill = "grey", col = "black",
                 binwidth = 0.10) +
  xlab("Varianz der Gruppenmittel unter H0") +
  ylab("Anzahl") +
  geom_vline(xintercept = 1.282355,
             colour = "red", linetype = 2) +
  annotate("text", x = 2, y = 1050, label = "s² > 1.28",
           angle = 0)
@

Die Verteilung dieser Varianzen (\texttt{var\_mittel\_H0})
wird in Abbildung \ref{fig:distributionvaranova} dargestellt.
Wir berechnen den entsprechenden $p_r$-Wert, also einen rechtsseitigen $p$-Wert.\footnote{Der linksseitigen $p$-Wert wird
in dieser Situation recht selten berechnet, denn sie passt zur Alternativhypothese, 
dass sich die Gruppenmittel \emph{weniger} voneinander 
abweichen als unter der Nullhypothese zu erwarten wäre. Eine Anwendung
des linksseitigen $p$-Wertes ist, Geschummel aufzudecken \citep{Simonsohn2013}.}

<<>>=
(sum(var_mittel_H0 >= 1.282355) + 1) / (m + 1)
@

In diesem Fall also $p = 0.062$.
Von Durchführung zu Durchführung wird sich dieser Wert
leicht ändern, da er auf `nur' 20'000 der fast
578 Quadrillionen möglicher Permutationen basiert.\footnote{Für die Interessierten:
Es gibt ${{60}\choose{20}} \approx 4.2 \cdot 10^{15}$ Möglichkeiten, um 20 Versuchspersonen
aus einer Gruppe von 60 zu wählen. Dann gibt es noch ${{40}\choose{20}} \approx 1.4 \cdot 10^{11}$ Möglichkeiten,
um 20 Versuchspersonen aus der restlichen Gruppe von 40 zu wählen. Zusammen ergibt das etwa
$4.2 \cdot 10^{15} \cdot 1.4 \cdot 10^{11} \approx 5.78 \cdot 10^{26}$ Möglichkeiten.}
Aber die Unterschiede werden minimal sein.

\section{Zweite Lösung: Varianzanalyse und $F$-Tests}
Randomisierungstests waren anno dazumals zu schwierig,
um durchzuführen. Ähnlich wie im letzten Kapitel gibt es
aber mathematische Kniffe, mit denen man mit weniger sturem Rechenaufwand
eine Annäherung erhält. Diese Tricks
heissen \term{Varianzanalyse}
(\textsc{anova}: \textit{analysis of variance})
und der \term{$F$-Test}. Da Forschungsartikel oft voller
\textsc{anova}s und $F$-Tests stehen, werden
diese Tricks hier vorgestellt.

\subsection{Streuungszerlegung}
Der erste Schritt in einer \textsc{anova}
besteht darin, dass man die Streuung im
outcome in zwei Teile zerlegt: Streuung zwischen
den Gruppen und Streuung innerhalb der Gruppen.
Dazu berechnet man zuerst die \term{Gesamtquadratsumme}
(vgl.\ Quadratsumme auf Seite \pageref{sec:sumsofsquares}).
Bei Ihnen werden diese Zahlen anders aussehen, da
die Daten zufällig generiert wurden.
<<>>=
# Gesamtquadratsumme
sq.total <- sum((d$ergebnis - mean(d$ergebnis))^2)
sq.total
@

Um die Streuung innerhalb der Gruppen,
die \term{Residuenquadratsumme}, zu berechnen, kann
man von allen Beobachtungen ihr Gruppenmittel abziehen.
Oder man modelliert die Daten in einem allgemeinen
linearen Modell und berechnet die Quadratsumme der
Residuen:
<<>>=
d.lm <- lm(ergebnis ~ gruppe, data = d)
sq.rest <- sum((resid(d.lm))^2)
sq.rest
@

Die vom Modell erfasste Quadratsumme beträgt also:
<<>>=
sq.gruppe <- sq.total - sq.rest
sq.gruppe
@

Wir haben, zusätzlich zum Intercept, zwei Parameter gebraucht,
um den Einfluss des Gruppenfaktors zu modellieren. Dies können
Sie kontrollieren, indem Sie das Modell \texttt{d.lm} inspizieren.
Im Schnitt beschreibt jeder zusätzliche Parameter also eine Quadratsumme von 25.6:
<<>>=
meanSq.gruppe <- sq.gruppe / 2
meanSq.gruppe
@

Es gibt 60 unabhängige Datenpunkte und das Modell schätzt
bereits 3 Parameter (Intercept + 2 Parameter für die
Gruppenunterschiede). Daher könnten wir im Prinzip
noch 57 Parameter schätzen lassen. Dies wäre zwar nicht
sinnvoll, aber möglich wäre es. Mehr Parameter zu schätzen,
ginge nicht: In einem allgemeinen linearen Modell kann man
nur so viele Parameter schätzen als es Beobachtungen gibt.
Im Schnitt würden diese 57 Parameter eine Quadratsumme
von 8.87 erfassen:
<<>>=
meanSq.rest <- sq.rest / (60 - 2 - 1)
meanSq.rest
@

Die kritische Einsicht ist nun, dass wenn die Nullhypothese
stimmt, die zwei Gruppenparameter im Schnitt nur die gleiche
Quadratsumme wie die 57 nicht-modellierten Parameter beschreiben
können. Also: Wenn $H_0$ zutrifft, sollten \texttt{meanSq.gruppe}
und \texttt{meanSq.rest} einander gleich sein. Eine andere
Art und Weise, um dies auszudrücken, ist, dass das Verhältnis von
\texttt{meanSq.gruppe} und \texttt{meanSq.rest} laut der Nullhypothese im Schnitt 1
sein soll:
\[
H_0: \mathbb{E}\left(\frac{\texttt{meanSq.gruppe}}{\texttt{meanSq.rest}}\right) = 1.
\]
Dieses Verhältnis bezeichnet man als $F$.
In unserem Fall beträgt $F$ 2.89.
<<>>=
f.wert <- meanSq.gruppe / meanSq.rest
f.wert
@

\subsection{Der $F$-Test}
Aufgrund des Stichprobenfehlers wird $F$ nie ganz genau 1 sein.
Die nächste Frage ist daher: Wie ungewöhnlich wäre ein $F$-Wert von 2.89 oder grösser,
wenn die Nullhypothese stimmt?
Wenn der Restfehler i.i.d.\ normalverteilt ist,
dann hat dieser $F$-Wert eine bestimmte Verteilung, nämlich eine \term{$F$-Verteilung}.

\mypar[$F$-Verteilung]{Definition}
  Es seien $X_1, \dots, X_k, X_{k+1}, \dots, X_{k + \ell} \sim \mathcal{N}(0, 1)$
  unabhängige Zufallsvariablen. Dann heisst die Verteilung von
  \[
    \frac{\frac{1}{k}\sum_{i=1}^{k} X_i^2}{\frac{1}{\ell}\sum_{i = k+1}^{k + \ell}X_i^2}
  \]
  die $F$-Verteilung mit $k$ und $\ell$ Freiheitsgraden; man schreibt $F_{k, \ell}$
  oder $F(k, \ell)$.
\parend

$F$-Verteilungen haben zwei Parameter (`Freiheitsgrade'):
die Anzahl Parameter, die man brauchte, um den Effekt zu modellieren (hier: 2),
und die Anzahl Beobachtungen, die man nicht aufgebraucht hat (hier: 57).
Abbildung \ref{fig:distf} zeigt eine $F(2, 57)$-Verteilung.
<<echo = FALSE, fig.width = 1.4*3, fig.height = 1.4*2, fig.cap = "Die $F$-Verteilung mit 2 und 57 Freiheitsgraden. Die Strichellinie stellt den beobachteten $F$-Wert dar. Die eingefärbte Fläche entspricht dem dazu gehörigen $p$-Wert.\\label{fig:distf}">>=
ggplot(data.frame(x = c(0, 5)),
       aes(x)) +
  stat_function(fun = function(x) df(x, 2, 57)) +
  xlab("F-Wert") +
  ylab("Wsk.-Dichte") +
  ggtitle("F(2, 57)-Verteilung") +
  geom_vline(xintercept = f.wert, linetype = 2) +
  stat_function(fun = function(x) df(x, 2, 57),
                xlim = c(f.wert, 5), geom = "area",
                fill = "steelblue")
@

Die Fläche unter der Kurve rechts von der Strichellinie
ist die Wahrscheinlichkeit, dass man einen $F$-Wert von
2.89 oder mehr beobachtet, wenn die Nullhypothese tatsächlich stimmt.
Sie kann so berechnet werden:
<<>>=
pf(f.wert, df = 2, df2 = 60 - 2 - 1, lower.tail = FALSE)
@

Dies ist fast das gleiche Ergebnis wie beim Permutationstest.

\subsection{Varianzanalyse in R}
Glücklicherweise muss man all diese Schritte nicht von Hand
ausführen. Es reicht, die Daten in einem allgemeinen linearen
Modell zu modellieren (was oben bereits gemacht wurde)
und die \texttt{anova()}-Funktion aufs Modell anzuwenden:
<<>>=
anova(d.lm)
@

Berichten würde man dieses Ergebnis wie folgt: $F(2, 57) = 2.89$, $p = 0.064$.

\mypar[$t$- vs.\ $F$-Test]{Aufgabe}
In Abschnitt \ref{sec:money_model} auf Seite \pageref{sec:money_model}
wurde ein echter Datensatz mit zwei Gruppen vorgestellt.
Testen Sie diesen Gruppenunterschied mit einem üblichen Zweistichproben-$t$-Test
und mit einer \textsc{anova}. Quadrieren Sie den $t$-Wert und ziehen Sie einen Merksatz.
\parend

\section{Varianzanalyse mit mehreren Prädiktoren}
Varianzanalyse kann man auch anwenden, wenn
es mehrere Prädiktoren in einem Modell gibt.
Die Daten aus der Dragan/Luca-Studie (siehe Seite \pageref{sec:berthele2011b}) kann
man wie folgt in einer \textsc{anova} auswerten:
<<message = FALSE>>=
b11 <- read_csv(here("data", "berthele2011.csv"))
b11.lm1 <- lm(Potenzial ~ CS * Name, data = b11)
anova(b11.lm1)
@

Jede Zeile in der Tabelle zeigt, wie
viele Parameter für einen bestimmten Prädiktor
modelliert werden mussten (\texttt{Df})
und was die assoziierte Quadratsumme
und seine $F$- und $p$-Werte sind.
Aber Vorsicht: Wenn wir die Reihenfolge
der Prädiktoren ändern, ändern sich ein paar
Zahlen:
<<>>=
b11.lm2 <- lm(Potenzial ~ Name * CS, data = b11)
anova(b11.lm2)
@

Der Grund hierfür ist, dass bei der Berechnung von \texttt{Sum Sq}
sequenziell vorgegangen wird.
Wenn wir die Varianzanalyse von Hand ausführen würden,
würden wir:
\begin{enumerate}
 \item die Gesamtquadratsumme berechnen;
<<>>=
(ss.gesamt <- sum((b11$Potenzial - mean(b11$Potenzial))^2))
@
 \item den Effekt der ersten Variable rausrechnen und berechnen,
 wie gross die Quadratsumme ist, die diese erfassen kann;
<<>>=
cs.lm <- lm(Potenzial ~ CS, data = b11)
(ss.cs <- ss.gesamt - sum(resid(cs.lm)^2))
@
 \item den Effekt der zweiten Variable rausrechnen und berechnen,
  wie gross die Quadratsumme ist, die diese erfassen kann;
<<>>=
name.lm <- lm(Potenzial ~ CS + Name, data = b11)
(ss.name <- ss.gesamt - ss.cs - sum(resid(name.lm)^2))
@
 \item den Interaktionseffekt rausrechnen und berechnen,
 wie gross die Quadratsumme ist, die dieser erfassen kann;
<<>>=
interaktion.lm <- lm(Potenzial ~ CS + Name + CS:Name, data = b11)
(ss.interaktion <- ss.gesamt - ss.cs - ss.name -
    sum(resid(interaktion.lm)^2))
@
 \item die restliche Quadratsumme berechnen;
<<>>=
(ss.rest <- ss.gesamt - ss.cs - ss.name - ss.interaktion)
@
 \item $F$-Werte für alle Effekte berechnen.
<<>>=
ss.cs / (ss.rest/151)
ss.name / (ss.rest/151)
ss.interaktion / (ss.rest/151)
@
\end{enumerate}

Je nachdem, ob wir \texttt{CS} oder \texttt{Name} als erste Variable
ins Modell eintragen, ändert sich die Quadratsumme, die dieser
Variable zugeschrieben wird. Der Grund dafür ist, dass
die Variablen \texttt{CS} und \texttt{Name} in diesem Datensatz
etwas miteinander korreliert sind: Es gibt mehr Datenpunkte für
Dragan ohne Codeswitches als mit und mehr Datenpunkte für Luca
mit Codeswitches als ohne:
<<>>=
xtabs(~ CS + Name, data = b11)
@

Ein Teil der Streuung in den \texttt{Potenzial}-Werten
kann daher nicht eindeutig dem einen oder dem anderen
Prädiktor zugewiesen werden. Wird \texttt{CS} als erste
Variable dem Modell hinzugefügt, dann wird all die Streuung,
für die es nicht ganz klar ist, ob sie \texttt{CS}
oder \texttt{Name} zu verdanken ist,
der Variable \texttt{CS} zugeschrieben; wird \texttt{Name}
als erste Variable dem Modell hinzugefügt, wird all diese
Streuung ihr zugeschrieben. Daher ändern sich die Ergebnisse
für diese Variablen, je nachdem, welche Variable zuerst hinzugefügt
wurde. Am Ergebnis für den letzten Effekt (die Interaktion) ändert
sich jedoch nichts.

\mypar[\textit{Type-I, Type-II, Type-III sum of squares}]{Bemerkung}
 Wenn die Quadratsummen und die von ihnen abgeleiteten $F$- und $p$-Werte
 sequenziell berechnet werden (wie oben),
 spricht man von \textit{Type-I sum of squares}.
 Diese Berechnungsart ist insbesondere dann sinnvoll, wenn man sich für den
 als letzten modellierten Effekt interessiert,
 aber zuerst noch ein paar andere
 Variablen berücksichtigt.

 Eine alternative Berechnungsart wird auf typierend kreative
 Weise \textit{Type-II sum of squares}
 genannt. Hierzu werden die Effekte in allen möglichen Reihenfolgen ins Modell eingetragen
 (aber Interaktionen kommen immer nach Haupteffekten) und gilt als der $F$- bzw.\ $p$-Wert eines
 Effektes jener $F$- bzw.\ $p$-Wert, den man antrifft, wenn der Effekt als letzter eingetragen wurde.
 In unserem Beispiel sähe das Ergebnis wie folgt aus:

 \begin{itemize}
  \item \texttt{CS}: Man übernimmt die Angaben für \texttt{CS} aus der \textsc{anova}-Tabelle
  für Modell \texttt{b11.lm2}, da hier \texttt{CS} als letzter Haupteffekt eingetragen wurde:
  $F(1, 151) = 1.9$, $p = 0.17$.

  \item \texttt{Name}: Man übernimmt die Angaben für \texttt{Name} aus der \textsc{anova}-Tabelle
  für Modell \texttt{b11.lm1}, da hier \texttt{Name} als letzter Haupteffekt eingetragen wurde:
  $F(1, 151) = 0.52$, $p = 0.47$.

  \item Die Interaktion kommt immer nach den Haupteffekten, weshalb ihr Ergebnis in beiden
  Modellen gleich ist: $F(1, 151) = 11.4$, $p < 0.001$.
 \end{itemize}

 Diese Ergebnisse kann man automatisch mit der \texttt{Anova()}-Funktion (mit grossem A)
 aus dem \texttt{car}-Package berechnen:
<<>>=
car::Anova(b11.lm1)
@

 Es gibt auch noch die Berechnungsart \textit{Type-III sum of squares},
 aber von dieser wird meistens abgeraten. \textit{Type-I} und \textit{Type-II sum of squares}
 können beide je nach der Situation nützlich sein, und wenn man sich nur für die Interaktion
 interessiert oder wenn man es genau die gleiche Anzahl Beobachtungen pro `Zelle' gibt,
 macht es eh nichts aus.
 Schmeissen Sie aber bitte keine Daten weg,
um so gleich grosse Zellen zu erhalten und nicht zwischen diesen
Berechnungsarten wählen zu müssen!
\parend

\mypar[Randomisierungstest]{Aufgabe}
  Testen Sie die Nullhypothese, dass die Faktoren \texttt{CS} und \texttt{Name}
  nur aufgrund der zufälligen Zuordnung
  miteinander interagieren, anhand eines eigens gestrickten Randomisierungstests.
\parend

\mypar[Begriffe]{Bemerkung} In der Forschungsliteratur haben sich einige 
Kürzel eingebürgert, um unterschiedliche Arten von Varianzanalyse voneinander 
zu unterscheiden.

\begin{description}
 \item[One-way \textsc{ANOVA} (einfaktorielle Varianzanalyse).]
 Eine Varianzanalyse mit einem
 Prädiktor. Meistens ist dies eine Gruppenvariable
 mit zwei oder mehreren Ausprägungen.

 \item[Two-way \textsc{ANOVA} (zweifaktorielle Varianzanalyse).]
 Eine Varianzanalyse mit zwei
 Prädiktoren. Oft ist hierbei die Interaktion zwischen den
 Prädiktoren von Interesse. Wenn die eine Variable zwei Ausprägungen
 hat und die andere vier, spricht man oft von einer
 \textit{$2 \times 4$ two-way \textsc{anova}}. \textit{Three-way},
 \textit{four-way}, usw., \textsc{anova} gibt es auch: Man fügt
 dem Modell einfach mehr Prädiktoren hinzu.

 \item[\textsc{ANCOVA}.] Steht für \textit{analysis of covariance}.
 Es handelt sich lediglich um eine Varianzanalyse, in der mindestens eine
 kontinuierliche Kontrollvariable berücksichtigt wird.
 % Für weiterführende Literatur, siehe \citet{Huitema2011}.

 \item[\textsc{RM-ANOVA}.] Steht für \textit{repeated-measures analysis of variance}.
 Dieses Vorgehen kann man verwenden, wenn etwa mehrere Datenpunkte der gleichen
 Variable pro Versuchsperson vorliegen. \textsc{rm-anova} wird hier
 nicht besprochen, da es mittlerweile flexiblere Werkzeuge gibt,
 um mit Abhängigkeitsstrukturen in den Daten umzugehen (gemischte Modelle).

 \item[\textsc{MANOVA}.] Steht für \textit{multivariate analysis of variance}.
 Es ist eine Erweiterung von \textsc{anova} auf mehrere \textit{outcomes}.
 Zu \textsc{manova} schreiben \citet{Everitt2011} in der Einführung
 ihres Buches \textit{An introduction to applied multivariate analysis with R}
 Folgendes:

\end{description}

\begin{quote}
``But there is an area of multivariate statistics that we have omitted
from this book, and that is multivariate analysis of variance (\textsc{manova})
and related techniques (\dots). There are a variety of reasons for this omission. First,
we are not convinced that \textsc{manova} is now of much more than historical
interest; researchers may occasionally pay lip service to using
the technique, but in most cases it really is no more than this.
They quickly move on to looking at the results for individual variables.
And \textsc{manova} for repeated measures has been largely superseded by the
models that we shall describe [in ihrem Buch].'' (S.~vii-viii) 
\end{quote}
\parend

Bemerkung \ref{bm:ttestapprox} gilt sinngemäss auch für $F$-Tests und Varianzanalyse.

\section{Geplante Vergleiche und Post-hoc-Tests}
Mit einfaktorieller Varianzanalyse versucht man die Frage
zu beantworten, ob sich die Gruppenmittel (\emph{irgendwelche}
Gruppenmittel) in der Population voneinander
unterscheiden oder ob \emph{irgendwelche} Gruppenmittel
sich nicht nur wegen der zufälligen Zuordnung unterscheiden.
Findet man einen
kleinen $p$-Wert, dann tendiert man dazu, davon auszugehen,
dass irgendwelche Gruppenmittel sich tatsächlich voneinander
unterscheiden. Die Varianzanalyse bietet aber keine
Antwort auf die naheliegende Folgefrage: Welche Gruppen
unterscheiden sich eigentlich genau voneinander?
Unterscheiden sich Gruppen A und B, oder eher Gruppen A und C
oder B und C?

Um solche Fragen zu beantworten, bedienen sich Forschende
oft auf die Varianzanalyse folgender Signifikanztests.
Wenn sich die gestellten Fragen erst \emph{nach} der
Daten\-erhebung ergeben und nicht im Vorhinein aus der
Theorie abgeleitet wurden (exploratorische Analyse),
spricht man von \term{Post-hoc-Tests}. Wenn die
Fragen schon \emph{vor} der Untersuchung vorlagen
(konfirmatorische Analyse), spricht man von
\term{geplanten Vergleichen}.

Häufig verwendete Verfahren für solche nachfolgende Tests
tragen Namen wie `$t$-Tests mit Bonferroni-Korrektur',
`$t$-Tests mit Holm--Bonferroni-Korrektur',
`Fishers \textit{least significant difference}-Test',
`Scheffé-Test' usw. Die Idee ist, dass das aufgrund
der mehrfachen Tests gestiegene globale Risiko,
einen Fehler der ersten Art zu begehen, kontrolliert
werden muss.

Zusätzliche Tests sind jedoch nicht immer nötig
oder erwünscht. Entscheidend sind meines Erachtens
die Theorie und die Hypothesen, die der Studie zu
Grunde lagen, und welche Datenmuster man als
Belege für diese Theorie und Hypothesen betrachten
würde:

\begin{itemize}
\item Sagt die Theorie voraus, dass es \emph{irgendwelche}
Gruppenunterschiede (egal welche) geben wird, dann reicht eine
Varianzanalyse aus. Man kann zusätzlich eventuell interessante
Gruppenunterschiede deskriptiv berichten, etwa indem man
das lineare Modell, auf das man die Varianzanalyse durchgeführt hat,
grafisch darstellt. Diese möglichen Unterschiede überlässt
man dann einer neuen, konfirmatorischen Studie (siehe \citealp{Bender2001}, S. 344).
Falls die Varianzanalyse keine Signifikanz ergibt,
sollte man in diesem Fall auch auf zusätzliche Tests verzichten,
sodass man den \textit{familywise Type-I error rate} nicht erhöht.

\item Sagt die Theorie jedoch einen \emph{spezifischen}
Gruppenunterschied voraus, oder werden mehrere separate Theorien
überprüft, die sich auf unterschiedliche Gruppenmittel beziehen
(z.B.\ A vs.\ B und C vs.\ D), dann braucht man eigentlich
die Varianzanalyse nicht auszuführen und reichen paarweise Vergleiche.
Allfällige interessante aber nicht vorhergesagte Gruppenunterschiede
werden deskriptiv (nicht inferenzstatistisch) berichtet und man
überlasst sie wiederum einer neuen, konfirmatorischen Studie.

\item Sagt die Theorie voraus, dass sich ein bestimmter
Unterschied \emph{oder} ein bestimmter anderer Unterschied zeigen wird,
dann sollte man sich über die oben angesprochenen Methoden
schlau machen. Man kann dann auf die Varianzanalyse selber verzichten
und direkt die relevanten Tests, mit einer geeigneten Korrektur, durchführen.

\item Sagt die Theorie komplexere
Gruppenunterschiede vorher, etwa `Das Gesamtmittel von Gruppen A und B
ist niedriger als das Gesamtmittel von Gruppen B, C und D', so sollte man
sich schlau machen über Methoden, mit denen man solche Fragen beantworten 
kann. Siehe hierzu \citet{Schad2020}.

\item Sagt die Theorie voraus, dass sich ein bestimmter Unterschied
\emph{und} ein bestimmter anderer Unterschied zeigen wird, dann reichen
m.E.\ wiederum zwei paarweise Vergleiche.
\end{itemize}

Eine kurze Einführung mit vielen Referenzen ist \citet{Bender2001}.
Konkrete Ratschläge finden Sie in \citet{Ruxton2008}. Diesen sind jedoch wohl
schwierig zu folgen ist,
wenn man noch keine konkrete Erfahrung mit derartigen
Analysen hat.
Ein Blogeintrage zum Thema ist \href{https://janhove.github.io/posts/2016-04-01-multiple-comparisons-scenarios/}{\textit{On correcting for multiple comparisons: Five scenarios}}
(1.4.2016).

\begin{framed}
\noindent \textbf{Empfehlung: Seien Sie vorsichtig und sparsam mit Post-Hoc-Erklärungen.}
 Im Nachhinein gelingt es einem oft, gewisse Muster in den Daten
 theoretisch zu deuten. Dabei ist es durchaus möglich, dass diese Muster
 rein zufallsbedingt sind und sich
 bei einer neuen Studie nicht mehr ergeben.

 Es ist übrigens erstaunlich einfach, sich selbst
 im \emph{Nach}hinein weiszumachen, dass man ein bestimmtes Muster
 in den Daten \emph{vorher}gesagt
 hat. Ein wichtiger Grund hierfür ist, dass wissenschaftliche Theorien
 und Hypothesen oft vage sind und mehreren statistischen Hypothesen entsprechen.
 Um zu vermeiden,
 dass man zuerst sich selbst und nachher seinen Lesenden vortäuscht, dass
 man die Muster in den Daten so vorhergesagt hat, wie sie sich ergeben haben,
 kann man seine wissenschaftlichen und statistischen Hypothesen
 präregistrieren: Man spezifiziert vor der Datenerhebung, welche Muster
 man erwartet und wie man die Daten analysieren wird.
 Für mehr Informationen zu Präregistration, siehe etwa
 \url{http://datacolada.org/64}, \citet{Wagenmakers2012b}
 und \citet{Chambers2017}.
\end{framed}

\section{$F$-Test im \texttt{summary()}-Output}\label{sec:ftestsummary}
Jetzt können wir auch endlich die letzte Zeile
im \texttt{summary()}-Output eines \texttt{lm()}-Modells
entziffern. Greifen wir nochmals das Modell \texttt{b11.lm1} auf:
<<>>=
summary(b11.lm1)
@

Der $F$-Test ($F(3, 151) = 4.8$, $p = 0.003$)
testet die Nullhypothese, dass alle Prädiktoren zusammen
im Modell eigentlich überhaupt keine Varianz im outcome
erfassen. Anhand des \texttt{anova()}-Outputs kann
man die Zahlen rekonstruieren.
<<>>=
anova(b11.lm1)
meanSq.total <- (1.755 + 0.364 + 7.965) / 3
meanSq.rest <- 105.786 / 151
fwert <- meanSq.total / meanSq.rest
fwert
@

Den $p$-Wert kann man dann mit der $F(3, 151)$-Verteilung
berechnen:
<<>>=
pf(fwert, 3, 151, lower.tail = FALSE)
@

Ich habe noch nirgends gesehen, dass dieser $F$-Test
relevant für die Beantwortung einer Forschungsfrage ist.
Sie können ihn also ohne Weiteres ignorieren.

\section{Zwischenfazit Signifikanztests}

\begin{itemize}
 \item Der Zweck von Signifikanztests ist, die Wahrscheinlichkeit zu kontrollieren,
 dass wir schlussfolgern, dass die Nullhypothese nicht stimmt, 
 wenn diese tatsächlich schon stimmt.
 
\item Die Nullhypothese ist fast ausnahmslos, dass das Muster nur aufgrund von
 Zufall zu Stande gekommen ist, sei dies wegen der zufälligen Zuordnung in
 einem Experiment oder wegen der Zufallsauswahl, wenn man mit Stichproben
 aus einer Population arbeitet. Interessiert man sich für Gruppenunterschiede,
 so entspricht dies in der Regel der Annahme, es eigentlich keine solchen Unterschied
 gibt.

 \item $p$-Werte schätzen in der Regel die Wahrscheinlichkeit, mit der man das beobachtete
 Muster (z.B.\ einen Gruppenunterschied oder eine andere Parameterschätzung)
 oder noch extremere Muster antreffen würde, wenn die Nullhypothese tatsächlich
 stimmen würde. 
 Ist diese Wahrscheinlichkeit niedrig, d.h., unter einer im
 Vorfeld definiert $\alpha$-Schwelle (meistens $\alpha = 0.05$), so betrachtet man
 das beobachtete Muster als inkompatibel mit der Nullhypothese und verwirft man letztere.

 \item Mit Signifikanztests überprüft man nicht, ob ein Unterschied oder ein Parameter gross ist; 
 man testet lediglich die Nullhypothese, dass dieser Parameter einen bestimmen Wert hat (in der Regel 0). Man kann Signifikanztests
 daher nicht einsetzen, um zu bestimmen, ob man irgendeinen beobachteten
 Unterschied als gross oder klein einstufen sollte.
 
 \item $p > 0.05$ heisst \emph{nicht}, dass die Nullhypothese stimmt;
       $p \leq 0.05$ heisst \emph{nicht}, dass sie nicht stimmt.

 \item Auch wenn die Nullhypothese nicht stimmt, muss das nicht heissen,
 dass dafür Ihre wissenschaftliche Hypothese stimmt: Vielleicht gibt es
 noch andere theoretische Ansätze, die auch mit den Befunden kompatibel sind,
 oder vielleicht verzerrt Ihre Datenerhebung die Ergebnisse.

 \item Randomisierungs- bzw.\ Permutationstests, $t$-Tests und $F$-Tests dienen alle dem gleichen
 Zweck. $t$-Tests verwendet man dabei, um die Nullhypothese für Parameterschätzungen
 (z.B.\ Gruppenunterschiede oder Regressionskoeffizienten) zu testen;
 $F$-Tests, um zu testen, ob ein Prädiktor (oft mit mehr als zwei Ausprägungen)
 mehr Streuung in den Daten beschreibt als man rein durch Zufall erwarten würde.

 \item $t$- und $F$-Tests können vom allgemeinen linearen Modell, unter Annahme eines
 i.i.d.\ normalverteilten Restfehlers, abgeleitet
 werden. Oft dienen sie als Annäherungen zu Randomisierungs- bzw.\ Permutationstests.
 
 \item Es gibt nicht für jede Situation \emph{den} richtigen Signifikanztest.
 Wenn es mehrere vertretbare Tests gibt, so brauchen ihre Ergebnisse
 nicht miteinander übereinzustimmen.

 \item Es gibt Signifikanztests, denen wir noch nicht begegnet sind.
 Von der Berechnung her unterscheiden diese sich von den $t$- und $F$-Tests,
 aber ihr Ziel ist nach wie vor gleich: Die Wahrscheinlichkeit berechnen,
 mit der man ein beobachtetes oder ein noch extremeres Muster antreffen würde,
 wenn dieses Muster nur Zufall zuzuschreiben wäre.
\end{itemize}