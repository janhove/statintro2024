\chapter{Der $t$-Test}\label{ch:ttest}
Obwohl Permutationstests gültige Signifikanztests
sind, deren Annahmen sich in kontrollierten Experimenten
durch das Forschungsdesign rechtfertigen lassen,
trifft man sie in der Praxis selten an.
Dies ist historischen Gründen und Trägheit zuzuschreiben.
Anno dazumal war es zu aufwendig, ein paar tausend
Permutationen der Daten durchzuackern, um
die Verteilung eines Gruppenunterschieds
oder eines Korrelationskoeffizienten unter
der Nullhypothese zu generieren.
Stattdessen wurden Signifikanztests entwickelt,
deren mathematische Herleitung zwar komplizierter
ist, die aber schneller ausgeführt werden können.
Beispiele solcher Tests sind der $t$-Test und
der $F$-Test. Dass diese so oft verwendet werden,
verdanken sie der Tatsache, dass ihre Ergebnisse oft mit
jenen der Randomisierungs- und Permutationstests übereinstimmen:
\begin{quote}
``the statistician does not carry out this very
simple and very tedious process [= Daten permutieren],
but his conclusions have no justification beyond the
fact that they agree with those which could have been
arrived at by this elementary method.''
(\citealp{Fisher1936})
\end{quote}
Im Folgenden stellen wir zunächst den $t$-Test vor, gefolgt
von Varianzanalyse und dem $F$-Test. Gerade in einfacheren kontrollierten
Experiment wie im letzten Kapitel würde ich jedoch nicht automatisch
auf diese Annäherungen aus der Prä-Computerära zurückgreifen, sondern
einen Randomisierungs- oder Permutationstest in Erwägung ziehen.

Die Randomisierungstests aus Kapitel \ref{ch:logik} gehen davon aus,
dass allfällige beobachtete Unterschiede laut der Nullhypothese
das Resultat einer zufälligen Zuordnung
(\textit{random assignment}) sind. 
Anders ist es beim $t$-Test (und beim $F$-Test): 
Hier wird davon ausgegangen, dass allfällige beobachtete Unterschiede
laut der Nullhypothese
einer Zufallsauswahl aus einer Population (\textit{random sampling})
zugeschrieben werden können.
Genauer gesagt geht man beim \term{Einstichproben-$t$-Test} aus von 
der Nullhypothese,
dass man $n$ i.i.d.\ Beobachtungen $X_1, \dots, X_n$ hat,
die nach einer Normalverteilung mit unbekanntem Mittel $\mu$ und
unbekannter Varianz $\sigma^2$ verteilt sind. Wie wir bereits
gesehen haben (Satz \vref{th:gosset}), gilt unter diesen Annahmen, dass
\[
  T := \frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t_{n-1},
\]
wo $\overline{X}$ das Mittel der $n$ Beobachtungen ist und $S$ 
die Stichprobenstandardabweichung. Wir können nun testen, 
ob die beobachteten Daten kompatibel mit einem bestimmten 
Populationsmittel $\mu_0$ sind. Konkret berechnen wir
\[
  T_0 := \frac{\overline{X} - \mu_0}{S/\sqrt{n}}
\]
und dann schlagen wir nach, wie extrem dieser Wert unter einer $t_{n-1}$-Verteilung
ist.

\mypar[Einstichproben-$t$-Test]{Beispiel}
  Wir beobachten 7 Werte mit Stichprobenmittel $\overline{X} = -2.4$
  und Stichprobenstandardabweichung $S = 5$. Wir stellen die Nullhypothese
  auf, dass diese sieben Werte zufällig aus einer Normalverteilung mit
  Mittel $\mu_0 = 1$ und unbekannter Varianz gezogen wurden.
  Der $t$-Wert zu dieser Nullhypothese beträgt
  \[
    \frac{-2.4 - 1}{5/\sqrt{7}} \approx -1.80.
  \]
  Für einen zweiseitigen $t$-Test schlagen wir nach, wie häufig wir
  bei einer $t(6)$-Verteilung Werte von $-1.80$ oder kleiner
  und von $1.80$ oder grösser antreffen; siehe auch Abbildung \ref{fig:t6}:
<<>>=
t0 <- (-2.4 - 1)/(5/sqrt(7))
pt(-abs(t0), df = 7 - 1) + pt(abs(t0), df = 7 - 1, lower.tail = FALSE)
@
  Das heisst, wir erhalten einen $p$-Wert von etwa $0.12$.
\parend

<<echo = FALSE, fig.width = 2*2, fig.height = 2*1.3, fig.cap = "Die Wahrscheinlichkeitsdichte einer $t_6$-Verteilung. Die Strichellinien zeigen die Werte $\\pm 1.80$. Die eingefärbte Fläche unter der Wahrscheinlichkeitsdichte entspricht dem zweiseitigen $p$-Wert zu $t = \\pm 1.80$.\\label{fig:t6}", out.width = ".4\\textwidth">>=
ggplot(data = tibble(t = c(-7, 7)),
       aes(x = t)) +
  stat_function(fun = dt, args = list(df = 6)) +
  geom_vline(xintercept = t0, linetype = 2) +
  geom_vline(xintercept = -t0, linetype = 2) +
  stat_function(fun = dt, args = list(df = 6), xlim = c(-7, t0), geom = "area",
                fill = "steelblue") +
  stat_function(fun = dt, args = list(df = 6), xlim = c(-t0, 7), geom = "area",
                fill = "steelblue") +
  ylab("Wsk.-Dichte")
@

Der Einstichproben-$t$-Test kann in R mit der Funktion \texttt{t.test()} ausgeführt
werden:
<<>>=
x <- rnorm(10, mean = 2, sd = 4)
t.test(x, mu = 1) # testet, ob das Mittel der Normalverteilung 1 ist
@

Das $t$-Wert-Verfahren kann so angepasst werden,
dass es für den Vergleich von zwei Gruppenmitteln geeignet ist.
Dies resultiert im \term{Zweistichproben-$t$-Test}.
Dieser geht von der Nullhypothese aus, dass zwei unabhängige Stichproben
gezogen wurden:
die erste zählt $n_1$ Beobachtungen 
aus einer Normalverteilung mit unbekanntem Mittel $\mu_1$ und
unbekannter Varianz $\sigma^2$;
die zweite zählt $n_2$ Beobachtungen aus einer Normalverteilung 
mit unbekanntem Mittel $\mu_2$
und der gleichen unbekannten Varianz $\sigma^2$.
Unter diesen Annahmen gilt für den Unterschied zwischen den Stichprobenmitteln,
dass
\[
  \overline{X}_1 - \overline{X}_2 \sim \mathcal{N}\left(\mu_1 - \mu_2, \sigma^2\left(\frac{1}{n_1} + \frac{1}{n_2}\right)\right).
\]
Die gemeinsame Varianz $\sigma^2$
können wir nun anhand der Stichprobenvarianzen $S_1^2, S_2^2$ schätzen, und zwar so
\[
  S^2 = \frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}.
\]
Diese Statistik nennt man die \term{gepoolte Varianz} (\textit{pooled variance}).
Man kann zeigen, dass
\[
  T := \frac{(\overline{X}_1 - \overline{X}_2) - (\mu_1 - \mu_2)}{S\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t_{n-2}.
\]
Die Herleitung dieser Tatsache schenken wir uns, 
aber im nächsten Beispiel wird sie anhand einer Simulation illustriert.
Für einen hypothetischen Unterschied zwischen den Mitteln der Populationen
kann man also testen, ob die Daten mit diesem Unterschied kompatibel sind.

\mypar[Simulation]{Beispiel}\label{bsp:tvalues}
  Mit den folgenden Befehlen werden 20'000 Mal
  zwei Stichproben mit 3 bzw.\ 5 Beobachtungen
  aus der gleichen Normalverteilung generiert.
  Also gilt $\mu_1 - \mu_2 = 0$.
  Wir berechnen jeweils den $t$-Wert mit der \texttt{t.test()}-Funktion;
  die Einstellung \texttt{var.equal = TRUE} gibt an, dass wir davon ausgehen,
  dass die Stichproben aus Normalverteilung mit gleicher Varianz stammen.
<<>>=
n_runs <- 20000
ttest_one_run <- function(n1, n2, sd1, sd2) {
  x1 <- rnorm(n1, mean = 0, sd = 3)
  x2 <- rnorm(n2, mean = 0, sd = 3)
  t.test(x1, x2, var.equal = TRUE)$statistic
}
n1 <- 3
n2 <- 5
t_values <- replicate(n_runs, ttest_one_run(n1, n2, 3, 3))
@
 Wenn wir die 20'000 $t$-Werte grafisch darstellen, sehen wir,
 dass ihre Verteilung einer $t$-Verteilung mit $3 + 5 - 2 = 6$ Freiheitsgraden
 entspricht (Abbildung \ref{fig:tvalues}).
<<fig.width = 2*2, fig.height = 2*1.3, fig.cap = "Simulierte und theoretische Verteilungsfunktion. Die blaue Kurve stellt die empirische Verteilungsfunktion der 20'000 simulierten $t$-Werte dar; die rote Strichellinie ist die Verteilungsfunktion der $t_6$-Verteilung.\\label{fig:tvalues}", out.width = ".4\\textwidth">>=
df_t <- tibble(t_values)
ggplot(df_t, aes(t_values)) +
  stat_ecdf(colour = "darkblue") +
  stat_function(fun = pt, args = list(df = n1 + n2 - 2),
                colour = "red", linetype = "dashed") +
  xlab("t") +
  xlim(-7, 7) +
  ylab("kumulative Wsk.")
@
Die Warnung sagt lediglich, dass einige der $t$-Werte in der Simulation
ausserhalb des Intervalls $[-7, 7]$ liegen.
\parend

\mypar[Zweistichproben-$t$-Test]{Beispiel}
  Wir beobachten zwei Stichproben. Wir gehen von der Nullhypothese aus,
  dass diese aus unabhängigen Beobachtungen aus Normalverteilungen mit der
  gleichen Varianz stammen und dass sich die Mittel dieser Normalverteilungen
  um $\mu_1 - \mu_2 = -2$ Einheiten unterscheiden.
  Für die erste Stichprobe erhalten wir $n_1 = 7, \overline{X}_1 = 3.4, S_1^2 = 4$.
  Für die zweite Stichprobe erhalten wir $n_2 = 10, \overline{X}_2 = 9, S_2^2 = 5$.

  Wir berechnen
  \[
    S^2 = \frac{(7-1)4 + (10-1)5}{7+10-2} = 4.6
  \]
  und
  \[
    T
    = \frac{3.4 - 9 -(-2)}{\sqrt{4.6}\sqrt{\frac{1}{7} + \frac{1}{10}}}
    \approx -3.41.
  \]
  Wir gleichen diesen Wert mit einer $t_{15}$-Verteilung ab:
<<>>=
pooled_var <- ((7 - 1)*4 + (10 - 1)*5) / (7 + 10 - 2)
t0 <- (3.4 - 9 - (-2)) / (sqrt(pooled_var) * sqrt(1/7 + 1/10))
pt(-abs(t0), df = 7 + 10 - 2) + pt(abs(t0), df = 7 + 10 - 2, lower.tail = FALSE)
@
Also $p = 0.004$. Falls wir eine $\alpha$-Schwelle von $0.05$ hantieren,
würden wir hier die Nullhypothese verwerfen, dass die Stichproben aus Normalverteilungen
mit gleicher Varianz und mit Mitteln, die sich um bloss 2 Einheiten unterscheiden, stammen.
\parend

\mypar[$t$-Test als lineares Modell]{Bemerkung}
Der Codeabschnitt unten zeigt drei äquivalente Möglichkeiten,
um einen Zweistichproben-$t$-Test durchzuführen.
<<>>=
n1 <- 12
n2 <- 28
gruppe <- rep(c("A", "B"), times = c(n1, n2))
ergebnis <- c(rnorm(n = n1, mean = 3, sd = 1),
              rnorm(n = n2, mean = 4, sd = 1))
d <- tibble(gruppe, ergebnis)

# t.test(x1, x2, ...)
t.test(ergebnis[gruppe == "A"], ergebnis[gruppe == "B"],
       mu = 0, var.equal = TRUE)

# t.test(outcome ~ predictor, ...)
t.test(ergebnis ~ gruppe, data = d, mu = 0, var.equal = TRUE)

# lineares Modell
mod.lm <- lm(ergebnis ~ gruppe, data = d)
summary(mod.lm)$coefficients
@
Ein Zweistichproben-$t$-Test kann somit aufgefasst werden als ein lineares Modell,
in dem nur ein Gruppenunterschied modelliert wird und in dem man
von einem i.i.d.\ normalverteilten Restfehler ausgeht.

Ein Einstichproben-$t$-Test entspricht übrigens einem linearen Modell,
in dem nur der Schnittpunkt modelliert wird und in dem von einem 
i.i.d.\ normalverteilten Restfehler ausgegangen wird.
\parend

\mypar[unterschiedliche Varianzen]{Aufgabe}\label{ex:diffvar}
  Erhöhen Sie in Beispiel \ref{bsp:tvalues} die Standardabweichung
  der Normalverteilung, aus der die grössere Stichprobe stammt, von $3$
  auf $9$ und vergleichen Sie die simulierte Verteilungsfunktion mit
  der Verteilungsfunktion einer $t_6$-Verteilung. Was stellen Sie fest?
  Wären $p$-Werte, die auf einer $t_6$-Verteilung basieren in diesem
  Fall richtig kalibriert, zu gross oder zu niedrig?
  
  Senken Sie danach die Standardabweichung für die grössere Stichprobe 
  auf $1$. Was stellen Sie diesmal fest? 
  
  Ziehen Sie ein Fazit.
  
  Hinweis: Machen Sie die grössere Gruppe noch grösser, wenn Ihnen 
  das Fazit entgeht.
\parend

\mypar[$t$-Test nach Welch]{Bemerkung}
  Aufgabe \ref{ex:diffvar} hebt hervor, dass die Nullhypothese
  beim Zweistichproben-$t$-Test die Gleichheit der Populationsvarianzen
  umfasst. Es ist aber auch möglich, die Nullhypothese zu testen, dass
  die eine Stichprobe aus Normalverteilungen mit allenfalls
  unterschiedlichen Varianzen $\sigma_1^2, \sigma_2^2$ stammen, deren
  Mittel sich um einen hypothetischen Wert unterscheiden. Hierzu kann
  man den $t$-Test nach Welch verwenden. Für diesen wird die gepoolte
  Varianz etwas anders berechnet, was in anderen $t$-Werten resultiert.
  Ausserdem werden die Freiheitsgrade anders bestimmt; diese brauchen
  auch nicht länger Ganzzahlen zu sein.
  
  Das folgende Beispiel testet für simulierte Daten, ob sich die Mittel
  der Normalverteilungen, aus denen sie stammen, um $0.5$ Einheiten
  unterscheiden -- und zwar ein Mal mit dem üblichen $t$-Test
  und ein Mal mit dem $t$-Test nach Welch.
<<>>=
x1 <- rnorm(5, mean = 2, sd = 3)
x2 <- rnorm(8, mean = 3, sd = 6)
t.test(x1, x2, mu = 0.5, var.equal = TRUE) # normaler t-Test
t.test(x1, x2, mu = 0.5, var.equal = FALSE) # nach Welch
@
  
  \citet{Ruxton2006} empfiehlt, dass der $t$-Test nach Welch
  immer dann verwendet werden sollte, wenn man sonst einen Zweistichproben-$t$-Test
  durchführen würde. Selber würde ich in vielen Fällen mittlerweile
  einen Randomisierungstest bevorzugen, da dieser von einer Nullhypothese
  ausgeht, die sich einfacher rechtfertigen lässt.
\parend

\mypar[Nutzen für nicht-normalverteilte Beobachtungen]{Bemerkung}\label{bm:ttestapprox}
  Die Nullhypothese, die beim $t$-Test überprüft wird, ist recht unrealistisch,
  besagt er doch, die Daten stammen aus einer Normalverteilung.
  Normalverteilungen sind idealisierte mathematische Konstrukte -- sie sind
  asymptotische Grenzverteilungen von Stichprobeneigenschaften.
  In Tat und Wahrheit sind echte Daten nie normalverteilt.
  Beispielsweise können sie nur Werte in einem beschränkten Bereich annehmen, oder
  sie können nur eine auflistbare (abzählbare) Anzahl von Werten annehmen.
  
  Für ausreichend grosse Stichproben kann man den $t$-Test jedoch als Annäherung
  von Random\-isierungs- und Permutationstests auffassen, wie bereits aus
  dem Zitat von Fisher am Anfang dieses Kapitels hervorging. Es gibt
  aber keine Garantie, dass diese Annäherung für eine konkrete, endliche
  Stichprobe bereits gut genug ist; 
  siehe hierzu etwa \citet[][Abschnitt 4.4]{Hesterberg2014}.
  Für einfachere Designs -- wie jene in Kapitel \ref{ch:logik} -- würde ich
  daher mittlerweile Randomisierungs- und Permutationstests bevorzugen.
  Für kompliziertere Designs und Modelle gibt es diese Möglichkeit nicht immer,
  sodass es sich trotzdem lohnt, mit den Annäherungsmethoden vertraut zu sein.
\parend 

\mypar[$t$-Tests für andere Parameterschätzungen]{Bemerkung}
$t$-Tests können auch eingesetzt werden, um
die Nullhypothese bei Parameterschätzungen,
die sich nicht auf Gruppenmittel beziehen, zu testen.
In den \texttt{summary()}-Outputs in
den vorigen Kapiteln finden Sie hierfür viele Beispiele:
\begin{itemize}
 \item Seite \pageref{sec:aoa}: Die Nullhypothesen, die überprüft werden, 
 besagen, dass die Parameter für \texttt{(Intercept)} und \texttt{AOA} 
 eigentlich gleich $0$ sind. 
 Diese erste Nullhypothese ist vermutlich für niemanden von Interesse ist, 
 da man sich kaum für den Interceptparameter interessiert und 
 da niemand behaupten würde, er dürfte gleich $0$ sein.
 Auch die zweite Nullhypothese kann kaum stimmen, denn sie besagt, dass das \texttt{AOA}
 überhaupt keinen linearen Zusammenhang zum \texttt{GJT} aufweist.

 \item Seite \pageref{sec:money}: 
 Die überprüften Nullhypothesen besagen wiederum, 
 dass die Parameter für \texttt{(Intercept)} und \texttt{n.Kondition} 
 in der Population gleich $0$ sind.
 Die $t$- und $p$-Werte für \texttt{n.Kondition} könnte man auch
 mit der \texttt{t.test()}-Funktion berechnen,
 wie wir es soeben gemacht haben.

 \item Seite \pageref{sec:strategy}:
 Wiederum besagen die überprüften Nullhypothesen,
 dass die Parameter in die Population gleich 0 sind.

 \item Seite \pageref{sec:dragan}: Idem.
 Von Interesse wäre hier allenfalls der Signifikanztest
 für den Interaktionsparameter (\texttt{DraganMitCS}).
\end{itemize}
Die obigen Tests sind exakt, wenn wir von einem i.i.d.\ normalverteilten
Restfehler ausgehen; ansonsten verstehen sie sich als Annäherungen, 
vgl.\ Bemerkung \ref{bm:ttestapprox}.

Auch für einen Korrelationskoeffizienten können wir einen $p$-Wert
mittels eines $t$-Tests berechnen. Mit den Permutationstests in Abschnitt
\vref{sec:permutationcorr} sind wir für den Korrelationskoeffizienten
für den Zusammenhang zwischen den Flanker- und Simondaten in \citet{Poarch2018}
bei einem $p$-Wert von 0.0069 ausgekommen. Mit \texttt{cor.test()}
erhalten wir grundsätzlich das gleiche Resultat ($t(32) = 2.95$, $p = 0.0059$):
<<>>=
cor.test(poarch$Flanker, poarch$Simon)
@

Übrigens erhält man den gleichen $p$-Wert, wenn man diese Daten
in einem Regressionsmodell analysiert:

<<eval = FALSE>>=
# Output nicht gezeigt
poarch1.lm <- lm(Flanker ~ Simon, data = poarch)
poarch2.lm <- lm(Simon ~ Flanker, data = poarch)
summary(poarch1.lm)
summary(poarch2.lm)
@
In diesem Fall besagt die Nullhypothese, dass der Korrelations- bzw.\ $\beta$-Koeffizient
gleich $0$ ist und dass der Restfehler i.i.d.\ normalverteilt ist.
Im Falle eines nicht-normalverteilten Restfehlers versteht sich der Test als
Annäherung.
\parend

% \subsection{$t$-Tests für andere Parameterschätzungen}

% \section{Der Binomialtest}

% \section{Mehrere Gruppen vergleichen: Das Problem}
% Wenn wir statt zwei nun mehrere Gruppen
% mithilfe von Signifikanztests
% hinsichtlich eines outcomes vergleichen möchten,
% läge es auf der Hand, mehrere $t$-Tests einzusetzen.
% Wenn man zum Beispiel drei Gruppen vergleichen
% möchte, könnte man zuerst Gruppen A und B vergleichen,
% dann Gruppen A und C und zu guter Letzt Gruppen B und C.
% Das Problem mit diesem Vorgehen ist, dass
% die Wahrscheinlichkeit, \emph{irgendeinen}
% signifikanten Unterschied zu finden, wenn die
% Nullhypothese tatsächlich stimmt, grösser als $\alpha$
% (in der Regel: $\alpha = 0.05$) ist.
% Die Simulationen in diesem Abschnitt stellen
% dieses Problem unter Beweis. Danach folgen mögliche
% Lösungen.
% 
% <<echo = FALSE>>=
% set.seed(123456789)
% @
% 
% Generieren wir zuerst einen Daten,
% für die wir wissen, dass die Nullhypothese buchstäblich
% stimmt. Wir gehen hier zwar von \textit{random sampling} aus,
% aber wenn wir von \textit{random assignment} ausgingen,
% würde sich am Problem und an der Lösung nichts ändern.
% Mit dem folgenden Befehl wird ein Datensatz (tibble)
% namens \texttt{d} kreiert. Dieser enthält
% 60 Beobachtungen von je zwei Variablen: \texttt{gruppe}
% (drei Ausprägungen mit je 20 Beobachtungen)
% und \texttt{ergebnis}. Letztere Variable wurde
% aus einer Normalverteilung generiert, deren Eigenschaften
% unabhängig von \texttt{gruppe} sind. Die Nullhypothese
% stimmt also. Abbildung \ref{fig:anova_zufallsdaten}
% stellt diese Daten grafisch dar.
% <<cache = TRUE>>=
% d <- tibble(
%   gruppe = rep(c("A", "B", "C"), times = 20),
%   ergebnis = rnorm(n = 3*20, mean = 10, sd = 3)
% )
% @
% 
% <<echo = FALSE, out.width=".6\\textwidth", cache = TRUE, fig.width = .8*6, fig.height = .8*2, fig.cap= "Zufällig generierte Daten: drei Zufallsstichproben von je 20 Beobachtungen aus der gleichen Normalverteilung. Bei Ihnen werden diese Daten natürlich anders aussehen.\\label{fig:anova_zufallsdaten}">>=
% ggplot(d,
%        aes(x = gruppe,
%            y = ergebnis)) +
%   geom_boxplot(outlier.shape = NA) +
%   geom_point(shape = 1,
%              position = position_jitter(width = 0.2, height = 0)) +
%   xlab("Gruppe") +
%   ylab("Ergebnis")
% @
% 
% Wir könnten nun diesen Datensatz drei Mal aufspalten:
% Ein Mal behalten wir nur die Daten aus den Gruppen
% A und B, ein Mal behalten wir nur die Daten aus Gruppen
% A und C, und ein Mal nur die Daten aus Gruppen B und C.
% Wir führen dann jedes Mal einen $t$-Test nach Student aus.
% Bei Ihnen werden der Output anders aussehen, da
% die Daten zufällig generiert wurden.
% Was den R-Code betrifft, bemerke man die 
% Verwendung von \texttt{\%in\%} sowie 
% des Platzhalters \texttt{\_}.
% Mit dem letzteren übergibt man das tibble,
% das man vor dem vorigen \texttt{|>} kreiert hat,
% einem bestimmten Parameter in einer nächsten
% Funktion (hier dem \texttt{data}-Parameter
% in der \texttt{t.test()})-Funktion).
% <<>>=
% # A vs. B
% d |> 
%   filter(gruppe %in% c("A", "B")) |> 
%   t.test(ergebnis ~ gruppe, data = _, var.equal = TRUE)
% 
% # A vs. C
% d |> 
%   filter(gruppe %in% c("A", "C")) |> 
%   t.test(ergebnis ~ gruppe, data = _, var.equal = TRUE)
% 
% # B vs. C
% d |> 
%   filter(gruppe %in% c("B", "C")) |> 
%   t.test(ergebnis ~ gruppe, data = _, var.equal = TRUE)
% @
% 
% Wenn man sich in die Logik des Signifikanztestens
% einschreibt, dann müsste man aufgrund dieser Ergebnisse
% die Nullhypothese, dass die Daten in jeder Gruppe
% aus der gleichen Verteilung stammen, ablehnen,
% denn für den Vergleich zwischen Gruppen A und B
% findet man ja einen $p$-Wert,
% der kleiner als $\alpha = 0.05$ ist ($p = 0.017$).
% Dies obwohl die Nullhypothese in dieser Simulation
% buchstäblich stimmt.
% Dass man Nullhypothesen, die tatsächlich stimmen,
% ab und zu zu Unrecht ablehnt, ist klar---und das
% ist hier auch nicht das Problem. Vielmehr ist das Problem,
% dass diese Fehlerquote (Fehler der ersten Art) angeblich
% bei $\alpha = 0.05$ festgelegt wurde, aber eigentlich
% wesentlich höher ist.
% 
% Diese Tatsache können wir mit einer Simulation illustrieren.
% Mit den folgenden Befehlen wird das gleiche Szenario wie oben
% (3 Gruppen mit je 20 Beobachtungen; Nullhypothese stimmt;
% 3 $t$-Tests) 5'000 durchlaufen. Jedes Mal wird der $p$-Wert
% der einzelnen $t$-Tests gespeichert sowie auch der kleinste
% der jeweils drei $p$-Werte.
% <<cache = TRUE>>=
% n_runs <- 5000
% pval_ab <- vector(length = n_runs)
% pval_ac <- vector(length = n_runs)
% pval_bc <- vector(length = n_runs)
% min_pval <- vector(length = n_runs)
% 
% for (i in 1:n_runs) {
%   sim_df <- tibble(
%     gruppe = rep(c("A", "B", "C"), times = 20),
%     ergebnis = rnorm(n = 3*20, mean = 10, sd = 3)
%   )
% 
%   # A vs. B
%   sim_df_ab <- sim_df |> 
%     filter(gruppe %in% c("A", "B"))
%   pval_ab[[i]] <- t.test(ergebnis ~ gruppe, data = sim_df_ab)$p.value
% 
%   # A vs. C
%   sim_df_ac <- sim_df |> 
%     filter(gruppe %in% c("A", "C"))
%   pval_ac[[i]] <- t.test(ergebnis ~ gruppe, data = sim_df_ac)$p.value
% 
%   # B vs. C
%   sim_df_bc <- sim_df |> 
%     filter(gruppe %in% c("B", "C"))
%   pval_bc[[i]] <- t.test(ergebnis ~ gruppe, data = sim_df_bc)$p.value
% 
%   # Kleinsten der 3 p-Werte speichern
%   min_pval[[i]] <- min(c(pval_ab[[i]], pval_ac[[i]], pval_bc[[i]]))
% }
% @
% 
% Wenn die Nullhypothese stimmt, müssten die $p$-Werte gleichverteilt
% sein (siehe Aufgaben letztes Kapitel).
% Wenn Sie Histogramme der $p$-Werte der einzelnen
% $t$-Tests zeichnen, werden Sie feststellen, dass
% dies tatsächlich der Fall ist. Die Abweichungen,
% die Sie feststellen werden, sind zufallsbedingt;
% wenn Sie die Anzahl Simulationen erhöhen, werden diese Abweichungen kleiner.
% Wenn Sie nun aber die Verteilung der jeweils
% kleinsten der drei $p$-Werte (\texttt{min\_pval})
% zeichnen, werden Sie feststellen, dass diese \emph{nicht}
% gleichverteilt sind; siehe Abbildung
% \vref{fig:anova_pvalues}.
% 
% Wenn wir uns bei unseren Inferenzen darüber, ob sich die drei Gruppen voneinander
% unterscheiden, nach dem kleinsten der drei $p$-Werte richten, würden wir nicht
% in bloss $\alpha =$ 5\% der Fälle die Nullhypothese zu Unrecht ablehnen,
% sondern in mehr als 10\% der Fälle, wenn drei Gruppen verglichen werden:
% <<>>=
% mean(min_pval < 0.05)
% @
% 
% \paragraph{Familywise Type-I error rate.}
%  Wenn man mehrere Signifikanztests verwendet, um eine Nullhypothese
%  zu überprüfen, steigt die Wahrscheinlichkeit, dass mindestens einer
%  von ihnen einen signifikanten $p$-Wert produziert---auch wenn die Nullhypothese
%  stimmt und jeder Test korrekt ausgeführt wurde.
%  Die Wahrscheinlichkeit, dass man mindestens einen signifikanten $p$-Wert
%  antrifft, wenn die Nullhypothese stimmt, nennt man die \textit{familywise Type-I error rate};
%  siehe Abbildung \vref{fig:familywise}.
% 
%  In Fällen wie im obigen Beispiel ist die Tatsache, dass man eine Nullhypothese
%  mit mehreren Tests überprüft hat (\textit{multiple comparisons}),
%  allen klar. Ausserdem kann das Problem in solchen Fällen relativ leicht behoben
%  werden. In anderen Fällen sind die \textit{multiple comparisons} weniger einfach
%  aufzudecken bzw.\ schwieriger bei der Interpretation der Ergebnisse zu berücksichtigen---zum
%  Beispiel, weil im Forschungsbericht nur eine Auswahl der ausgeführten Signifikanztests
%  beschrieben wird. Siehe hierzu Kapitel \ref{ch:QRP}.
% 
% <<echo = FALSE, warning = FALSE, message = FALSE, out.width="\\textwidth", fig.width = 6, fig.height = 1.5, fig.pos = "tp", fig.cap = "Wenn die Nullhypothese stimmt, sollte der $p$-Wert aus einer Gleichverteilung stammen. Für die $p$-Werte aus den einzelnen $t$-Tests ist dies auch der Fall: Die Wahrscheinlichkeit, dass man einen $p$-Wert kleiner als 0.05 beobachtet, liegt tatsächlich bei 5\\%, wenn die Nullhypothese stimmt. Wenn man aber auf der Basis des jeweils kleinsten der drei $p$-Werte schliesst, ob sich die Gruppen unterscheiden, dann sind die relevanten $p$-Werte nicht gleich- sondern rechtsschief verteilt: Die Wahrscheinlichkeit, dass man \\emph{irgendeinen} $p$-Wert kleiner als 0.05 beobachtet, ist erheblich höher als 5\\%, auch wenn die Nullhypothese stimmt.\\label{fig:anova_pvalues}">>=
% cols <- RColorBrewer::brewer.pal(3, "Set1")
% par(mfrow = c(1, 4), las = 2, mar = c(4, 4, 3, 1) + 0.1)
% hist(pval_ab, main = "Vergleich AB", xlab = "p",
%      col = cols[2], freq = FALSE,
%      breaks = seq(0, 1, 0.1))
% hist(pval_ac, main = "Vergleich AC", xlab = "p",
%      col = cols[2], freq = FALSE,
%      breaks = seq(0, 1, 0.1))
% hist(pval_bc, main = "Vergleich BC", xlab = "p",
%      col = cols[2], freq = FALSE,
%      breaks = seq(0, 1, 0.1))
% hist(min_pval, main = "Jeweils kleinster\nder 3 p-Werte", xlab = "min p",
%      col = cols[1], freq = FALSE,
%      breaks = seq(0, 1, 0.1))
% par(op)
% @
% 
% <<echo = FALSE, out.width = ".7\\textwidth", fig.width = 6, fig.height = 3, fig.pos = "tp", fig.cap = "Wenn alle Nullhypothesen stimmen und mehrere unabhängige Signifikanztests durchgeführt werden, dann wird die Wahrscheinlichkeit, dass mindestens ein Test ein signifikantes Ergebnis produziert immer grösser ($y = 1 - (1 - 0.05)^\\textrm{Anzahl Tests}$). Diese Wahrscheinlichkeit ist der \\textit{familywise Type-I error rate}. Zwei Tests sind unabhängig voneinander, wenn das Ergebnis des einen Tests überhaupt keine Indizien darüber gibt, was das Ergebnis des anderen Tests sein wird---z.B., weil komplett andere Daten benutzt werden. Wenn die Tests nicht unabhängig voneinander sind (z.B., weil man die gleichen Daten mit mehreren Tests auswertet), wird dieses Problem zwar immer noch vorhanden sein, aber es wird weniger ausgeprägt sein.\\label{fig:familywise}">>=
% n_tests <- seq(1, 30, by = 1)
% error_rate <- 1 - 0.95^n_tests
% df_error_rate <- tibble(n_tests, error_rate)
% ggplot(df_error_rate,
%        aes(x = n_tests,
%            y = error_rate)) +
%   geom_point() +
%   geom_line() +
%   xlab("Anzahl unabhängiger Tests") +
%   ylim(0, 1) +
%   ylab("Fehler der 1. Art (Wsk.)") +
%   geom_hline(yintercept = 0.05, linetype = 2) +
%   annotate(geom = "text",
%            x = 15, y = 0.10,
%            label = "angeblicher Fehler der 1. Art (5%)") +
%   annotate(geom = "text",
%            x = 15, y = 0.60, angle = 18,
%            label = "tatsächlicher Fehler der 1. Art")
% @
% 
% \section{Erste Lösung: Randomisierungstest}
% Die einfachste Art und Weise, das \textit{multiple comparisons}-Problem
% zu lösen, ist, die Nullhypothese mit einem einzigen Test zu überprüfen
% statt mit mehreren Tests. Am häufigsten wird hierzu Varianzanalyse (\textsc{anova}: \textit{analysis of variance})
% bzw.\ der $F$-Test eingesetzt (siehe nächsten Abschnitt). Aber das Gleiche
% kann man bewirken mit einem Randomisierungstest, der m.E.\ einfacher nachvollziehbar ist.
% 
% Der Randomisierungstest geht von der gleichen Nullhypothese wie im letzten Kapitel aus:
% Die Versuchspersonen (oder was auch immer beobachtet wurde) wurden nach dem Zufallsprinzip
% den Gruppen zugeordnet und die Unterschiede zwischen den Gruppenmitteln, die wir hier
% gerade berechnen, sind lediglich
% das Ergebnis dieser zufälligen Zuordnung.
% <<>>=
% d |> 
%   group_by(gruppe) |> 
%   summarise(mittel = mean(ergebnis))
% @
% 
% Die entscheidende Frage ist nun:
% Wie wahrscheinlich wäre es, dass sich die Mittel so stark
% oder noch stärker voneinander unterscheiden würden, wenn diese Nullhypothese tatsächlich stimmt?
% Um diese Frage zu beantworten, müssen wir zuerst in einer Zahl ausdrücken, wie stark
% sich diese drei Mittel voneinander unterscheiden.
% Die Varianz der Gruppenmittel bietet sich hierfür an.
% <<>>=
% d |> 
%   group_by(gruppe) |> 
%   summarise(mittel = mean(ergebnis)) |> 
%   select(mittel) |> 
%   var()
% @
% 
% Die Varianz der Stichprobenmittel beträgt also 1.282.
% Bei Ihnen wird das Ergebnis aufgrund der zufällig
% generierten Daten anders aussehen.
% Um die Verteilung der Varianzen der Stichprobenmittel
% unter Annahme der Nullhypothese zu generieren,
% können wir die Beobachtungen ein paar tausend Mal
% zufällig den Gruppenbezeichnungen zuordnen
% und jedes Mal die Varianz der Stichprobenmittel
% der permutierten Daten berechnen.
% Die Logik ist identisch mit jener der Permutationstests
% aus dem letzten Kapitel, nur schauen wir uns die Varianz
% der Gruppenmittel statt des Unterschieds zwischen den
% zwei Gruppenmitteln an.
% <<cache = TRUE>>=
% n_runs <- 10000
% var_mittel_H0 <- vector(length = n_runs)
% 
% # Der Datensatz d wird hier als d_H0 kopiert,
% # sodass er nachher nicht überschrieben wird.
% d_H0 <- d
% 
% for (i in 1:n_runs) {
%   d_H0$gruppe <- sample(d_H0$gruppe)
% 
%   var_mittel_H0[[i]] <- d_H0 |> 
%     group_by(gruppe) |> 
%     summarise(mittel = mean(ergebnis)) |> 
%     select(mittel) |> 
%     var()
% }
% @
% 
% <<fig.width = 1.2*3, fig.height = 1.2*2, echo = FALSE, fig.cap = "Die Verteilung der Varianzen der Stichprobenmittel, wenn die Beobachtungen zufällig den Gruppen neu zugeordnet werden. Die Proportion der Varianzen, die mindestens so gross wie die beobachtete Varianz sind, stellt ein gültiger $p$-Wert dar.\\label{fig:distributionvaranova}">>=
% df_var_H0 <- tibble(var_mittel_H0)
% ggplot(df_var_H0,
%        aes(x = var_mittel_H0)) +
%   geom_histogram(fill = "grey", col = "black",
%                  binwidth = 0.10) +
%   xlab("Varianz der Gruppenmittel unter H0") +
%   ylab("Anzahl") +
%   geom_vline(xintercept = 1.282355,
%              colour = "red", linetype = 2) +
%   annotate("text", x = 2, y = 1050, label = "s² > 1.28",
%            angle = 0)
% @
% 
% Die Verteilung dieser Varianzen (\texttt{var\_mittel\_H0})
% wird in Abbildung \ref{fig:distributionvaranova} dargestellt.
% Die Proportion der Varianzen, die mindestens so gross wie die
% beobachtete Varianz sind, ist ein gültiger $p$-Wert.
% <<>>=
% mean(var_mittel_H0 > 1.282355)
% @
% 
% In diesem Fall also $p = 0.066$.
% Von Durchführung zu Durchführung wird sich dieser Wert
% leicht ändern, da er auf `nur' 10'000 der fast
% 578 Quadrillionen möglicher Permutationen basiert.\footnote{Für die Interessierten:
% Es gibt ${{60}\choose{20}} \approx 4.2 \cdot 10^{15}$ Möglichkeiten, um 20 Versuchspersonen
% aus einer Gruppe von 60 zu wählen. Dann gibt es noch ${{40}\choose{20}} \approx 1.4 \cdot 10^{11}$ Möglichkeiten,
% um 20 Versuchspersonen aus der restlichen Gruppe von 40 zu wählen. Zusammen ergibt das etwa
% $4.2 \cdot 10^{15} \cdot 1.4 \cdot 10^{11} \approx 5.78 \cdot 10^{26}$ Möglichkeiten.}
% Aber die Unterschiede werden minimal sein.
% 
% \paragraph{Aufgabe 1.} 
% Wir haben die Unterschiede zwischen den Gruppenmitteln
% mit dem Varianzmass erfasst. Würde sich am Ergebnis etwas ändern, wenn wir
% stattdessen die Standardabweichung benutzt hätten? Versuchen Sie die Frage zu beantworten,
% ohne den Computer zu verwenden.
% 
% \paragraph{Aufgabe 2.}
% In Abschnitt \ref{sec:unterschiede_mehrere_gruppen} auf
% Seite \pageref{sec:unterschiede_mehrere_gruppen} wurde
% ein echter Datensatz mit drei Gruppen vorgestellt.
% Überprüfen Sie die Nullhypothese, dass sich die
% Durchschnittsleistung zwischen den `no information'-,
% `information'- und `strategy'-Konditionen nicht unterscheidet
% und zwar mit einem Permutationstest.
% Über die leicht unterschiedlichen
% Gruppengrössen brauchen Sie sich keine Sorgen zu machen, d.h., bei Ihrer Berechnung
% können Sie stets 15 Datenpunkte der `information'-Kondition zuordnen,
% 14 der `no information'-Kondition und 16 der `strategy'-Kondition.
% 
% \paragraph{Mit dem \texttt{coin}-Package.}
% Permutationstests wie diese braucht man nicht selber
% zu programmieren, obwohl ich es für didaktisch sinnvoll halte,
% dass man dies eben schon tut.
% Mit der Funktion \texttt{independence\_test()}
% aus dem \texttt{coin}-Package können sie schnell
% und einfach durchgeführt werden. Am Ergebnis ändert sich nichts.
% Wenn Sie
% diese Funktion mehrmals laufen lassen, werden Sie feststellen,
% dass die kleineren Abweichungen dem Zufallsverfahren hinter dem Test
% zuzuweisen sind.
% <<cache = TRUE>>=
% coin::independence_test(ergebnis ~ factor(gruppe), data = d,
%                         distribution = approximate(nresample = 10000))
% @
% 
% \section{Zweite Lösung: Varianzanalyse und $F$-Tests}
% Permutationtests waren anno dazumals zu schwierig,
% um durchzuführen. Ähnlich wie im letzten Kapitel gibt es
% aber mathematische Kniffe, mit dem man in der Regel zu recht
% ähnlichen Ergebnissen kommt. Diese Tricks
% heissen Varianzanalyse
% (\textsc{anova}: \textit{analysis of variance})
% und der $F$-Test. Da Forschungsartikel oft voller
% \textsc{anova}s und $F$-Tests stehen, werden
% diese Tricks hier kurz vorgestellt.
% 
% \subsection{Streuungszerlegung}
% Der erste Schritt in einer \textsc{anova}
% besteht darin, dass man die Streuung im
% outcome in zwei Teile zerlegt: Streuung zwischen
% den Gruppen und Streuung innerhalb der Gruppen.
% Dazu berechnet man zuerst die Gesamtquadratsumme
% (vgl.\ Quadratsumme auf Seite \pageref{sec:sumsofsquares}).
% Bei Ihnen werden diese Zahlen anders aussehen, da
% die Daten zufällig generiert wurden.
% <<>>=
% # Gesamtquadratsumme
% sq.total <- sum((d$ergebnis - mean(d$ergebnis))^2)
% sq.total
% @
% 
% Um die Streuung innerhalb der Gruppen,
% die Residuenquadratsumme, zu berechnen, kann
% man von allen Beobachtungen ihr Gruppenmittel abziehen.
% Oder man modelliert die Daten in einem allgemeinen
% linearen Modell und berechnet die Quadratsumme der
% Residuen:
% <<>>=
% d.lm <- lm(ergebnis ~ gruppe, data = d)
% 
% # Residuenquadratsumme
% sq.rest <- sum((resid(d.lm))^2)
% sq.rest
% @
% 
% Die vom Modell erfasste Quadratsumme beträgt also:
% <<>>=
% sq.gruppe <- sq.total - sq.rest
% sq.gruppe
% @
% 
% Wir haben, zusätzlich zum Intercept, zwei Parameter gebraucht,
% um den Einfluss des Gruppenfaktors zu modellieren. Dies können
% Sie kontrollieren, indem Sie das Modell \texttt{df.lm} inspizieren.
% Im Schnitt beschreibt jeder zusätzliche Parameter also eine Quadratsumme von 25.6:
% <<>>=
% meanSq.gruppe <- sq.gruppe / 2
% meanSq.gruppe
% @
% 
% Es gibt 60 unabhängige Datenpunkte und das Modell schätzt
% bereits 3 Parameter (Intercept + 2 Parameter für die
% Gruppenunterschiede). Daher könnten wir im Prinzip
% noch 57 Parameter schätzen lassen. Dies wäre zwar nicht
% sinnvoll, aber es wäre möglich. Mehr Parameter zu schätzen,
% ginge nicht: In einem allgemeinen linearen Modell kann man
% nur so viele Parameter schätzen als es Beobachtungen gibt.
% Im Schnitt würden diese 57 Parameter eine Quadratsumme
% von 8.87 erfassen:
% <<>>=
% meanSq.rest <- sq.rest / (60 - 2 - 1)
% meanSq.rest
% @
% 
% Die kritische Einsicht ist nun, dass wenn die Nullhypothese
% stimmt, die zwei Gruppenparameter im Schnitt nur die gleiche
% Quadratsumme wie die 57 nicht-modellierten Parameter beschreiben
% können. Also: Wenn $H_0$ zutrifft, sollten \texttt{meanSq.gruppe}
% und \texttt{meanSq.rest} einander gleich sein. Eine andere
% Art und Weise, um dies auszudrücken, ist, dass das Verhältnis von
% \texttt{meanSq.gruppe} und \texttt{meanSq.rest} laut der Nullhypothese im Schnitt (`$\mathbb{E}$') 1
% sein soll:
% \[
% H_0: \mathbb{E}\left(\frac{\texttt{meanSq.gruppe}}{\texttt{meanSq.rest}}\right) = 1.
% \]
% Dieses Verhältnis bezeichnet man als $F$.
% In unserem Fall beträgt $F$ 2.89.
% <<>>=
% f.wert <- meanSq.gruppe / meanSq.rest
% f.wert
% @
% 
% \subsection{Der $F$-Test}
% Aufgrund des Stichprobenfehlers wird $F$ nie ganz genau 1 sein.
% Die nächste Frage ist daher: Wie ungewöhnlich wäre ein $F$-Wert von 2.89 oder grösser,
% wenn die Nullhypothese stimmt?
% Wenn die Daten in den unterschiedlichen
% Gruppen aus Normalverteilungen
% mit dem gleichen Mittel und der gleichen Varianz stammen,
% dann fällt der $F$-Wert in eine bestimmte Verteilung, nämlich eine $F$-Verteilung.
% $F$-Verteilungen haben zwei Parameter (`Freiheitsgrade'):
% die Anzahl Parameter, die man brauchte, um den Effekt zu modellieren (hier: 2),
% und die Anzahl Beobachtungen, die man nicht aufgebraucht hat (hier: 57).
% Abbildung \ref{fig:distf} zeigt eine $F(2, 57)$-Verteilung.\footnote{Achtung: $2, 57$ ist keine Kommazahl; es handelt sich um zwei separate Zahlen, die Freiheitsgrade.}
% <<echo = FALSE, fig.width = 1.4*3, fig.height = 1.4*2, fig.cap = "Die $F$-Verteilung mit 2 und 57 Freiheitsgraden. ($F$-Verteilungen haben zwei Freiheitsgrade.) Die Strichellinie stellt den beobachteten $F$-Wert dar.\\label{fig:distf}">>=
% ggplot(data.frame(x = c(0, 5)),
%        aes(x)) +
%   stat_function(fun = function(x) df(x, 2, 57)) +
%   xlab("F-Wert") +
%   ylab("Wsk.-Dichte") +
%   ggtitle("F(2, 57)-Verteilung") +
%   geom_vline(xintercept = f.wert, linetype = 2)
% @
% 
% Die Fläche unter der Kurve rechts von der Strichellinie
% ist die Wahrscheinlichkeit, dass man einen $F$-Wert von
% 2.89 oder mehr beobachtet, wenn die Nullhypothese tatsächlich stimmt.
% Sie kann so berechnet werden:
% <<>>=
% pf(f.wert, df = 2, df2 = 60 - 2 - 1, lower.tail = FALSE)
% @
% 
% Dies ist fast das gleiche Ergebnis wie beim Permutationstest.
% 
% \subsection{Varianzanalyse in R}
% Glücklicherweise muss man all diese Schritte nicht von Hand
% ausführen. Es reicht, die Daten in einem allgemeinen linearen
% Modell zu modellieren (was oben bereits gemacht wurde)
% und die \texttt{anova()}-Funktion aufs Modell anzuwenden:
% <<>>=
% anova(d.lm)
% @
% 
% Berichten würde man dieses Ergebnis wie folgt: $F(2, 57) = 2.89$, $p = 0.064$.
% 
% \paragraph{Aufgabe 1.}
% In Abschnitt \ref{sec:unterschiede_mehrere_gruppen} auf
% Seite \pageref{sec:unterschiede_mehrere_gruppen} wurde
% ein echter Datensatz mit drei Gruppen vorgestellt.
% Überprüfen Sie die Nullhypothese, dass sich die
% Durchschnittsleistung zwischen den `no information'-,
% `information'- und `strategy'-Konditionen nicht unterscheidet
% und zwar mit einer \textsc{anova}.
% 
% \paragraph{Aufgabe 2.}
% In Abschnitt \ref{sec:money_model} auf Seite \pageref{sec:money_model}
% wurde ein echter Datensatz mit zwei Gruppen vorgestellt.
% Diesen haben Sie im letzten Kapitel mit einem $t$-Test analysiert.
% Analysieren Sie ihn hier nochmals mit einem normalen $t$-Test (also nicht
% nach Welch) und mit einer \textsc{anova}. Welchen Merksatz ziehen Sie?\\
% (Tipp: Quadrieren Sie den $t$-Wert, den Sie beim $t$-Test erhalten.)
% 
% 
% \section{Varianzanalyse mit mehreren Prädiktoren}
% Varianzanalyse kann man auch anwenden, wenn
% es mehrere Prädiktoren in einem Modell gibt.
% Die Daten aus der Dragan/Luca-Studie (siehe Seite \pageref{sec:berthele2011b}) kann
% man wie folgt in einer \textsc{anova} auswerten:
% <<message = FALSE>>=
% b11 <- read_csv(here("data", "berthele2011.csv"))
% b11.lm1 <- lm(Potenzial ~ CS * Name, data = b11)
% anova(b11.lm1)
% @
% 
% Jede Zeile in der Tabelle zeigt, wie
% viele Parameter für einen bestimmten Prädiktor
% modelliert werden mussten (\texttt{Df})
% und was die assoziierte Quadratsumme
% und seine $F$- und $p$-Werte sind.
% Aber Vorsicht: Wenn wir die Reihenfolge
% der Prädiktoren ändern, ändern sich ein paar
% Zahlen:
% <<>>=
% b11.lm2 <- lm(Potenzial ~ Name * CS, data = b11)
% anova(b11.lm2)
% @
% 
% Der Grund hierfür ist, dass bei der Berechnung von \texttt{Sum Sq}
% sequenziell vorgegangen wird.
% Wenn wir die Varianzanalyse von Hand ausführen würden,
% würden wir:
% \begin{enumerate}
%  \item die Gesamtquadratsumme berechnen;
% <<>>=
% (ss.gesamt <- sum((b11$Potenzial - mean(b11$Potenzial))^2))
% @
%  \item den Effekt der ersten Variable rausrechnen und berechnen,
%  wie gross die Quadratsumme ist, die diese erfassen kann;
% <<>>=
% cs.lm <- lm(Potenzial ~ CS, data = b11)
% (ss.cs <- ss.gesamt - sum(resid(cs.lm)^2))
% @
%  \item den Effekt der zweiten Variable rausrechnen und berechnen,
%   wie gross die Quadratsumme ist, die diese erfassen kann;
% <<>>=
% name.lm <- lm(Potenzial ~ CS + Name, data = b11)
% (ss.name <- ss.gesamt - ss.cs - sum(resid(name.lm)^2))
% @
%  \item den Interaktionseffekt rausrechnen und berechnen,
%  wie gross die Quadratsumme ist, die dieser erfassen kann;
% <<>>=
% interaktion.lm <- lm(Potenzial ~ CS + Name + CS:Name, data = b11)
% (ss.interaktion <- ss.gesamt - ss.cs - ss.name - 
%     sum(resid(interaktion.lm)^2))
% @
%  \item die restliche Quadratsumme berechnen;
% <<>>=
% (ss.rest <- ss.gesamt - ss.cs - ss.name - ss.interaktion)
% @
%  \item $F$-Werte für alle Effekte berechnen.
% <<>>=
% ss.cs / (ss.rest/151)
% ss.name / (ss.rest/151)
% ss.interaktion / (ss.rest/151)
% @
% \end{enumerate}
% 
% Je nachdem, ob wir \texttt{CS} oder \texttt{Name} als erste Variable
% ins Modell eintragen, ändert sich die Quadratsumme, die dieser
% Variable zugeschrieben wird. Der Grund dafür ist, dass
% die Variablen \texttt{CS} und \texttt{Name} in diesem Datensatz
% etwas miteinander korreliert sind: Es gibt mehr Datenpunkte für
% Dragan ohne Codeswitches als mit und mehr Datenpunkte für Luca
% mit Codeswitches als ohne:
% <<>>=
% xtabs(~ CS + Name, data = b11)
% @
% 
% Ein Teil der Streuung in den \texttt{Potenzial}-Werten
% kann daher nicht eindeutig dem einen oder dem anderen
% Prädiktor zugewiesen werden. Wird \texttt{CS} als erste
% Variable dem Modell hinzugefügt, dann wird all die Streuung,
% für die es nicht ganz klar ist, ob sie \texttt{CS}
% oder \texttt{Name} zu verdanken ist,
% der Variable \texttt{CS} zugeschrieben; wird \texttt{Name}
% als erste Variable dem Modell hinzugefügt, wird all diese
% Streuung ihr zugeschrieben. Daher ändern sich die Ergebnisse
% für diese Variablen, je nachdem, welche Variable zuerst hinzugefügt
% wurde. Am Ergebnis für den letzten Effekt (die Interaktion) ändert
% sich jedoch nichts.
% 
% \paragraph{\textit{Type-I, Type-II, Type-III sum of squares}.}
%  Wenn die Quadratsummen und die von ihnen abgeleiteten $F$- und $p$-Werte
%  sequenziell berechnet werden (wie oben),
%  spricht man von \textit{Type-I sum of squares}.
%  Diese Berechnungsart ist insbesondere dann sinnvoll, wenn man sich für den
%  als letzten modellierten Effekt interessiert,
%  aber zuerst noch ein paar andere
%  Variablen berücksichtigt.
% 
%  Eine alternative Berechnungsart wird auf typische kreative
%  Weise \textit{Type-II sum of squares}
%  genannt. Hierzu werden die Effekte in allen möglichen Reihenfolgen ins Modell eingetragen
%  (aber Interaktionen kommen immer nach Haupteffekten) und gilt als der $F$- und $p$-Wert eines
%  Effektes jene $F$- und $p$-Werte, die man antrifft, wenn der Effekt als letzter eingetragen wurde.
%  In unserem Beispiel sähe das Ergebnis wie folgt aus:
% 
%  \begin{itemize}
%   \item \texttt{CS}: Man übernimmt die Angaben für \texttt{CS} aus der \textsc{anova}-Tabelle
%   für Modell \texttt{b11.lm2}, da hier \texttt{CS} als letzter Haupteffekt eingetragen wurde:
%   $F(1, 151) = 1.9$, $p = 0.17$.
% 
%   \item \texttt{Name}: Man übernimmt die Angaben für \texttt{Name} aus der \textsc{anova}-Tabelle
%   für Modell \texttt{b11.lm1}, da hier \texttt{Name} als letzter Haupteffekt eingetragen wurde:
%   $F(1, 151) = 0.52$, $p = 0.47$.
% 
%   \item Die Interaktion kommt immer nach den Haupteffekten, weshalb ihr Ergebnis in beiden
%   Modellen gleich ist: $F(1, 151) = 11.4$, $p < 0.001$.
%  \end{itemize}
% 
%  Diese Ergebnisse kann man automatisch mit der \texttt{Anova()}-Funktion (mit grossem A)
%  aus dem \texttt{car}-Package berechnen:
% <<>>=
% car::Anova(b11.lm1)
% @
% 
%  Es gibt auch noch die Berechnungsart \textit{Type-III sum of squares},
%  aber von dieser wird meistens abgeraten. \textit{Type-I} und \textit{Type-II sum of squares}
%  können beide je nach der Situation nützlich sein, und wenn man sich nur für die Interaktion
%  interessiert oder wenn man es genau die gleiche Anzahl Beobachtungen pro `Zelle' gibt,
%  macht es eh nichts aus.
%  Schmeissen Sie aber bitte keine Daten weg,
% um so gleich grosse Zellen zu erhalten und nicht zwischen diesen
% Berechnungsarten wählen zu müssen!
% 
% \section{Begriffe}
% 
% \begin{description}
%  \item[One-way ANOVA (einfaktorielle Varianzanalyse).]
%  Eine Varianzanalyse mit einem
%  Prädiktor. Meistens ist dies eine Gruppenvariable
%  mit zwei oder mehreren Ausprägungen.
% 
%  \item[Two-way ANOVA (zweifaktorielle Varianzanalyse).]
%  Eine Varianzanalyse mit zwei
%  Prädiktoren. Oft ist hierbei die Interaktion zwischen den
%  Prädiktoren von Interesse. Wenn die eine Variable zwei Ausprägungen
%  hat und die andere vier, spricht man oft von einer
%  \textit{$2 \times 4$ two-way \textsc{anova}}. \textit{Three-way},
%  \textit{four-way}, usw., \textsc{anova} gibt es auch: Man fügt
%  dem Modell einfach mehr Prädiktoren hinzu.
% 
%  \item[\textsc{ANCOVA}.] Steht für \textit{analysis of covariance}.
%  Es handelt sich lediglich um eine Varianzanalyse, in der mindestens eine
%  kontinuierliche Kontrollvariable berücksichtigt wird.
%  % Für weiterführende Literatur, siehe \citet{Huitema2011}.
% 
%  \item[\textsc{RM-ANOVA}.] Steht für \textit{repeated-measures analysis of variance}.
%  Dieses Vorgehen kann man verwenden, wenn etwa mehrere Datenpunkte der gleichen
%  Variable pro Versuchsperson vorliegen. \textsc{rm-anova} wird hier
%  nicht besprochen, da es mittlerweile flexiblere Werkzeuge gibt,
%  um mit Abhängigkeitsstrukturen in den Daten umzugehen (gemischte Modelle).
% 
%  \item[\textsc{MANOVA}.] Steht für \textit{multivariate analysis of variance}.
%  Es ist eine Erweiterung von \textsc{anova} auf mehrere \textit{outcomes}.
%  Zu \textsc{manova} schreiben \citet{Everitt2011} in der Einführung
%  ihres Buches \textit{An introduction to applied multivariate analysis with R}
%  Folgendes:
% 
% \end{description}
% 
% \begin{quote}
% ``But there is an area of multivariate statistics that we have omitted
% from this book, and that is multivariate analysis of variance (\textsc{manova})
% and related techniques (\dots). There are a variety of reasons for this omission. First,
% we are not convinced that \textsc{manova} is now of much more than historical
% interest; researchers may occasionally pay lip service to using
% the technique, but in most cases it really is no more than this.
% They quickly move on to looking at the results for individual variables.
% And \textsc{manova} for repeated measures has been largely superseded by the
% models that we shall describe [in ihrem Buch].'' (S.~vii-viii)
% \end{quote}
% 
% \section{Geplante Vergleiche und Post-hoc-Tests}
% Mit einfaktorieller Varianzanalyse versucht man die Frage
% zu beantworten, ob sich die Gruppenmittel (\emph{irgendwelche}
% Gruppenmittel) in der Population voneinander
% unterscheiden oder ob \emph{irgendwelche} Gruppenmittel
% sich nicht nur wegen der zufälligen Zuordnung unterscheiden.
% Findet man einen
% kleinen $p$-Wert, dann tendiert man dazu, davon auszugehen,
% dass irgendwelche Gruppenmittel sich tatsächlich voneinander
% unterscheiden. Die Varianzanalyse bietet aber keine
% Antwort auf die naheliegende Folgefrage: Welche Gruppen
% unterscheiden sich eigentlich genau voneinander?
% Unterscheiden sich Gruppen A und B, oder eher Gruppen A und C
% oder B und C?
% 
% Um solche Fragen zu beantworten, bedienen sich Forschende
% oft auf die Varianzanalyse folgender Signifikanztests.
% Wenn sich die gestellten Fragen erst \emph{nach} der
% Daten\-erhebung ergeben und nicht im Vorhinein aus der
% Theorie abgeleitet wurden (exploratorische Analyse),
% spricht man von \textbf{Post-hoc-Tests}. Wenn die
% Fragen schon \emph{vor} der Untersuchung vorlagen
% (konfirmatorische Analyse), spricht man von
% \textbf{geplanten Vergleichen}.
% 
% Häufig verwendete Verfahren für solche nachfolgende Tests
% tragen Namen wie `$t$-Tests mit Bonferroni-Korrektur',
% `$t$-Tests mit Holm--Bonferroni-Korrektur',
% `Fishers \textit{least significant difference}-Test',
% `Scheffé-Test' usw. Die Idee ist, dass das aufgrund
% der mehrfachen Tests gestiegene globale Risiko,
% einen Fehler der ersten Art zu begehen, kontrolliert
% werden muss.
% 
% Zusätzliche Tests sind jedoch nicht immer nötig
% oder erwünscht. Entscheidend sind meines Erachtens
% die Theorie und die Hypothesen, die der Studie zu
% Grunde lagen, und welche Datenmuster man als
% Belege für dese Theorie und Hypothesen betrachten
% würde:
% 
% \begin{itemize}
% \item Sagt die Theorie voraus, dass es \emph{irgendwelche}
% Gruppenunterschiede (egal welche) geben wird, dann reicht eine
% Varianzanalyse aus. Man kann zusätzlich eventuell interessante
% Gruppenunterschiede deskriptiv berichten, etwa indem man
% das lineare Modell, für das man die Varianzanalyse ausgeführt hat,
% grafisch darstellt. Diese möglichen Unterschiede überlässt
% man dann einer neuen, konfirmatorischen Studie (siehe \citealp{Bender2001}, S. 344).
% Falls die Varianzanalyse keine Signifikanz ergibt,
% sollte man in diesem Fall auch auf zusätzliche Tests verzichten,
% sodass man den \textit{familywise Type-I error rate} nicht erhöht.
% 
% \item Sagt die Theorie jedoch einen \emph{spezifischen}
% Gruppenunterschied voraus, oder werden mehrere separate Theorien
% überprüft, die sich auf unterschiedliche Gruppenmittel beziehen
% (z.B.\ A vs.\ B und C vs.\ D), dann braucht man eigentlich
% die Varianzanalyse nicht auszuführen und reichen $t$-Tests.
% Allfällige interessante aber nicht vorhergesagte Gruppenunterschiede
% werden deskriptiv (nicht inferenzstatistisch) berichtet und man
% überlasst sie wiederum einer neuen, konfirmatorischen Studie.
% 
% \item Sagt die Theorie voraus, dass sich ein bestimmter
% Unterschied \emph{oder} ein bestimmter anderer Unterschied zeigen wird,
% dann sollte man sich über die oben angesprochenen Methoden
% schlau machen. Dies gilt auch wenn die Theorie komplexere
% Gruppenunterschiede vorhersagt, etwa `Das Gesamtmittel von Gruppen A und B
% ist niedriger als das Gesamtmittel von Gruppen B, C und D'.
% Siehe hierzu \citet{Schad2020}.
% 
% \item Sagt die Theorie voraus, dass sich ein bestimmter Unterschied
% \emph{und} ein bestimmter anderer Unterschied zeigen wird, dann reichen
% m.E.\ wiederum zwei t-Tests.
% % Man kann in diesem Fall einen signifikanten
% % und einen nicht-signifikanten Unterschied natürlich nicht als Evidenz für die Theorie betrachten: Vorhergesagt wurden ja zwei Unterschiede.
% \end{itemize}
% 
% 
% Eine kurze Einführung mit vielen Referenzen ist \citet{Bender2001}.
% Konkrete Ratschläge finden Sie in \citet{Ruxton2008}. Diesen sind jedoch wohl
% schwierig zu folgen ist,
% wenn man noch keine konkrete Erfahrung mit derartigen
% Analysen hat. Ein Blogeintrage zum Thema ist \href{https://janhove.github.io/analysis/2016/04/01/multiple-comparisons-scenarios}{\textit{On correcting for multiple comparisons: Five scenarios}}
% (1.4.2016).
% 
% \medskip
% 
% \begin{framed}
% \noindent \textbf{Seien Sie vorsichtig und sparsam mit Post-Hoc-Erklärungen.}
%  Im Nachhinein gelingt es einem oft, gewisse Muster in den Daten
%  theoretisch zu deuten. Dabei ist es durchaus möglich, dass diese Muster
%  rein zufallsbedingt sind und sich
%  bei einer neuen Studie nicht mehr ergeben.
% 
%  Es ist übrigens erstaunlich einfach, sich selbst
%  im \emph{Nach}hinein weiszumachen, dass man ein bestimmtes Muster
%  in den Daten \emph{vorher}gesagt
%  hat. Ein wichtiger Grund hierfür ist, dass wissenschaftliche Theorien
%  und Hypothesen oft vage sind und mehreren statistischen Hypothesen entsprechen.
%  Um zu vermeiden,
%  dass man zuerst sich selbst und nachher seinen Lesenden vortäuscht, dass
%  man die Muster in den Daten so vorhergesagt hat, wie sie sich ergeben haben,
%  kann man seine wissenschaftlichen und statistischen Hypothesen
%  präregistrieren: Man spezifiziert vor der Datenerhebung, welche Muster
%  man erwartet und wie man die Daten analysieren wird.
%  Für mehr Informationen zu Präregistration, siehe etwa
%  \url{http://datacolada.org/64}, \citet{Wagenmakers2012b}
%  und \citet{Chambers2017}.
% \end{framed}
% 
% \section{Zwischenfazit Signifikanztests}
% 
% \begin{itemize}
%  \item $p$-Werte drücken die Wahrscheinlichkeit aus, mit der man das beobachtete
%  Muster (z.B.\ einen Gruppenunterschied oder eine andere Parameterschätzung)
%  oder noch extremere Muster antreffen würde, wenn die Nullhypothese tatsächlich
%  stimmen würde.
% 
%  \item Die Nullhypothese ist fast ausnahmslos, dass das Muster nur aufgrund von
%  Zufall zu Stande gekommen ist, sei dies wegen der zufälligen Zuordnung in
%  einem Experiment oder wegen der Zufallsauswahl, wenn man mit Stichproben
%  aus einer Population arbeitet. Anders ausgedrückt: Sie besagt, dass der eigentliche
%  Gruppenunterschied bzw.\ der Parameter, den man zu schätzen versucht hat,
%  gleich 0 ist.
% 
%  \item Mit Signifikanztests überprüft man nicht, ob ein Unterschied oder ein Parameter gross ist; man testet lediglich die Nullhypothese, dass dieser Parameter einen bestimmen Wert hat (in der Regel 0). Man kann Signifikanztests
%  daher nicht einsetzen, um zu bestimmen, ob man irgendeinen beobachteten
%  Unterschied als gross oder klein einstufen sollte.
% 
%  \item Auch wenn die Nullhypothese nicht stimmt, muss das nicht heissen,
%  dass dafür Ihre wissenschaftliche Hypothese stimmt: Vielleicht gibt es
%  noch andere theoretische Ansätze, die mit den Befunden kompatibel sind,
%  oder vielleicht verzerrt Ihre Datenerhebung die Ergebnisse.
% 
%  \item Oft versucht man die Wahrscheinlicht, dass man die Nullhypothese
%  ablehnt, obwohl diese stimmt, auf 5\% zu reduzieren, indem
%  man nicht schlussfolgert, dass die Nullhypothese nicht stimmt, wenn $p > 0.05$.
% 
%  \item $p > 0.05$ heisst aber \emph{nicht}, dass die Nullhypothese stimmt.
% 
%  \item Randomisierungs- bzw.\ Permutationstests, $t$-Tests und $F$-Tests dienen alle dem gleichen
%  Zweck. $t$-Tests verwendet man dabei, um die Nullhypothese für Parameterschätzungen
%  (z.B.\ Gruppenunterschiede oder Regressionskoeffizienten) zu testen;
%  $F$-Tests, um zu testen, ob ein Prädiktor (oft mit mehr als zwei Ausprägungen)
%  mehr Streuung in den Daten beschreibt als man rein durch Zufall erwarten würde.
% 
%  \item $t$- und $F$-Tests können vom allgemeinen linearen Modell abgeleitet
%  werden. Ihre Annahmen sind den Annahmen dieses Modells gleich. Man kann
%  sie aber durchaus auch als Annäherungen zu Randomisierungs- bzw.\ Permutationstests
%  verstehen und einsetzen.
% 
%  \item Es gibt Signifikanztests, denen wir noch nicht begegnet sind.
%  Von der Berechnung her unterscheiden diese sich von den $t$- und $F$-Tests,
%  aber ihr Ziel ist nach wie vor gleich: Die Wahrscheinlichkeit berechnen,
%  mit der man ein beobachtetes oder ein noch extremeres Muster antreffen würde,
%  wenn dieses Muster nur Zufall zuzuschreiben wäre.
% \end{itemize}
% 
% \section{$F$-Test im \texttt{summary()}-Output}\label{sec:ftestsummary}
% Jetzt können wir auch endlich die letzte Zeile
% im \texttt{summary()}-Output eines \texttt{lm()}-Modells
% entziffern. Greifen wir nochmals das Modell \texttt{b11.lm1} auf:
% <<>>=
% summary(b11.lm1)
% @
% 
% Der $F$-Test ($F(3, 151) = 4.8$, $p = 0.003$)
% testet die Nullhypothese, dass alle Prädiktoren zusammen
% im Modell überhaupt keine Varianz im outcome
% erfassen. Anhand des \texttt{anova()}-Outputs kann
% man die Zahlen rekonstruieren.
% <<>>=
% anova(b11.lm1)
% meanSq.total <- (1.755 + 0.364 + 7.965) / 3
% meanSq.rest <- 105.786 / 151
% fwert <- meanSq.total / meanSq.rest
% fwert
% @
% 
% Den $p$-Wert kann man dann mit der $F(3, 151)$-Verteilung
% berechnen:
% <<>>=
% pf(fwert, 3, 151, lower.tail = FALSE)
% @
% 
% Ich habe noch nirgends gesehen, dass dieser $F$-Test
% relevant für die Beantwortung einer Forschungsfrage ist.
% Sie können ihn also ohne Weiteres ignorieren.